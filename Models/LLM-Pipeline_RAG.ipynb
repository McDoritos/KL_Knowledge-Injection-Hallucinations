{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c7c760",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540551da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from ollama import embed\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbd9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "GENERATION_MODEL = \"qwen3:1.7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d70483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7902b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    query_embedding = embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "\n",
    "    with open(\"../Vector-store/vector_db.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "        VECTOR_DB = [(item[\"chunk\"], item[\"embedding\"]) for item in raw]\n",
    "\n",
    "    # temporary list to store (chunk, similarity) pairs\n",
    "    similarities = []\n",
    "\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        # embedding is already a list of floats (from your JSON)\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "\n",
    "    # Sort after finishing loop\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f1d51c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.83) To demonstrate the universal effectiveness of CARAFE , we conduct comprehensive evaluations across a wide range of dense prediction tasks , i.e. , object detection , instance segmentation , semantic segmentation , image inpainting , with mainstream architectures .\n",
      " - (similarity: 0.79) CARAFE can boost the performance of Faster RCNN [ 3 0 ] [ 4 3 , 4 4 ] val in semantic segmentation , and improves Global&Local [ 1 1 ] by 1. 1 dB of PSNR on Places [ 4 2 ] val in image inpainting .\n",
      " - (similarity: 0.79) With negligible additional parameters , CARAFE benefits state - of - the - art methods in both highlevel and low - level tasks , such as object detection , instance segmentation , semantic segmentation and image inpainting .\n"
     ]
    }
   ],
   "source": [
    "input_query = \"What relationship exists between CARAFE and object detection?\"\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "  print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "\n",
    "instruction_prompt = f'''instruction_prompt =\n",
    "Answer using the context only.\n",
    "Then provide a step-by-step justification under the section \"JUSTIFICATION\",\n",
    "where each step must explicitly reference which context snippet supports it.\n",
    "\n",
    "You can reveal chain-of-thought. Don't make up any new information: {'\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79eba443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "The relationship between CARAFE and object detection is that CARAFE is evaluated across dense prediction tasks, including object detection, and is shown to improve performance in tasks like semantic segmentation (which is a subset of object detection). \n",
      "\n",
      "**JUSTIFICATION:**  \n",
      "1. The first context snippet states: \"To demonstrate the universal effectiveness of CARAFE, we conduct comprehensive evaluations across a wide range of dense prediction tasks, i.e., object detection, instance segmentation, semantic segmentation, image inpainting.\"  \n",
      "   - This explicitly links CARAFE to object detection as one of the evaluated tasks.  \n",
      "\n",
      "2. The second context snippet mentions: \"CARAFE can boost the performance of Faster RCNN [30][43,44] val in semantic segmentation, and improves Global&Local [11] by 1.1 dB of PSNR on Places [42] val in image inpainting.\"  \n",
      "   - Semantic segmentation is a subtask of object detection, and CARAFE's improvement in this task demonstrates its effectiveness in object detection.  \n",
      "\n",
      "3. The third context snippet states: \"With negligible additional parameters, CARAFE benefits state-of-the-art methods in both high-level and low-level tasks, such as object detection, instance segmentation, semantic segmentation and image inpainting.\"  \n",
      "   - CARAFE is explicitly cited as benefiting object detection as a high-level task, reinforcing its relevance to object detection.  \n",
      "\n",
      "Thus, CARAFE is designed to enhance object detection performance through its universal effectiveness in dense prediction tasks."
     ]
    }
   ],
   "source": [
    "stream = chat(\n",
    "    model=GENERATION_MODEL, \n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system', \n",
    "            'content': instruction_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': input_query\n",
    "        },\n",
    "        ],\n",
    "        stream=True,\n",
    "        )\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
