{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281f3e1e",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing: Grouping by Document ID\n",
    "\n",
    "This notebook processes the SciER dataset to group all sentences by document ID, combining content and extracting all relations for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8597b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process the JSONL dataset to group sentences by document ID.\n",
    "    Each document will contain:\n",
    "    - doc_id: The document identifier\n",
    "    - content: List of all sentences in the document\n",
    "    - relations: All extracted relations (rel_plus) from all sentences\n",
    "    \"\"\"\n",
    "    # Dictionary to group data by document ID\n",
    "    documents = defaultdict(lambda: {\n",
    "        'doc_id': None,\n",
    "        'content': [],\n",
    "        'relations': []\n",
    "    })\n",
    "    \n",
    "    # Read the JSONL file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            doc_id = data['doc_id']\n",
    "            \n",
    "            # Initialize doc_id if not set\n",
    "            if documents[doc_id]['doc_id'] is None:\n",
    "                documents[doc_id]['doc_id'] = doc_id\n",
    "            \n",
    "            # Add sentence to content\n",
    "            documents[doc_id]['content'].append(data['sentence'])\n",
    "            \n",
    "            # Add relations (using rel_plus as it includes entity types)\n",
    "            if data.get('rel_plus'):\n",
    "                documents[doc_id]['relations'].extend(data['rel_plus'])\n",
    "    \n",
    "    # Convert to list and write to JSON file\n",
    "    documents_list = list(documents.values())\n",
    "    \n",
    "    # Only write to file if output_file is provided\n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(documents_list, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef823b9d",
   "metadata": {},
   "source": [
    "## Process Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0689ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80 documents from train.jsonl\n",
      "Output saved to: dataset\\SciER\\LLM\\train_by_document.json\n",
      "\n",
      "Example - First Document:\n",
      "Doc ID: 51923817\n",
      "Number of sentences: 51\n",
      "Number of relations: 57\n",
      "\n",
      "First sentence: We propose CornerNet , a new approach to object detection where we detect an object bounding box as ...\n",
      "First relation: ['convolution neural network:Method', 'Part-Of', 'CornerNet:Method']\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "train_input = Path('dataset/SciER/LLM/train.jsonl')\n",
    "train_output = Path('dataset/SciER/LLM/train_by_document.json')\n",
    "\n",
    "# Process the train dataset\n",
    "train_docs = process_dataset(train_input, train_output)\n",
    "\n",
    "print(f\"Processed {len(train_docs)} documents from train.jsonl\")\n",
    "print(f\"Output saved to: {train_output}\")\n",
    "\n",
    "# Show example of first document\n",
    "if train_docs:\n",
    "    print(f\"\\nExample - First Document:\")\n",
    "    print(f\"Doc ID: {train_docs[0]['doc_id']}\")\n",
    "    print(f\"Number of sentences: {len(train_docs[0]['content'])}\")\n",
    "    print(f\"Number of relations: {len(train_docs[0]['relations'])}\")\n",
    "    print(f\"\\nFirst sentence: {train_docs[0]['content'][0][:100]}...\")\n",
    "    if train_docs[0]['relations']:\n",
    "        print(f\"First relation: {train_docs[0]['relations'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ddfd",
   "metadata": {},
   "source": [
    "## Process All Datasets (Dev, Test, Test OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ac820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ dev: 10 documents -> dataset\\SciER\\LLM\\dev_by_document.json\n",
      "✓ test: 10 documents -> dataset\\SciER\\LLM\\test_by_document.json\n",
      "✓ test_ood: 6 documents -> dataset\\SciER\\LLM\\test_ood_by_document.json\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "==================================================\n",
      "Train:    80 documents\n",
      "Dev       10 documents\n",
      "Test      10 documents\n",
      "Test_ood  6 documents\n"
     ]
    }
   ],
   "source": [
    "# Process all datasets\n",
    "datasets = {\n",
    "    'dev': ('dataset/SciER/LLM/dev.jsonl', 'dataset/SciER/LLM/dev_by_document.json'),\n",
    "    'test': ('dataset/SciER/LLM/test.jsonl', 'dataset/SciER/LLM/test_by_document.json'),\n",
    "    'test_ood': ('dataset/SciER/LLM/test_ood.jsonl', 'dataset/SciER/LLM/test_ood_by_document.json')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (input_file, output_file) in datasets.items():\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    if input_path.exists():\n",
    "        docs = process_dataset(input_path, output_path)\n",
    "        results[name] = len(docs)\n",
    "        print(f\"✓ {name}: {len(docs)} documents -> {output_path}\")\n",
    "    else:\n",
    "        print(f\"✗ {name}: File not found - {input_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train:    {len(train_docs)} documents\")\n",
    "for name, count in results.items():\n",
    "    print(f\"{name.capitalize():9} {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5e1ea",
   "metadata": {},
   "source": [
    "## Inspect Sample Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8e7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENT ID: 51923817\n",
      "================================================================================\n",
      "\n",
      "CONTENT:\n",
      "--------------------------------------------------------------------------------\n",
      "1. We propose CornerNet , a new approach to object detection where we detect an object bounding box as a pair of keypoints , the top - left corner and the bottom - right corner , using a single convolution neural network .\n",
      "2. Experiments show that CornerNet achieves a 4 2 . 2 % AP on MS COCO , outperforming all existing one - stage detectors .\n",
      "3. Object detectors based on convolutional neural networks ( ConvNets ) ( Krizhevsky et al. , 2 0 1 2 ; Simonyan and Zisserman , 2 0 1 4 ; He et al. , 2 0 1 6 ) have achieved state - of - the - art results on various challenging benchmarks ( Lin et al. , 2 0 1 4 ; Deng et al. , 2 0 0 9 ; Everingham et al. , 2 0 1 5 ) .\n",
      "... and 48 more sentences\n",
      "\n",
      "RELATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. ['convolution neural network:Method', 'Part-Of', 'CornerNet:Method']\n",
      "2. ['CornerNet:Method', 'Used-For', 'object detection:Task']\n",
      "3. ['CornerNet:Method', 'Evaluated-With', 'MS COCO:Dataset']\n",
      "4. ['ConvNets:Method', 'Synonym-Of', 'convolutional neural networks:Method']\n",
      "5. ['CornerNet:Method', 'Used-For', 'object detection:Task']\n",
      "... and 52 more relations\n",
      "\n",
      "================================================================================\n",
      "Total Sentences: 51\n",
      "Total Relations: 57\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display a complete sample document\n",
    "if train_docs:\n",
    "    sample_doc = train_docs[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"DOCUMENT ID: {sample_doc['doc_id']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nCONTENT:\")\n",
    "    print(\"-\"*80)\n",
    "    for i, sentence in enumerate(sample_doc['content'][:3], 1):\n",
    "        print(f\"{i}. {sentence}\")\n",
    "    if len(sample_doc['content']) > 3:\n",
    "        print(f\"... and {len(sample_doc['content']) - 3} more sentences\")\n",
    "    \n",
    "    print(\"\\nRELATIONS:\")\n",
    "    print(\"-\"*80)\n",
    "    for i, relation in enumerate(sample_doc['relations'][:5], 1):\n",
    "        print(f\"{i}. {relation}\")\n",
    "    if len(sample_doc['relations']) > 5:\n",
    "        print(f\"... and {len(sample_doc['relations']) - 5} more relations\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Total Sentences: {len(sample_doc['content'])}\")\n",
    "    print(f\"Total Relations: {len(sample_doc['relations'])}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51539b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd0609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing and Chunking All Datasets for RAG\n",
      "================================================================================\n",
      "\n",
      "Processing train...\n",
      "✓ train: 240 chunks saved to postprocessed-dataset\\train_chunks.json\n",
      "  Example - Doc ID: 51923817, Chunk: 1\n",
      "  Sentences: 17, Relations: 19\n",
      "\n",
      "Processing dev...\n",
      "✓ dev: 30 chunks saved to postprocessed-dataset\\dev_chunks.json\n",
      "  Example - Doc ID: 53719258, Chunk: 1\n",
      "  Sentences: 36, Relations: 73\n",
      "\n",
      "Processing test...\n",
      "✓ test: 30 chunks saved to postprocessed-dataset\\test_chunks.json\n",
      "  Example - Doc ID: 192546007, Chunk: 1\n",
      "  Sentences: 19, Relations: 50\n",
      "\n",
      "Processing test_ood...\n",
      "✓ test_ood: 18 chunks saved to postprocessed-dataset\\test_ood_chunks.json\n",
      "  Example - Doc ID: AAAI2024, Chunk: 1\n",
      "  Sentences: 29, Relations: 25\n",
      "\n",
      "================================================================================\n",
      "Summary - Chunked Datasets:\n",
      "================================================================================\n",
      "TRAIN       240 chunks from  80 documents\n",
      "DEV          30 chunks from  10 documents\n",
      "TEST         30 chunks from  10 documents\n",
      "TEST_OOD     18 chunks from   6 documents\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Chunking for RAG ---\n",
    "def chunk_document(doc, num_chunks=3):\n",
    "    sentences = doc['content']\n",
    "    relations = doc['relations']\n",
    "    n = len(sentences)\n",
    "    chunk_size = (n + num_chunks - 1) // num_chunks  # ceil division\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, n)\n",
    "        chunk_sentences = sentences[start:end]\n",
    "        # Find relations for sentences in this chunk\n",
    "        # We assume relations are extracted from sentences in order\n",
    "        # So we need to know which relations belong to which sentence\n",
    "        # If relations are not mapped to sentences, we distribute evenly\n",
    "        # Here, we will use a simple approach: if relations are present, split them proportionally\n",
    "        rel_chunk_size = (len(relations) + num_chunks - 1) // num_chunks\n",
    "        rel_start = i * rel_chunk_size\n",
    "        rel_end = min((i + 1) * rel_chunk_size, len(relations))\n",
    "        chunk_relations = relations[rel_start:rel_end]\n",
    "        chunks.append({\n",
    "            'doc_id': doc['doc_id'],\n",
    "            'chunk_id': i + 1,\n",
    "            'content': chunk_sentences,\n",
    "            'relations': chunk_relations\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def process_and_chunk_dataset(input_file, output_file, num_chunks=3):\n",
    "    \"\"\"\n",
    "    Process the JSONL dataset, group by document, then split each document into chunks for RAG.\n",
    "    Each chunk contains:\n",
    "    - doc_id\n",
    "    - chunk_id\n",
    "    - content: sentences in the chunk\n",
    "    - relations: relations for those sentences\n",
    "    \"\"\"\n",
    "    # Use previous process_dataset to group by document\n",
    "    documents = process_dataset(input_file, None)\n",
    "    all_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = chunk_document(doc, num_chunks=num_chunks)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "    return all_chunks\n",
    "\n",
    "# --- Process all datasets with chunking ---\n",
    "output_dir = Path('postprocessed-dataset')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "datasets_to_chunk = {\n",
    "    'train': (Path('dataset/SciER/LLM/train.jsonl'), output_dir / 'train_chunks.json'),\n",
    "    'dev': (Path('dataset/SciER/LLM/dev.jsonl'), output_dir / 'dev_chunks.json'),\n",
    "    'test': (Path('dataset/SciER/LLM/test.jsonl'), output_dir / 'test_chunks.json'),\n",
    "    'test_ood': (Path('dataset/SciER/LLM/test_ood.jsonl'), output_dir / 'test_ood_chunks.json')\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Processing and Chunking All Datasets for RAG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chunk_results = {}\n",
    "for name, (input_path, output_path) in datasets_to_chunk.items():\n",
    "    if input_path.exists():\n",
    "        print(f\"\\nProcessing {name}...\")\n",
    "        chunks = process_and_chunk_dataset(input_path, output_path, num_chunks=3)\n",
    "        chunk_results[name] = chunks\n",
    "        print(f\"✓ {name}: {len(chunks)} chunks saved to {output_path}\")\n",
    "        \n",
    "        # Show example from first chunk\n",
    "        if chunks:\n",
    "            print(f\"  Example - Doc ID: {chunks[0]['doc_id']}, Chunk: {chunks[0]['chunk_id']}\")\n",
    "            print(f\"  Sentences: {len(chunks[0]['content'])}, Relations: {len(chunks[0]['relations'])}\")\n",
    "    else:\n",
    "        print(f\"✗ {name}: File not found - {input_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary - Chunked Datasets:\")\n",
    "print(\"=\"*80)\n",
    "for name, chunks in chunk_results.items():\n",
    "    num_docs = len(set(chunk['doc_id'] for chunk in chunks))\n",
    "    print(f\"{name.upper():10} {len(chunks):4} chunks from {num_docs:3} documents\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
