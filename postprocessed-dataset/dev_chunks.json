[
  {
    "doc_id": "53719258",
    "chunk_id": 1,
    "content": [
      "Recently , three dimensional ( 3D ) convolutional neural networks ( CNNs ) have emerged as dominant methods to capture spatiotemporal representations in videos , by adding to pre - existing 2D CNNs a third , temporal dimension .",
      "Such 3D CNNs , however , are anti - causal ( i.e. , they exploit information from both the past and the future frames to produce feature representations , thus preventing their use in online settings ) , constrain the temporal reasoning horizon to the size of the temporal convolution kernel , and are not temporal resolution - preserving for video sequence - to - sequence modelling , as , for instance , in action detection .",
      "Namely , we propose a novel Recurrent Convolutional Network ( RCN ) , which relies on recurrence to capture the temporal context across frames at each network level .",
      "Our experiments on the large - scale large Kinetics and MultiThumos datasets show that the proposed method performs comparably to anti - causal 3D CNNs , while being causal and using fewer parameters .",
      "Convolutional neural networks ( CNN ) are starting to exhibit gains in action recognition from videos similar to those previously observed in image recognition [ 2 1 , 3 6 ] thanks to new 3D CNNs [ 3 , 5 2 , 4 5 , 1 1 , 5 1 ] .",
      "For instance , Hare et al. [ 1 1 ] have shown that that is the case for the 3D version of 2D residual networks ( ResNets ) [ 1 2 ] .",
      "Other recent works [ 5 2 , 4 5 ] show that 3D convolutions can be decomposed into 2D ( spatial ) and 1D ( temporal ) convolutions ( yielding the S 3 D architecture ) , and that these separate convolution operators not only have fewer parameters to train [ 5 2 ] , but also perform better than 3D ( spatiotemporal ) convolutions .",
      "Causal inference is essential for many problems in video understanding , e.g. , online action detection [ 3 8 , 4 0 ] , future action label prediction [ 2 0 ] , and future representation prediction [ 4 7 ] .",
      "Preserving temporal resolution , in opposition , is essential in problems such where we needs predictions to be made on each frame of input clip while reasoning about temporal context , e.g. bounding box regression on each frame for action tube detection [ 9 , 3 8 ] or temporal label prediction on each frame for temporal action segmentation [ 3 4 , 3 0 ] or online video segmentation [ 5 4 ] .",
      "Hidden state models , such as Markov ones [ 1 ] , recurrent neural networks ( RNN ) [ 1 5 , 8 ] , and long short - term memory ( LSTM ) networks [ 1 3 ] can all be used to model temporal dynamics in videos [ 7 , 2 8 ] , allowing flexible temporal reasoning .",
      "In an approach which aims to combine the representation power of explicit dynamical models with the discriminative power of 3D networks , in this work we propose a recurrent alternative to 3D convolution illustrated in Figure 1 ( c ) . [ 3 ] or C 3 D [ 4 4 ] . ( b ) 3D convolution decomposed into a 2D spatial convolution followed by a 1D temporal one , as in S 3 D [ 5 2 ] .",
      "Our proposed method , in opposition , solves both problems via a recurrent convolutional network ( RCN ) which explictly performs temporal reasoning at each level of the network thanks to recurrence , while maintaining temporal resolution and being causal , without decline in performance .",
      "Famously , when Tran et al. [ 4 4 ] first proposed 3D CNNs for video action recognition their observed performance turned out to be merely comparable to that of 2D CNNs [ 3 5 ] , e.g. , on the Sports 1 M dataset [ 1 7 ] .",
      "For these reasons , Carreira et al. [ 3 ] later proposed to use transfer learning to boost 3D CNN performance .",
      "There , 2D CNNs are inflated into 3D ones by replacing 2D convolutions with 3D convolution : as a result , 2D network weights as pretrained on ImageNet [ 6 ] can be used to initialise their 3D CNNs .",
      "This makes the use of 3D CNNs more widely accessible , for training a full 3D CNN is a computationally expensive task : 6 4 GPUs were used to train the latest state - of - the - art 3D CNNs [ 3 , 4 5 , 2 ] , which is a big ask for smaller research groups .",
      "Unlike Tran et al. [ 4 5 ] , where the number of filters changes , our recurrent convolutional network exhibits similar performance improvement gains when it comes to ImageNet initialisation as those of inflated 3D CNNs ( I 3 D ) [ 3 ] .",
      "Interestingly , Le et al. [ 2 2 ] show that simple RNNs can exhibit long - term memory properties if appropriately initialised , even better than LSTMs .",
      "They use ReLU [ 1 0 ] activation functions because of their fast convergence and sparsity properties [ 1 0 ] , as opposed to what happens with traditional RNNs , and in line with standard practice in CNNs .",
      "Contributions : In summary , we present a new approach to video feature extraction based on an original convolutional network with recurrent hidden states at each depth level , which : • allows flexible temporal reasoning , exploiting information coming from all the input sequence observed up to time t ; • generates output representations in a causal way , allowing online video processing and enabling the use of 3D networks in scenarios in which causality is key ; • preserves temporal resolution to produce predictions for each frame , e.g. segmentation [ 3 0 , 3 4 , 5 4 ] . • is designed to directly benefit from model initialisation via ImageNet pre - trained weights , as opposed to state of the art approaches , and in line with clear emerging trends in the field .",
      "In our experiments we show that our proposed RCN outperforms baseline I 3 D models , while displying all the above desirable properties .",
      "Since the two - stream 2D CNNs proposed by Simonyan et al. [ 3 5 ] produced performances comparable to that of traditional features such as IDT [ 4 8 , 4 9 ] , HOG [ 4 ] , HOG 3 D [ 1 9 ] , HOF [ 5 ] , 2D features has been extensively used in action recognition and detection .",
      "For instance , Donahue used LSTMs [ 7 , 5 5 ] on top of 2D CNN features .",
      "Other approaches include , among others , CNN features in combination with LSTMs [ 2 4 ] for temporal action detection , 2D features used in an encoderdecoder setup along with temporal convolutions [ 3 0 ] , and conditional random field on series of 2D features [ 3 3 ] for temporal action detection and recognition .",
      "Later , the 3D CNNs proposed by Ji et al. [ 1 4 ] and Tran et al. [ 1 4 , 4 4 ] ( C 3 D architecture ) promised to be able to perform spatial and temporal reasoning at the same time .",
      "Carreira et al. [ 3 ] thus proposed to address both problems by inflating 2D CNNs into 3D CNNs .",
      "They used the weights of 2D CNNs pre - trained on ImageNet [ 6 ] to initialise 3D networks , and trained the latter on the large scale Kinetics dataset [ 1 8 ] .",
      "This inspired [ 5 2 , 2 9 , 4 5 ] to decompose 3D convolutions into 2D ( spatial ) and 1D ( temporal ) convolutions .",
      "Also , temporal resolution is not preserved in temporal convolutions with strides : to address this problem , Shou et al. [ 3 2 ] uses temporal deconvolution layers on top of C 3 D network [ 4 4 ] to produce oneto - one mapping from input frames to coorsponding framelevel label prediction for temporal action detection .",
      "We propose to solve all the above described problems with 3D CNNs ( causality , long - term dependencies , temporal resolution ) thanks to our proposed Recurrent Convolutional Network ( RCN ) .",
      "In addition , our RCN uses fewer parameters compared to any existing 3D CNN architecture but still perform better than I 3 D networks .",
      "Recurrent convolutions have indeed been tried for image generation [ 1 6 , 2 6 ] , scene labeling [ 2 7 ] , and scene text recognition [ 3 1 ] and video reperesnetations [ 5 3 ] .",
      "In particular , the convolutional LSTM ( C - LSTM ) proposed in [ 5 3 ] for precipitation forecasting is closely related to our work .",
      "The authors proposed to use a network made of convolutional LSTMs , whereas we use 2D convolutions for spatial reasoning and an additional convolution , applied in a recurrent fashion , for temporal reasoning .",
      "C - LSTM has been applied to videos [ 2 3 ] to capture spatial attention over time on top of 2D feature maps .",
      "Our Recurrent Convolutional Network exploits both these benefits , along with the 3D CNN philosophy and wisdom ."
    ],
    "relations": [
      [
        "CNNs:Method",
        "Synonym-Of",
        "convolutional neural networks:Method"
      ],
      [
        "2D CNNs:Method",
        "Part-Of",
        "convolutional neural networks:Method"
      ],
      [
        "temporal convolution kernel:Method",
        "Part-Of",
        "3D CNNs:Method"
      ],
      [
        "RCN:Method",
        "Synonym-Of",
        "Recurrent Convolutional Network:Method"
      ],
      [
        "3D CNNs:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "3D CNNs:Method",
        "Evaluated-With",
        "MultiThumos:Dataset"
      ],
      [
        "CNN:Method",
        "Synonym-Of",
        "Convolutional neural networks:Method"
      ],
      [
        "Convolutional neural networks:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "Convolutional neural networks:Method",
        "Used-For",
        "image recognition:Task"
      ],
      [
        "ResNets:Method",
        "Synonym-Of",
        "2D residual networks:Method"
      ],
      [
        "1D ( temporal ) convolutions:Method",
        "Part-Of",
        "3D convolutions:Method"
      ],
      [
        "3D convolutions:Method",
        "Part-Of",
        "S 3 D:Method"
      ],
      [
        "1D ( temporal ) convolutions:Method",
        "Part-Of",
        "S 3 D:Method"
      ],
      [
        "3D convolutions:Method",
        "Compare-With",
        "3D ( spatiotemporal ) convolutions:Method"
      ],
      [
        "Causal inference:Method",
        "Used-For",
        "video understanding:Task"
      ],
      [
        "online action detection:Task",
        "SubTask-Of",
        "video understanding:Task"
      ],
      [
        "future action label prediction:Task",
        "SubTask-Of",
        "video understanding:Task"
      ],
      [
        "future representation prediction:Task",
        "SubTask-Of",
        "video understanding:Task"
      ],
      [
        "Causal inference:Method",
        "Used-For",
        "online action detection:Task"
      ],
      [
        "Causal inference:Method",
        "Used-For",
        "future action label prediction:Task"
      ],
      [
        "Causal inference:Method",
        "Used-For",
        "future representation prediction:Task"
      ],
      [
        "bounding box regression:Task",
        "Used-For",
        "action tube detection:Task"
      ],
      [
        "temporal label prediction:Task",
        "Used-For",
        "temporal action segmentation:Task"
      ],
      [
        "temporal label prediction:Task",
        "Used-For",
        "online video segmentation:Task"
      ],
      [
        "RNN:Method",
        "Synonym-Of",
        "recurrent neural networks:Method"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long short - term memory:Method"
      ],
      [
        "C 3 D:Method",
        "Synonym-Of",
        "3D convolution:Method"
      ],
      [
        "2D spatial convolution:Method",
        "Part-Of",
        "3D convolution:Method"
      ],
      [
        "1D temporal:Method",
        "Part-Of",
        "3D convolution:Method"
      ],
      [
        "1D temporal:Method",
        "Part-Of",
        "S 3 D:Method"
      ],
      [
        "3D convolution:Method",
        "Part-Of",
        "S 3 D:Method"
      ],
      [
        "2D spatial convolution:Method",
        "Part-Of",
        "S 3 D:Method"
      ],
      [
        "RCN:Method",
        "Synonym-Of",
        "recurrent convolutional network:Method"
      ],
      [
        "3D CNNs:Method",
        "Used-For",
        "video action recognition:Task"
      ],
      [
        "2D CNNs:Method",
        "Used-For",
        "video action recognition:Task"
      ],
      [
        "Sports 1 M:Dataset",
        "Benchmark-For",
        "video action recognition:Task"
      ],
      [
        "3D CNNs:Method",
        "Compare-With",
        "2D CNNs:Method"
      ],
      [
        "3D CNNs:Method",
        "Evaluated-With",
        "Sports 1 M:Dataset"
      ],
      [
        "2D CNNs:Method",
        "Evaluated-With",
        "Sports 1 M:Dataset"
      ],
      [
        "transfer learning:Task",
        "Used-For",
        "3D CNN:Method"
      ],
      [
        "3D convolution:Method",
        "Part-Of",
        "2D CNNs:Method"
      ],
      [
        "3D CNNs:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "inflated 3D CNNs:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "recurrent convolutional network:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "I 3 D:Method",
        "Synonym-Of",
        "inflated 3D CNNs:Method"
      ],
      [
        "RNNs:Method",
        "Compare-With",
        "LSTMs:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "2D CNNs:Method",
        "Compare-With",
        "IDT:Method"
      ],
      [
        "2D CNNs:Method",
        "Compare-With",
        "HOG:Method"
      ],
      [
        "2D CNNs:Method",
        "Compare-With",
        "HOG 3 D:Method"
      ],
      [
        "2D CNNs:Method",
        "Compare-With",
        "HOF:Method"
      ],
      [
        "2D CNNs:Method",
        "Compare-With",
        "2D features:Method"
      ],
      [
        "2D CNNs:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "IDT:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "HOG:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "HOG 3 D:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "HOF:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "2D features:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "2D CNNs:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "IDT:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "HOG:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "HOG 3 D:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "HOF:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "2D features:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "2D CNN:Method",
        "Part-Of",
        "LSTMs:Method"
      ],
      [
        "CNN features:Method",
        "Part-Of",
        "LSTMs:Method"
      ],
      [
        "LSTMs:Method",
        "Used-For",
        "temporal action detection:Task"
      ],
      [
        "2D features:Method",
        "Part-Of",
        "encoderdecoder:Method"
      ],
      [
        "temporal convolutions:Method",
        "Part-Of",
        "encoderdecoder:Method"
      ],
      [
        "2D features:Method",
        "Part-Of",
        "conditional random field:Method"
      ],
      [
        "conditional random field:Method",
        "Used-For",
        "temporal action detection:Task"
      ],
      [
        "encoderdecoder:Method",
        "Used-For",
        "temporal action detection:Task"
      ],
      [
        "conditional random field:Method",
        "Used-For",
        "temporal action detection:Task"
      ]
    ]
  },
  {
    "doc_id": "53719258",
    "chunk_id": 2,
    "content": [
      "There are two main reason why 3D CNNs [ 3 , 4 5 , 5 2 , 5 1 ] evolved from 2D CNNs [ 3 6 , 5 0 ] perform better than 3D CNNs built from scratch [ 4 4 , 1 4 ] .",
      "Firstly , 2D CNNs are well tried and tested on the problem of image recognition and a video is , after all , a sequence of images -hence , transferability makes sense .",
      "In this section we recall the two basic types of 3D CNNs that can be built using a 2D CNN architecture .",
      "Usually , the kernel 's temporal dimension n is set to be equal to the spatial dimension d , as in the inflated 3D network ( I 3 D ) [ 3 ] or the convolutional 3D network ( C 3 D ) [ 4 4 ] .",
      "Here , we inflate the 1 8 layer ResNet [ 1 2 ] network into an I 3 D one as shown in Table 1 , where each 2D convolution is inflated into a 3D convolution .",
      "Similarly to the I 3 D network in [ 3 ] , a convolutional layer is used for classification , instead of the fully connected layer used in [ 4 5 , 5 1 ] .",
      "Usually , the size of the temporal kernel t is set to be equal to its spatial dimension d , as in both I 3 D [ 3 ] and C 3 D [ 4 4 ] .",
      "Such a separated convolutional network ( S 3 D ) was introduced by Xi et al. [ 5 2 ] .",
      "After taking a closer look at 3D convolution separation , Tran et al. [ 4 5 ] argued that if the number of kernels M i used in spatial convolution ( Figure   1(b ) ) are increased in such way that the numbers of parameters of spatial and temporal convolution combined are equal to the number of parameters in 3D convolution , then performance actually improves over 3D networks .",
      "Although the latter can be considered a special case of Pseudo - 3 D networks ( P 3 D ) [ 2 9 ] models , because of its homogeneity and simplicity the ( 2 + 1 )D model performs better than P 3 D .",
      "We re - implemented ( 2 + 1 )D without ImageNet initialisation as an additional baseline as it has the most promising result without any additional trick like gating in S 3 D .",
      "In this section , we describe the architecture of our Recurrent Convolutional ( 3D ) Network ( RCN ) and its properties in detail .",
      "Firstly , we show how Recurrent Convolutional Units ( RCU ) A pictorial diagram of our proposed recurrent convolutional unit ( RCU ) in Figure 1(c ) .",
      "Analytically , a recurrent convolutional unit can be described by the following relation : where w hh and w xh are parameters of the RCU , and * represents the convolution operator .",
      "Figure 2 represents a simple recurrent convolutional network ( RCN ) composed by a single RCU unit , unrolled up to time t. At each time step t , an input x t is processed by the RCU and the other layers to produce an output y t .",
      "The unrolling principle allows us to build an RCN from 2D/ 3 D networks , e.g. by replacing 3D convolutions with RCUs in any I 3 D network .",
      "Indeed , the network architecture of our proposed model builds on the I 3 D network architecture shown in Table 1 , where the same parameters ( d , number of kernels ) used for 3D convolutions are used for our RCU .",
      "Unlike I 3 D , however , our RCN does not require a temporal convolution size n ( cfr .",
      "As in 2D or I 3 D ResNet models [ 1 2 , 4 5 , 1 1 ] , our proposed RCN also has residual connections .",
      "The initial hidden state h 0 , as shown in Figure 2 , is initialised by the output of the bottom 2D convolution layer at t 0 .",
      "The hidden state h t at time t is considered to be the output at that time instant -as such , it acts as input to next hidden state and to the whole next depth - level layer .   The output sizes for both I 3 D and our proposed RCN are shown in Table 1 .",
      "Our RCN only uses spatial pooling and a convolutional layer for classification , unlike the spatiotemporal pooling of [ 3 , 5 1 , 4 5 ] .",
      "From Table 1 , compared to I 3 D , RCN produces 1 6 classification score vectors with an input sequence length of 1 6 .",
      "This one - to - one mapping from input to output is essential in many tasks , ranging from temporal action segmentation [ 3 2 , 3 0 ] , to temporal action detection [ 3 7 ] , to action tube detection [ 3 8 ] .",
      "Unlike the temporal deconvolution proposed in [ 3 2 , 3 0 ] , our RCN inherently addresses this problem ( see Table 1 ) .",
      "Thus our RCN as presented here is not only causal , but poses no constraints on the modelling of temporal dependencies ( as opposed to the upper bound of n in the case of temporal convolutions ) .",
      "The I 3 D model proposed by Carreira et al. [ 3 ] greatly owes its success to a good initialisation from 2D models trained on ImageNet [ 6 ] .",
      "It is noteworthy that the other state - of - the - art ( 2 + 1 )D model by Tran et al. [ 4 5 ] can not , instead , exploit ImageNet initialisation , because of the change in the number of kernels .",
      "In response to a similar issue , Le et al. [ 2 2 ] presented a simple way to initialise RNNs when used with ReLU [ 1 0 ] activation functions .",
      "In this section , we evaluate our recurrent convolutional network on the challenging Kinetics [ 1 8 ] and UCF 1 0 1 [ 4 1 ] datasets to study its various original features and compare it with state - of - the - art 3D CNN models .",
      "Kinetics has become a defacto benchmark for recent action recognition works [ 2 , 5 2 , 4 5 , 3 ] .",
      "UCF 1 0 1 dataset has 1 0 1 classes and 1 3 K videos ; nowadays , it is used to evaluate the action recognition [ 4 1 ] and transfer learning [ 3 ] capabilities of 3D CNNs .",
      "E.g. , the ResNet - 1 8 - based I 3 D model trained by Tran et al. [ 4 5 ] is 1 0 % better in terms of final video accuracy as compared to a similar model trained by Hara et al. [ 1 1 ] .",
      "Another important aspect of the training process is the presence of various input data argumentation operations , e.g. , random crop , horizontal flip , image intensities jittering and temporal jittering .",
      "Therefore we worked to reproduce the results in [ 4 4 ] for I 3 D and their proposed ( 2 + 1 )D model using our training setup for fair comparison , as shown in Tables 2 and 3 .",
      "We used 8 or 1 6 frames long RGB clips to train our RCN and baseline I 3 D and ( 2 + 1 )D , models ."
    ],
    "relations": [
      [
        "conditional random field:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "encoderdecoder:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "conditional random field:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "2D CNNs:Method",
        "Part-Of",
        "3D CNNs:Method"
      ],
      [
        "2D CNNs:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "2D CNNs:Method",
        "Part-Of",
        "3D networks:Method"
      ],
      [
        "3D networks:Method",
        "Trained-With",
        "Kinetics:Dataset"
      ],
      [
        "1D ( temporal ) convolutions:Method",
        "Part-Of",
        "3D convolutions:Method"
      ],
      [
        "temporal deconvolution:Method",
        "Part-Of",
        "C 3 D:Method"
      ],
      [
        "C 3 D:Method",
        "Used-For",
        "temporal action detection:Task"
      ],
      [
        "3D CNNs:Method",
        "Used-For",
        "causality:Task"
      ],
      [
        "3D CNNs:Method",
        "Used-For",
        "long - term dependencies:Task"
      ],
      [
        "3D CNNs:Method",
        "Used-For",
        "temporal resolution:Task"
      ],
      [
        "RCN:Method",
        "Synonym-Of",
        "Recurrent Convolutional Network:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "3D CNN:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "Recurrent convolutions:Method",
        "Used-For",
        "image generation:Task"
      ],
      [
        "Recurrent convolutions:Method",
        "Used-For",
        "scene labeling:Task"
      ],
      [
        "Recurrent convolutions:Method",
        "Used-For",
        "scene text recognition:Task"
      ],
      [
        "Recurrent convolutions:Method",
        "Used-For",
        "video reperesnetations:Task"
      ],
      [
        "C - LSTM:Method",
        "Synonym-Of",
        "convolutional LSTM:Method"
      ],
      [
        "C - LSTM:Method",
        "Used-For",
        "spatial attention:Method"
      ],
      [
        "3D CNN:Method",
        "Part-Of",
        "Recurrent Convolutional Network:Method"
      ],
      [
        "2D CNNs:Method",
        "Used-For",
        "image recognition:Task"
      ],
      [
        "2D CNN:Method",
        "Part-Of",
        "3D CNNs:Method"
      ],
      [
        "I 3 D:Method",
        "Synonym-Of",
        "inflated 3D network:Method"
      ],
      [
        "C 3 D:Method",
        "Synonym-Of",
        "convolutional 3D network:Method"
      ],
      [
        "ResNet:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "2D convolution:Method",
        "Part-Of",
        "3D convolution:Method"
      ],
      [
        "convolutional layer:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "convolutional layer:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "I 3 D:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "S 3 D:Method",
        "Synonym-Of",
        "separated convolutional network:Method"
      ],
      [
        "spatial convolution:Method",
        "Part-Of",
        "3D convolution:Method"
      ],
      [
        "temporal convolution:Method",
        "Part-Of",
        "3D convolution:Method"
      ],
      [
        "P 3 D:Method",
        "Synonym-Of",
        "Pseudo - 3 D networks:Method"
      ],
      [
        "( 2 + 1 )D:Method",
        "Compare-With",
        "P 3 D:Method"
      ],
      [
        "RCN:Method",
        "Synonym-Of",
        "Recurrent Convolutional ( 3D ) Network:Method"
      ],
      [
        "RCU:Method",
        "Synonym-Of",
        "Recurrent Convolutional Units:Method"
      ],
      [
        "RCU:Method",
        "Synonym-Of",
        "recurrent convolutional unit:Method"
      ],
      [
        "convolution operator:Method",
        "Part-Of",
        "RCU:Method"
      ],
      [
        "RCN:Method",
        "Synonym-Of",
        "recurrent convolutional network:Method"
      ],
      [
        "RCU unit:Method",
        "Part-Of",
        "recurrent convolutional network:Method"
      ],
      [
        "2D/ 3 D networks:Method",
        "Used-For",
        "RCN:Method"
      ],
      [
        "RCUs:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "3D convolutions:Method",
        "Part-Of",
        "RCU:Method"
      ],
      [
        "I 3 D:Method",
        "Compare-With",
        "RCN:Method"
      ],
      [
        "residual connections:Method",
        "Part-Of",
        "I 3 D ResNet:Method"
      ],
      [
        "residual connections:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "spatial pooling:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "convolutional layer:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "RCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "I 3 D:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "ReLU:Method",
        "Part-Of",
        "RNNs:Method"
      ],
      [
        "recurrent convolutional network:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "3D CNN:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "recurrent convolutional network:Method",
        "Evaluated-With",
        "UCF 1 0 1:Dataset"
      ],
      [
        "3D CNN:Method",
        "Evaluated-With",
        "UCF 1 0 1:Dataset"
      ],
      [
        "recurrent convolutional network:Method",
        "Compare-With",
        "3D CNN:Method"
      ],
      [
        "Kinetics:Dataset",
        "Benchmark-For",
        "action recognition:Task"
      ],
      [
        "3D CNNs:Method",
        "Evaluated-With",
        "UCF 1 0 1:Dataset"
      ],
      [
        "UCF 1 0 1:Dataset",
        "Benchmark-For",
        "action recognition:Task"
      ],
      [
        "3D CNNs:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "UCF 1 0 1:Dataset",
        "Benchmark-For",
        "transfer learning:Task"
      ],
      [
        "3D CNNs:Method",
        "Used-For",
        "transfer learning:Task"
      ],
      [
        "ResNet - 1 8:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "random crop:Method",
        "SubClass-Of",
        "data argumentation operations:Method"
      ],
      [
        "horizontal flip:Method",
        "SubClass-Of",
        "data argumentation operations:Method"
      ],
      [
        "image intensities jittering:Method",
        "SubClass-Of",
        "data argumentation operations:Method"
      ],
      [
        "temporal jittering:Method",
        "SubClass-Of",
        "data argumentation operations:Method"
      ],
      [
        "random crop:Method",
        "SubClass-Of",
        "data augmentation:Method"
      ],
      [
        "horizontal flip:Method",
        "SubClass-Of",
        "data augmentation:Method"
      ]
    ]
  },
  {
    "doc_id": "53719258",
    "chunk_id": 3,
    "content": [
      "As for data augmentation , we used random crop and horizontal flip with 5 0 % probability , along with mean subtraction and normalisation with standard deviation .",
      "As mentioned , we re - implemented the I 3 D and ( 2 + 1 )D models using ResNet - 1 8 and ResNet - 3 4 as a backbone .",
      "The Resnet - 1 8 - I 3 D architecture is presented in Table 1 .",
      "Clip - level and video - level action recognition accuracy on the validation set of the Kinetics dataset for different ResNet - 1 8 - based models , trained using 8 - frame - long clips as input .",
      "Comparison of our RCN with state - of - the - art I 3 D and ( 2 + 1 )D models on the validation set of Kinetics using a ResNet - 3 4 architecture trained using 1 6 - frame - long clips . we matched the number of parameters of separated convolutions to that of standard 3D convolutions , as explained in [ 4 5 ] .",
      "The results of the I 3 D and ( 2 + 1 )D implementations reported in Tran et al. [ 4 4 ] are shown in the top half of Table 2 .",
      "The number of parameters in our proposed RCN model is 1 2 . 8 million ( M ) , as opposed to 3 3 . 4 M in both the I 3 D and ( 2 + 1 )D models , see Table 2 .",
      "It is remarkable to see that , despite a 2. 6 times reduction in the number of parameters , RCN still outperforms both I 3 D and ( 2 + 1 )D when trained using ImageNet initialisation .",
      "Further , RCN surpasses I 3 D also under random initialisation , while using 2. 6 times fewer model parameters .",
      "Similarly , Table 3 shows that RCN outperforms both I 3 D and ( 2 + 1 )D models , when the base model is ResNet - 3 4 and the input clip length is 1 6 , again , while using far fewer parameters .",
      "ImageNet initialisation proves to be useful for both the I 3 D and our RCN models .",
      "While ( 2 + 1 )D performs ( Table 2 , row 7 ) better than RCN ( row 6 ) with random initialisation , our RCN recovers to improve over ( 2 + 1 )D ( row 7 ) with ImageNet initialisation , whereas ( 2 + 1 )D can not make use of free ImageNet initialisation .",
      "This seems to be a severe drawback for the ( 2 + 1 )D model , and a big advantage for I 3 D and RCN .",
      "One may argue that , if the purpose is to build on existing 2D models , then RCN and I 3 D are a better choice , whereas if new 3D models are preferred then ( 2 + 1 )D might prove useful .",
      "A comparison with other 3D causal networks is a must , as we claim the causal nature of the network to be one of the main contributions of our work , making RCN best suited to online applications such as action detection and prediction .",
      "Their sequential version of I 3 D , however , shows a drop in performance as compared to I 3 D as seen in lines 1 and 2 of Table 4 .",
      "Clip - length Acc% As to our new model , we can show that CRN consistently outperforms I 3 D in all the considered settings , namely : when using ResNet - 1 8 as base models , input clip size equal to 8 , with or without ImageNet initialisation ( cf .",
      "It is fair to say that our CRN network is the best performing causal network out there when compared with the corresponding I 3 D version .",
      "The S 3 Dg model has a gating operation applied to the outputs of Separated 3D ( S 3 D ) [ 5 2 ] .",
      "Video - level action classification accuracy of different models on the validation set of the Kinetics dataset .",
      "Non - Local ( NL ) operations as proposed by Wang et al. [ 5 1 ] also can be proved to improve performance over the base I 3 D model , as shown in the second part of Table 5 .",
      "What needs to be remarked is that gating and NL operations are not at all constrained to be applied on top of I 3 D or S 3 D models : indeed , they can also be used in conjunction with ( 2 + 1 )D and our own RCN model .",
      "As in this work we focus on comparing our network with other 3D models , we chose I 3 D and ( 2 + 1 )D as baselines ( Sec. 5. 2 ) .",
      "From the last three rows of Table 5 , we can sensibly speculate that , as our RCN performs better than those 3D models , the application of further gating and NL layers is likely to lead to performances superior to those of [ 5 2 ] and [ 5 1 ] .",
      "Among the latter , ResNet 1 0 1 - I 3 D - NL [ 5 1 ] is shown to work better with an even longer input clip length .",
      "In our experiments , as mentioned , we stuck to 1 6 - frame clips as input and compared our proposed RCN with baseline I 3 D models ( bottom rows of Table 5 ) .",
      "The last three rows of Table 6 shows the performance of baseline I 3 D , ( 2 + 1 )D and RCN models on UCF 1 0 1 .",
      "As on Kinetics , our RCN ( last row ) outperforms our baseline implementation of I 3 D and ( 2 + 1 )D. The second and third part of Table 6 shows the state - ofthe - art results achieved by the S 3 Dg [ 5 2 ] and ( 2 + 1 )D models as implemented in [ 4 5 ] Table 7 .",
      "Video - level action recognition accuracy on the Kinetics validation set for different ResNet - 1 8 - based RCN models . 6 .",
      "Discussion Two basic things are clear from our experience with training heavy 3D models ( I 3 D , ( 2 + 1 )D , RCN ) on largescale datasets such as Kinetics .",
      "In the case of both I 3 D and RCN , ImageNet initialisation improves the video classification accuracy on Kinetics by almost 3% compared to random initialisation when using the same number of training iterations , as shown in the first and last row of Table 7 .",
      "The bottom line is that we should strive for more efficient implementations of 3D models for the sake of their adoption .   To conclude , it is interesting to take a closer look at the statistics for the weight matrices ( w hh ) associated with the hidden state at every RCU layer in the RCN network .",
      "We showed that ImageNet - based initialisation is at the heart of the success of 3D CNNs .",
      "The causal nature of our recurrent 3D convolutional network opens up manifold research directions , with direct and promising potential application in areas such as online action detection and future event/action prediction ."
    ],
    "relations": [
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "ResNet - 1 8:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "( 2 + 1 )D:Method"
      ],
      [
        "ResNet - 1 8:Method",
        "Part-Of",
        "( 2 + 1 )D:Method"
      ],
      [
        "ResNet - 1 8 - based models:Method",
        "Used-For",
        "action recognition:Task"
      ],
      [
        "Kinetics:Dataset",
        "Benchmark-For",
        "action recognition:Task"
      ],
      [
        "ResNet - 1 8 - based models:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "( 2 + 1 )D:Method"
      ],
      [
        "RCN:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "I 3 D:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "( 2 + 1 )D:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "RCN:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "I 3 D:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "( 2 + 1 )D:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "ResNet - 3 4:Method",
        "Part-Of",
        "( 2 + 1 )D:Method"
      ],
      [
        "I 3 D:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "RCN:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "( 2 + 1 )D:Method",
        "Compare-With",
        "RCN:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "RCN:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "( 2 + 1 )D:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "I 3 D:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D:Method"
      ],
      [
        "RCN:Method",
        "Used-For",
        "action detection:Task"
      ],
      [
        "RCN:Method",
        "Used-For",
        "prediction:Task"
      ],
      [
        "ResNet - 1 8:Method",
        "Part-Of",
        "CRN:Method"
      ],
      [
        "CRN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "ResNet - 1 8:Method",
        "Part-Of",
        "I 3 D:Method"
      ],
      [
        "CRN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "gating operation:Method",
        "Part-Of",
        "S 3 Dg:Method"
      ],
      [
        "Separated 3D:Method",
        "Part-Of",
        "S 3 Dg:Method"
      ],
      [
        "S 3 D:Method",
        "Synonym-Of",
        "Separated 3D:Method"
      ],
      [
        "Kinetics:Dataset",
        "Benchmark-For",
        "Video - level action classification:Task"
      ],
      [
        "NL:Method",
        "Synonym-Of",
        "Non - Local:Method"
      ],
      [
        "Non - Local:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "NL:Method",
        "Part-Of",
        "( 2 + 1 )D:Method"
      ],
      [
        "NL:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "I 3 D:Method",
        "Evaluated-With",
        "UCF 1 0 1:Dataset"
      ],
      [
        "( 2 + 1 )D:Method",
        "Evaluated-With",
        "UCF 1 0 1:Dataset"
      ],
      [
        "RCN:Method",
        "Evaluated-With",
        "UCF 1 0 1:Dataset"
      ],
      [
        "RCN:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "I 3 D:Method"
      ],
      [
        "RCN:Method",
        "Compare-With",
        "( 2 + 1 )D.:Method"
      ],
      [
        "Kinetics:Dataset",
        "Benchmark-For",
        "Video - level action recognition:Task"
      ],
      [
        "RCN:Method",
        "Used-For",
        "Video - level action recognition:Task"
      ],
      [
        "RCN:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "ResNet - 1 8:Method",
        "Part-Of",
        "RCN:Method"
      ],
      [
        "RCN:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "( 2 + 1 )D:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "I 3 D:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "RCN:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "I 3 D:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "I 3 D:Method",
        "Used-For",
        "video classification:Task"
      ],
      [
        "RCN:Method",
        "Used-For",
        "video classification:Task"
      ],
      [
        "I 3 D:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "RCN:Method",
        "Evaluated-With",
        "Kinetics:Dataset"
      ],
      [
        "3D CNNs:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "3D convolutional network:Method",
        "Used-For",
        "online action detection:Task"
      ],
      [
        "3D convolutional network:Method",
        "Used-For",
        "event/action prediction:Task"
      ]
    ]
  },
  {
    "doc_id": "2032742",
    "chunk_id": 1,
    "content": [
      "Phenomenally successful in practical inference problems , convolutional neural networks ( CNN ) are widely deployed in mobile devices , data centers , and even supercomputers .",
      "While pruning the fully connected layers reduces a CNN 's size considerably , it does not improve inference speed noticeably as the compute heavy parts lie in convolutions .",
      "Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures , thus limiting the achievable sparsity levels .    We present a method to realize simultaneously size economy and speed improvement while pruning CNNs .",
      "Deep neural networks , especially CNNs , have been pervasive in computer vision recently and served as a foundation for many critical applications ranging from image recognition [ 1 7 ] and video analytics [ 2 1 ] to autonomous driving [ 1 ] .",
      "The trained CNN models are deployed broadly for classification on a large variety of platforms , covering the spectrum of data center servers to mobile clients such as smart phones and autonomous - driving cars .",
      "When the trained CNN models perform classification on these platforms , especially on resource constraint mobile platforms , accuracy , ( classification ) speed , and model size are the three key requirements that together form the threepronged \" golden trident \" .",
      "Exploiting or imposing sparsity in CNNs [ 1 2 , 1 1 , 7 , 1 9 , 8 , 1 4 ] have been recently studied to reduce model size and/or accelerate classification speed with minimal or zero accuracy loss .",
      "One research thrust [ 1 2 , 1 1 , 7 ] of sparse CNN has been focusing on reducing model size by sparsifying/compressing fully connected layers that have most of the parameters of the entire CNN model .",
      "However , such designs provide limited benefits to the classification speed of CNN , because the majority of the computation , i.e. , FLOPS , are within the convolution layers instead .",
      "For example , in AlexNet , while fully connected layers comprise more than 9 0 % of the total model size , it is the convolution layers that comprise more than 9 0 % of the total computation .",
      "Therefore , another research thrust [ 1 9 , 8 , 1 4 ] of sparse CNN focuses on convolution layers to reduce compute requirements and thus to improve classification speed .",
      "However , there are challenges for sparse CNN to achieve its full potential , including 1 ) sparse convolution has lower arithmetic intensity , thus additional data transfers involved in lowering tensors to matrices used in [ 1 9 , 8 , 1 4 ] add higher overhead , 2 ) some prior sparse convolution methods [ 8 , 1 4 ] are only applicable to large filters ( e.g. , 9 × 9 ) , which limits their usage for modern CNNs where small filters ( e.g. , 3 × 3 ) are popular .",
      "Moreover , while current sparse CNN methods focus mostly on either fully connected layers or convolution layers , it is desirable to apply sparse methods to both layers simultaneously .",
      "We also provide guidelines for the range of sparsity that should be targeted during training . • A highly - optimized sparse CNN implementation that provides 3. 4 × and 7. 3 × speedups of convolution layers in AlexNet over the best dense direct convolution performance on Xeon and Atom processors , respectively , with no accuracy drop .",
      "We also show 2, 3 7 1 and 1 2 0 images per second AlexNet classification throughput , which are significantly higher than the best published performance on respective platforms .",
      "Sparse methods have been popular to reduce model size and accelerate classification speed of CNNs with minimal or zero accuracy loss ."
    ],
    "relations": [
      [
        "CNN:Method",
        "Synonym-Of",
        "convolutional neural networks:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "CNNs:Method",
        "SubClass-Of",
        "Deep neural networks:Method"
      ],
      [
        "Deep neural networks:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "image recognition:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "video analytics:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "autonomous driving:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "sparse CNN:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "sparse CNN:Method"
      ],
      [
        "sparse CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "sparse convolution:Method",
        "Part-Of",
        "sparse CNN:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "sparse CNN:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "sparse CNN:Method"
      ]
    ]
  },
  {
    "doc_id": "2032742",
    "chunk_id": 2,
    "content": [
      "However , the speedup of end - to - end performance is limited , because the fully connected layers only account for less than 1 0 % total computation in most CNNs such as AlexNet [ 1 7 ] .",
      "Another research thrust [ 1 9 , 8 , 1 4 ] of sparse CNN focuses on convolution layers , mostly targeting the classification speed .",
      "However , prior work on sparse convolution involves the overhead of transforming tensors to matrices , and some are inapplicable to small kernel size and thus can not exploit the full potential of the sparse convolution .",
      "While Perforated - CNN [ 9 ] also improves the classification speed of convolution layers by eliminating redundant computations , it relies on interpolating redundant convolution computation instead of using sparse methods .",
      "Therefore , the acceleration due to sparse methods on classification speed of CNNs is not as good as the sparse - method - enabled reduction of model size .",
      "The second challenge is the lack of holistical optimization of speed and size of sparse CNNs ( convolution layers and fully connected layers mostly affect speed and size , respectively ) .",
      "This section first presents our direct sparse convolution algorithm to advance the stateof - the - art in sparse convolution for speeding up CNNs .",
      "Finally , we discuss the synergies between sparse convolution and sparse methods on fully connected layers .",
      "Figure 3 : Ideal speedups of sparse convolution over dense convolution . conv 2 / 4 _direct : direct sparse convolution , conv 2 / 4 _lowered : sparse convolution on tensors lowered to matrices ( see Section 3. 1 . 2 for details ) .",
      "In convolution layers of CNN , an input channel is reused against multiple output channels and vice versa , thus the arithmetic intensity of our direct sparse convolution algorithm is higher than SpMV that is memory bandwidth bound but lower than dense convolution that is compute - bound .",
      "The potential acceleration of sparse convolution over dense convolution can be expressed by where x is the proportion of non - zeros in filters ( lower the x , higher the sparsity of weight tensor W ) , and this proportion directly affects the final speedup .",
      "Therefore , sparse convolution 's actual FLOP/s is lower than that of dense convolution .",
      "Hence , there is a lower bound of useful sparsity such that , with a sparsity lower than that , sparse convolution provides no speedup over dense convolution .",
      "Therefore , in Atom processors , sparse convolution outperform dense convolution in a wider range of sparsity and the speedups are higher as will be shown in Section 5 .",
      "The lowering and lifting process has demonstrated overhead for dense convolution [ 1 0 ] , and it is particularly problematic for sparse convolution with intensity already much lower than its dense counter part .",
      "Even though a bulk of computation belongs to convolution layers , fully connected layers can become a bottleneck after sparse convolution optimizations ."
    ],
    "relations": [
      [
        "convolution layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "AlexNet:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "AlexNet:Method",
        "SubClass-Of",
        "CNNs:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "CNNs:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "sparse CNN:Method"
      ],
      [
        "sparse CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "Perforated - CNN:Method"
      ],
      [
        "Perforated - CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "CNNs:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "CNNs:Method"
      ],
      [
        "sparse convolution:Method",
        "Part-Of",
        "CNNs:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "SpMV:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ]
    ]
  },
  {
    "doc_id": "2032742",
    "chunk_id": 3,
    "content": [
      "Exploiting sparsity in fully connected layers is actually simpler than convolution layers , because fully connected layers are implemented as GEMM and we can leverage work done before on sparse matrix and dense matrix multiplication ( SpMDM ) .",
      "Simultaneously learning the sparsity of convolution and fully connected layers is beneficial in amortizing the training cost and in performing holistic trade - off among accuracy , classification speed , and model size .",
      "However , when targeting no accuracy loss from sparse models , we find that using the same regularization parameters across all layers tends to provide a high sparsity on fully connected ( fc ) layers but not enough sparsity on convolution layers to obtain speedups from sparse convolutions .",
      "Therefore , trading off a small increase in model size of fc layers for a large speedup of convolution layers can be often beneficial for holistically optimizing the accuracy - speed - size of CNN .",
      "The lower convolution layers such as conv 1 in AlexNet are much harder to sparsify than higher layers such as conv 2 and above unless we can tolerate a large accuracy drop .",
      "Specifically , we use weight decay 1e - 5 , 5e - 5 , and 6e - 5 , for FC decay multiplier 1 , 0. 1 , and 0.0 1 , respectively . among accuracy , speed , and model size .",
      "Our Holistic SparseCNN achieves a classification speed of 2, 3 7 1 images per second on BDW , without accuracy drop compared to the reference Caffe model .",
      "With no accuracy loss , our Holistic SparseCNN achieves a classification speed of 1 2 0 images per second , about 2. 8 × higher than a best case dense CNN implementation where all conv and fc layers would perform at the speed of SGEMM .",
      "This provides high sparsity on fc layers but sparsity on conv layers is so low that sparse convolution is slower than dense convolution on BDW .",
      "We use highly optimized SGEMM performance as a proxy of dense convolution performance to quantify layer - wise speedups of our Holistic SparseCNN .",
      "As a centerpiece of deep learning , CNNs are under relentless pressure to be smaller , faster , and with higher evaluation/classification accuracy .",
      "In this paper , we present Holistic SparseCNN , a collection of techniques that pave the road to unleash the full potential of sparse CNNs .",
      "Secondly , unlike prior research focusing on using sparse methods on convolution layers or fully connected layers in isolation , our new cross - layer holistic methodology not only exploits the synergies of sparse methods on convolution layers and fully connected layers but also enables optimizing accuracy , speed , and size together .",
      "Our results on AlexNet demonstrated 3. 4 × and 7. 3 × speedups of convolution layers over the best dense convolution on representative server and mobile platforms as well as the best classification throughputs on these platforms ."
    ],
    "relations": [
      [
        "fully connected layers:Method",
        "Compare-With",
        "convolution layers:Method"
      ],
      [
        "fully connected layers:Method",
        "Part-Of",
        "GEMM:Method"
      ],
      [
        "SpMDM:Method",
        "Synonym-Of",
        "sparse matrix and dense matrix multiplication:Method"
      ],
      [
        "fc:Method",
        "Synonym-Of",
        "fully connected:Method"
      ],
      [
        "fully connected:Method",
        "Compare-With",
        "convolution layers:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "conv 1:Method",
        "SubClass-Of",
        "convolution layers:Method"
      ],
      [
        "conv 1:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "conv 2:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "conv 1:Method",
        "Compare-With",
        "conv 2:Method"
      ],
      [
        "Holistic SparseCNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Holistic SparseCNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Holistic SparseCNN:Method",
        "Compare-With",
        "dense CNN:Method"
      ],
      [
        "sparse convolution:Method",
        "Compare-With",
        "dense convolution:Method"
      ],
      [
        "SGEMM:Method",
        "Part-Of",
        "dense convolution:Method"
      ],
      [
        "CNNs:Method",
        "SubClass-Of",
        "deep learning:Method"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "evaluation/classification:Task"
      ],
      [
        "Holistic SparseCNN:Method",
        "SubClass-Of",
        "sparse CNNs:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "AlexNet:Method",
        "Used-For",
        "classification:Task"
      ]
    ]
  },
  {
    "doc_id": "198967567",
    "chunk_id": 1,
    "content": [
      "Over the past few years , we have witnessed the success of deep learning in image recognition thanks to the availability of large - scale human - annotated datasets such as PASCAL VOC , ImageNet , and COCO .",
      "To evaluate and validate the performance of our approach , we have built a few - shot segmentation dataset , FSS - 1 0 0 0 , which consists of 1 0 0 0 object classes with pixelwise annotation of ground - truth segmentation .",
      "We build our baseline model using standard backbone networks such as VGG - 1 6 , ResNet - 1 0 1 , and Inception .",
      "To our surprise , we found that training our model from scratch using FSS - 1 0 0 0 achieves comparable and even better results than training with weights pre - trained by ImageNet which is more than 1 0 0 times larger than FSS - 1 0 0 0 .",
      "Although unprecedented in the number of object categories when first released , contemporary image datasets for training deep neural networks such as PASCAL VOC [ 5 ] ( 1 9 , 7 4 0 images , 2 0 classes ) , ILSVRC [ 2 8 ] ( 1, 2 8 1 , 1 6 7 images , 1, 0 0 0 classes ) , and COCO [ 2 1 ] ( 2 0 4 , 7 2 1 images , 8 0 classes ) are actually quite limited for visual recognition tasks in the real world : a rough estimate of the number of different objects on the Earth falls in the range of 5 0 0 , 0 0 0 to 7 0 0 , 0 0 0 , following the total number of nouns in the En - * Equal contribution . glish language .",
      "Thus , Few - Shot Learning has emerged as an attractive alternative for important computer vision tasks , especially when the given new dataset is very small and dissimilar so relying on the aforementioned pre - trained weights may not work well .",
      "Previous research on few - shot segmentation relies on a manual split of the PASCAL VOC dataset to train and evaluate a new model [ 3 0 , 2 4 ] , but only 2 0 and 8 0 classes in the PASCAL VOC and COCO datasets respectively contain pixelwise segmentation information .",
      "FSS - 1 0 0 0 is the first large - scale dataset for few - shot segmentation with built - in object category hierarchy which emphasizes the number of object classes rather than the number of images .",
      "All existing datasets are biased toward a number of object categories except FSS - 1 0 0 0 ( red ) . pending a decoder module to the relation network [ 3 3 ] , which is a simple and elegant deep model effective and originally designed for few - shot image classification only .",
      "Reshaping the relation network into a fully - convolutional U - Net architecture [ 2 6 ] , our extensive experimental results show that this baseline model trained from scratch on FSS - 1 0 0 0 , which is less than 1% of the size of contemporary large - scale datasets , outperforms the model fine - tuned from weights pre - trained on ImageNet/COCO dataset .",
      "With its excellent segmentation performance as well as extensibility , FSS - 1 0 0 0 is expected to make a lasting contribution to few - shot image segmentation .",
      "We first review the relationship and difference between FSS - 1 0 0 0 and modern datasets aiming to solve image segmentation and few - shot classification .",
      "Then we review contemporary research on few - shot learning and semantic segmentation and discuss how we relate the few - shot segmentation to previous research .",
      "The PASCAL VOC [ 5 ] was the first to provide a challenging image dataset for object class recognition and semantic segmentation .",
      "The latest version VOC 2 0 1 2 contains 2 0 object classes and 9, 9 9 3 images with segmentation annotations .",
      "Despite the absence of segmentation labels , the Imagenet [ 4 ] is built upon the backbone of WordNet and provides image - level labels for 5, 2 4 7 classes for training , out of which a subset of 1, 0 0 0 categories are split out to form the ILSVRC [ 2 8 ] dataset .",
      "This challenge has made a significant impact on the rapid progress in visual recognition task and computer vision in recent years .",
      "The latest Open Image dataset [ 1 7 ] contains 7, 1 8 6 trainable distinct object classes for classification and 6 0 0 classes for detection , making it the largest existing dataset with object classes and location annotations .",
      "Following the PASCAL VOC and ImageNet , the COCO segmentation dataset [ 2 1 ] includes more than 2 0 0 , 0 0 0 images with instance - wise semantic segmentation labels .",
      "Our FSS - 1 0 0 0 consists of 1, 0 0 0 object classes , wherein each class we label 1 0 images with binary segmentation annotation .",
      "We are particularly interested in segmentation due to its obvious benefits : segmentation captures the essential feature of an object without background ; instance level segmentation can be ready from segmentation .",
      "But none of these few - shot learning datasets incorporate dense pixelwise segmentation labels , which is essential in training a deep network model for semantic segmentation .",
      "Few - Shot Learning Recent research in few - shot classification can be classified into 1 ) learn a good initial condition for the network to be fine - tuned on extremely small training set , as proposed in [ 8 , 2 5 ] ; 2 ) rely on memory properties of RNN , introduced in [ 2 2 , 2 9 ] ; 3 ) learn a metric between few - shot samples and queries , as in [ 2 , 1 0 , 1 8 , 1 6 , 3 3 ] .",
      "In this paper , we simply modify the loss to calculate pixelwise differences between the segmentation ground truth and heatmap .",
      "In OSLSM [ 3 0 ] , the authors proposed a two - branch network to solve few - shot segmentation ."
    ],
    "relations": [
      [
        "deep learning:Method",
        "Used-For",
        "image recognition:Task"
      ],
      [
        "PASCAL VOC:Dataset",
        "Benchmark-For",
        "image recognition:Task"
      ],
      [
        "ImageNet:Dataset",
        "Benchmark-For",
        "image recognition:Task"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "image recognition:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "deep neural networks:Method",
        "Trained-With",
        "PASCAL VOC:Dataset"
      ],
      [
        "deep neural networks:Method",
        "Trained-With",
        "ILSVRC:Dataset"
      ],
      [
        "deep neural networks:Method",
        "Trained-With",
        "COCO:Dataset"
      ],
      [
        "Few - Shot Learning:Task",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "PASCAL VOC:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "fully - convolutional U - Net:Method",
        "Trained-With",
        "FSS - 1 0 0 0:Dataset"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot image segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "image segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot classification:Task"
      ],
      [
        "PASCAL VOC:Dataset",
        "Benchmark-For",
        "object class recognition:Task"
      ],
      [
        "PASCAL VOC:Dataset",
        "Benchmark-For",
        "semantic segmentation:Task"
      ],
      [
        "Open Image:Dataset",
        "Benchmark-For",
        "classification:Task"
      ]
    ]
  },
  {
    "doc_id": "198967567",
    "chunk_id": 2,
    "content": [
      "Semantic Image Segmentation Previous research exploiting CNN to make dense prediction often relied on patchwise training [ 3 , 6 , 2 3 ] and pre - and post - processing of superpixels [ 6 , 1 1 ] .",
      "In [ 3 1 ] the authors first proposed a simple and elegant fully convolutional network ( FCN ) to solve semantic segmentation .",
      "Notably , this is the first work which was trained end - to - end on a fully convolutional network for dense pixel prediction , which showed that the last layer feature maps from a good backbone network such as VGG - 1 6 contain sufficient foreground features which can be decoded by the upsampling network to produce segmentation results .",
      "Recent few - shot datasets [ 1 8 , 3 5 ] support few - shot classification but there is no large - scale few - shot segmentation dataset .",
      "FSS - 1 0 0 0 targets at solving general objects few - shot segmentation problem .",
      "Object Classes We first referred to the classes in ILSVRC [ 2 8 ] in our choice of object categories for FSS - 1 0 0 0 .",
      "Consequently , FSS - 1 0 0 0 has 5 8 4 classes out of its 1, 0 0 0 classes overlap with the classes in the ILSVRC dataset .",
      "Pixelwise Segmentation Annotation We used Photoshop 's \" quick selection \" tool which allows users to loosely select an object automatically , and refined or corrected the selected area to produce the desired segmentation .",
      "This section summarizes the three desirable properties of FSS - 1 0 0 0 : Scalability To extend FSS - 1 0 0 0 to include a new class , all it takes are 1 0 images with pixelwise binary segmen - Figure 3 .",
      "This is significantly easier than other datasets such as PASCAL VOC and COCO .",
      "Instance FSS - 1 0 0 0 dataset supports instance - level segmentation with instance segmentation labels in 7 5 8 out of the 1, 0 0 0 classes in the dataset , which are significantly more classes than PASCAL VOC and MS COCO .",
      "One major difference between our dataset and PASCAL VOC / MS COCO instance level segmentation is that our dataset only annotates one type of objects in one image , despite there may be other object categories appearing in the background .",
      "We annotate at most 1 0 instances in a single image , which follows the same instance annotation principle adopted by COCO .   In few - shot learning , the train - test split is on object categories .",
      "In few - shot segmentation , we adopt this notation but extend the query output to be per - pixel classification of the query image , rather than a single class label .",
      "However , a general C - way - K - shot segmentation could be solved by a union of C binary segmentation tasks .",
      "One can design his/her own or choose any popular feature extraction backbone such as VGG - 1 6 [ 3 2 ] , ResNet [ 1 3 ] and Inception [ 3 4 ] as the encoder module inside the network .",
      "ReLU activation is applied throughout the deep network except for the last layer 's activation where Sigmoid is used in order to scale the output to a suitable range to calculate cross - entropy loss .",
      "Method MeanIoU OSLSM - 1 shot [ 3 0 ] 7 0 . 2 9 % OSLSM - 5 shot 7 3 . 0 2 % Guided Network - 1 shot [ 2 4 ] 7 1 . 9 4 % Guided Network - 5 shot 7 4 . 2 7 % Ours - 1 shot 7 3 . 4 7 % Ours - 5 shot 8 0 . 1 2 % Table 4 .",
      "GN is Guided Network and Ours * is our model trained on FSS - 1 0 0 0 .",
      "We evaluate models with the same network architecture but trained on different datasets to show that FSS - 1 0 0 0 is the best choice for few - shot segmentation task .",
      "Finally we illustrate that models trained on FSS - 1 0 0 0 are capable to generalize the few - shot segmentation knowledge to new unseen classes .",
      "Table 2 tabulates the respective performance on VGG - 1 6 , ResNet - 1 0 1 and InceptionNet as backbone , and BCE and MSE as loss function .",
      "Based on the result , we choose VGG - 1 6 as feature extractor and use BCE loss in our model throughout the experimental section .   We train OSLSM and Guided Network on FSS - 1 0 0 0 to provide benchmarks and justify our dataset .",
      "Each model ( row ) shows the training stages , e.g. , model I uses the pretrained weights from ImageNet then fine - tuned on fsPASCAL .",
      "Most importantly , we train our network merely on FSS - 1 0 0 0 without fine - tuning on PASCAL - 5 i to avoid any potential overfitting , and this model achieves much better results compared to models trained on PASCAL - 5 i , which justify the effectiveness of FSS - 1 0 0 0 ."
    ],
    "relations": [
      [
        "Open Image:Dataset",
        "Benchmark-For",
        "detection:Task"
      ],
      [
        "instance level segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "pixelwise segmentation:Task",
        "SubTask-Of",
        "semantic segmentation:Task"
      ],
      [
        "OSLSM:Method",
        "Used-For",
        "few - shot segmentation:Task"
      ],
      [
        "FCN:Method",
        "Synonym-Of",
        "fully convolutional network:Method"
      ],
      [
        "fully convolutional network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "fully convolutional network:Method",
        "Used-For",
        "dense pixel prediction:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "objects few - shot segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Compare-With",
        "PASCAL VOC:Dataset"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Compare-With",
        "MS COCO:Dataset"
      ],
      [
        "C binary segmentation:Task",
        "SubTask-Of",
        "C - way - K - shot segmentation:Task"
      ],
      [
        "VGG - 1 6:Method",
        "SubClass-Of",
        "feature extraction backbone:Method"
      ],
      [
        "ResNet:Method",
        "SubClass-Of",
        "feature extraction backbone:Method"
      ],
      [
        "Inception:Method",
        "SubClass-Of",
        "feature extraction backbone:Method"
      ],
      [
        "GN:Method",
        "Synonym-Of",
        "Guided Network:Method"
      ],
      [
        "GN:Method",
        "Trained-With",
        "FSS - 1 0 0 0:Dataset"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "VGG - 1 6:Method",
        "SubClass-Of",
        "feature extractor:Method"
      ]
    ]
  },
  {
    "doc_id": "198967567",
    "chunk_id": 3,
    "content": [
      "We compare our network model trained on different datasets to demonstrate the effectiveness of FSS - 1 0 0 0 in few - shot segmentation .",
      "Since there are no publicly available few - shot image segmentation datasets , we convert PASCAL VOC 2 0 1 2 and COCO datasets by setting the desired foreground class label as positive and all others as negative , followed by the identical clean - up stage described in section 3. 1 to the binarized labels .",
      "Image results of our baseline model respectively trained on fsPASCAL , fsCOCO and FSS - 1 0 0 0 .",
      "The classes in the first two rows are present in fs - PASCAL and fsCOCO whereas the rest are unique in FSS - 1 0 0 0 . thus produced : fsPASCAL and fsCOCO .",
      "There are respectively 4, 3 1 8 image and label pairs in 2 0 object classes in fsPASCAL , and 4 8 , 0 1 5 image and label pairs in 8 0 object classes in fsCOCO .",
      "All available data in fsPASCAL and fs - COCO are used in training .",
      "Using the pre - trained weights from ImageNet , Model III trained on FSS - 1 0 0 0 outperforms respectively the fsPAS - CAL model I and fsCOCO model II by over a large margin of 2 0 % and 1 0 % .",
      "Interestingly , Model VI pre - trained on COCO and finetuned on FSS - 1 0 0 0 achieves the best result , outperforming the model III pre - trained on ILSRVC .",
      "We believe this is due to the difference in requirement of feature maps ideal for classification and segmentation task .",
      "Intuitively , semantic segmentation requires more accurate low - level features to produce fine details in segmentation map , while classification focuses on high - level features for image understanding .",
      "Overall , models respectively trained on fsPASCAL and fsCOCO produce quite good results in object classes that are included in PASCAL and COCO , or similar to PAS - CAL and COCO classes .",
      "For these classes , sometimes their segmentation results are better in local details compared to the results produced by models trained on FSS - 1 0 0 0 due to more variations in the support training set .",
      "However , they respectively fail in classes significantly different from the 2 0 PASCAL classes and 8 0 COCO classes .",
      "Figure 8 demonstrates the effect of support set , which shows that scale and pose of the object to be segmented are the most important characteristics to guide few - shot semantic segmentation on FSS - 1 0 0 0 .",
      "Since FSS - 1 0 0 0 does not explicitly consider scale variations ( future work ) , a tiny or oversized object in the support set is not a good reference for segmentation .",
      "Besides , significantly different poses in support and query sets can result in bad segmentation results , due to the intrinsic fragility to rotation in CNN features .",
      "Table 6 tabulates the tradeoff in time and accuracy for annotating 5 0 0 test images in FSS - 1 0 0 0 by humans ( using Photoshop and GrabCut [ 2 7 ] algorithm ) and our few - shot segmentation .",
      "From top to bottom : android robot ; the river from UC Merced Land Use Dataset [ 3 7 ] ; a large cell image cropped into patches ; herds of sheep ; penguin from Oxford penguin counting dataset [ 1 ] ; flock of wild goose ; different images of fields of sunflower depict various scales in the presence of occlusion and perspective distortion . ple where saliency detection does not work in general .",
      "The cell example shows the good potential of FSS - 1 0 0 0 in instance segmentation which significantly contributes to cell counting in medical image analysis where , for instance , a patient 's health directly correlates to his or her red blood cell count .",
      "With the advance of whole slide images ( WSI ) in which the width and height often exceed 1 0 0 , 0 0 0 pixels ( and thus many cells to count ) , using our few - shot segmentation trained on FSS - 1 0 0 0 , pathologists only need to label 5 image relevant regions and then the rest of the WSI will be automatically labeled .",
      "Similarly , the related animal examples of sheep , penguin and wild goose show FSS - 1 0 0 0 's potential for large - scale instance segmentation .",
      "Few - shot learning/segmentation is an emerging attractive alternative , where prediction is made given only a few training examples .",
      "In this paper , we address the limitation of existing large - scale datasets in their biases and lack of scalability , and build the first few - shot segmentation dataset FSS - 1 0 0 0 emphasizing class diversity rather than dataset size .",
      "This baseline few - shot segmentation model , even trained exclusively on FSS - 1 0 0 0 without using pre - trained weights , achieves higher accuracy than previous methods .",
      "We further demonstrated the efficacy and potential of FSS - 1 0 0 0 in large - scale segmentation on totally unseen classes without re - training or fine - tuning , and showed its promise on few - shot instance segmentation and iterative few - shot recognition tasks ."
    ],
    "relations": [
      [
        "OSLSM:Method",
        "Trained-With",
        "FSS - 1 0 0 0:Dataset"
      ],
      [
        "Guided Network:Method",
        "Trained-With",
        "FSS - 1 0 0 0:Dataset"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "semantic segmentation:Task",
        "SubTask-Of",
        "segmentation map:Task"
      ],
      [
        "classification:Task",
        "SubTask-Of",
        "image understanding:Task"
      ],
      [
        "PASCAL:Dataset",
        "Compare-With",
        "COCO:Dataset"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot semantic segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "instance segmentation:Task"
      ],
      [
        "instance segmentation:Task",
        "SubTask-Of",
        "medical image analysis:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "instance segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "large - scale segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "few - shot instance segmentation:Task"
      ],
      [
        "FSS - 1 0 0 0:Dataset",
        "Benchmark-For",
        "iterative few - shot recognition:Task"
      ]
    ]
  },
  {
    "doc_id": "201646244",
    "chunk_id": 1,
    "content": [
      "We introduce FinBERT , a language model based on BERT , to tackle NLP tasks in the financial domain .",
      "Hence , automated sentiment or polarity analysis of texts produced by financial actors using natural language processing ( NLP ) methods has gained popularity during the last decade [ 4 ] .",
      "It requires to address two challenges : 1 ) The most sophisticated classification methods that make use of neural nets require vast amounts of labeled data and labeling financial text snippets requires costly expertise . 2 ) The sentiment analysis models trained on general corpora are not suited to the task , because financial texts have a specialized language with unique vocabulary and have a tendency to use vague expressions instead of easilyidentified negative/positive words .",
      "NLP transfer learning methods look like a promising solution to both of the challenges mentioned above , and are the focus of this thesis .",
      "For that , sentiment of a sentence from a financial news article towards the financial actor depicted in the sentence will be tried to be predicted , using the Financial PhraseBank created by Malo et al. ( 2 0 1 4 ) [ 1 7 ] and FiQA Task 1 sentiment scoring dataset [ 1 5 ] .",
      "The main contributions of this thesis are the following : • We introduce FinBERT , which is a language model based on BERT for financial NLP tasks .",
      "We evaluate FinBERT on two financial sentiment analysis datasets . • We achieve the state - of - the - art on FiQA sentiment scoring and Financial PhraseBank . • We implement two other pre - trained language models , ULMFit and ELMo for financial sentiment analysis and compare these with FinBERT . • We conduct experiments to investigate several aspects of the model , including : effects of further pre - training on financial corpus , training strategies to prevent catastrophic forgetting and fine - tuning only a small subset of model layers for decreasing training time without a significant drop in performance .",
      "This section describes previous research conducted on sentiment analysis in finance ( 2. 1 ) and text classification using pre - trained language models ( 2. 2 ) .",
      "Financial sentiment analysis differs from general sentiment analysis not only in domain , but also the purpose .",
      "Sohangir et al. ( 2 0 1 8) [ 2 6 ] apply several generic neural network architectures to a StockTwits dataset , finding CNN as the best performing neural network architecture .",
      "Lutz et al. 2 0 1 8 [ 1 3 ] take the approach of using doc 2 vec to generate sentence embeddings in a particular company ad - hoc announcement and utilize multi - instance learning to predict stock market outcomes . [ 1 4 ] use a combination of text simplification and LSTM network to classify a set of sentences from financial news according to their sentiment and achieve stateof - the - art results for the Financial PhraseBank , which is used in thesis as well .",
      "One of the most important recent developments in natural language processing is the realization that a model trained for language modeling can be successfully fine - tuned for most down - stream NLP tasks with small modifications .",
      "Using the pre - trained weights of ELMo , contextualized word embeddings can be calculated for any piece of text .",
      "Initializing embeddings for down - stream tasks with those were shown to improve performance on most tasks compared to static word embeddings such as word 2 vec or GloVe .",
      "For text classification tasks like SST - 5 , it achieved state - of - the - art performance when used together with a bi - attentive classification network [ 2 0 ] .",
      "ULMFit ( Universal Language Model Fine - tuning ) [ 5 ] was the first paper to achieve true transfer learning for NLP , as using novel techniques such as discriminative fine - tuning , slanted triangular learning rates and gradual unfreezing .",
      "ULMFit 's main idea of efficiently fine - tuning a pre - trained a language model for down - stream tasks was brought to another level with Bidirectional Encoder Representations from Transformers ( BERT ) [ 3 ] , which is also the main focus of this paper .",
      "These two factors enabled in to achieve state - of - the - art results in multiple NLP tasks such as , natural language inference or question answering .",
      "The specifics of fine - tuning BERT for text classification has not been researched thoroughly .",
      "They conduct a series of experiments regarding different configurations of BERT for text classification .",
      "In this section , we will present our BERT implementation for financial domain named as FinBERT , after giving a brief background on relevant neural architectures . 3. 1 . 1 LSTM .",
      "Long short - term memory ( LSTM ) is a type of recurrent neural network that allows long - term dependencies in a sequence to persist in the network by using \" forget \" and \" update \" gates .",
      "Since a text is a sequence of tokens , the first choice for any LSTM natural language processing model is determining how to initially represent a single token .",
      "One such pre - training algorithm is GLoVe ( Global Vectors for Word Representation ) [ 2 2 ] .",
      "In the center of ELMo , there is a bidirectional language model with multiple LSTM layers ."
    ],
    "relations": [
      [
        "FinBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "NLP:Task",
        "Synonym-Of",
        "natural language processing:Task"
      ],
      [
        "FinBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "financial NLP:Task"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "financial sentiment analysis:Task"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "financial sentiment analysis:Task"
      ],
      [
        "ELMo:Method",
        "Used-For",
        "financial sentiment analysis:Task"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "financial sentiment analysis:Task"
      ],
      [
        "ULMFit:Method",
        "Compare-With",
        "FinBERT:Method"
      ],
      [
        "ELMo:Method",
        "Compare-With",
        "FinBERT:Method"
      ],
      [
        "Financial sentiment analysis:Task",
        "SubTask-Of",
        "sentiment analysis:Task"
      ],
      [
        "neural network:Method",
        "Used-For",
        "StockTwits:Dataset"
      ],
      [
        "CNN:Method",
        "Used-For",
        "StockTwits:Dataset"
      ],
      [
        "CNN:Method",
        "SubClass-Of",
        "neural network:Method"
      ],
      [
        "doc 2 vec:Method",
        "Used-For",
        "sentence embeddings:Task"
      ],
      [
        "LSTM:Method",
        "Evaluated-With",
        "Financial PhraseBank:Dataset"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "SST - 5:Dataset",
        "Benchmark-For",
        "text classification:Task"
      ],
      [
        "discriminative fine - tuning:Method",
        "Used-For",
        "ULMFit:Method"
      ],
      [
        "slanted triangular learning rates:Method",
        "Used-For",
        "ULMFit:Method"
      ],
      [
        "gradual unfreezing:Method",
        "Used-For",
        "ULMFit:Method"
      ],
      [
        "ULMFit:Method",
        "Synonym-Of",
        "Universal Language Model Fine - tuning:Method"
      ],
      [
        "transfer learning:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "BERT:Method",
        "Synonym-Of",
        "Bidirectional Encoder Representations from Transformers:Method"
      ],
      [
        "natural language inference:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "question answering:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "FinBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "Long short - term memory:Method"
      ]
    ]
  },
  {
    "doc_id": "201646244",
    "chunk_id": 2,
    "content": [
      "Once the contextualized representations are extracted , these can be used to initialize any down - stream NLP task 2 . 3. 1 . 3 ULMFit .",
      "ULMFit is a transfer learning model for down - stream NLP tasks , that make use of language model pre - training [ 5 ] .",
      "Unlike ELMo , with ULMFit , the whole language model is fine - tuned together with the task - specific layers .",
      "The underlying language model used in ULMFit is AWD - LSTM , which uses sophisticated dropout tuning strategies to better regularize its LSTM model [ 2 1 ] .",
      "For classification using ULMFit two linear layers are added to the pre - trained AWD - LSTM , first of which takes the pooled last hidden states as input .",
      "We implement these strategies with FinBERT as explained in section 3. 2 . 1 The pre - trained weights for GLoVE can be found here : https://nlp.stanford.edu/projects/glove/ 2 The pre - trained ELMo models can be found here : https://allennlp.org/elmo 3. 1 . 4 Transformer .",
      "The Transformer is an attention - based architecture for modeling sequential information , that is an alternative to recurrent neural networks [ 2 9 ] .",
      "As it was argued by Vaswani 2 0 1 7 [ 2 9 ] , Transformer architecture has several advantages over the RNN - based approaches .",
      "Because of RNNs ' sequential nature , they are much harder to parallelize on GPUs and too many steps between far away elements in a sequence make it hard for information to persist . 3. 1 . 5 BERT .",
      "BERT [ 3 ] is in essence a language model that consists of a set of Transformer encoders stacked on top of each other .",
      "However it defines the language modeling task differently from ELMo and AWD - LSTM .",
      "A second task BERT is trained on is \" next sentence prediction \" .",
      "For all classification tasks , including the next sentence prediction , [ CLS ] token is used .",
      "BERT has two versions : BERT - base , with 1 2 encoder layers , hidden size of 7 6 8 , 1 2 multi - head attention heads and 1 1 0 M parameters in total and BERT - large , with 2 4 encoder layers , hidden size of 1 0 2 4 , 1 6 multi - head attention heads and 3 4 0 M parameters .",
      "Both of these models have been trained on BookCorpus [ 3 3 ] and English Wikipedia , which have in total more than 3, 5 0 0 M words 3 .",
      "In this subsection we will describe our implementation of BERT : 1 ) how further pre - training on domain corpus is done , 2 - 3 ) how we implemented BERT for classification and regression tasks , 4 ) training strategies we used during fine - tuning to prevent catastrophic forgetting .",
      "FinBERT for text classification .",
      "This is the recommended practice for using BERT for any classification task [ 3 ] .",
      "In order to deal with this phenomenon , we apply three techniques as it was proposed by Howard and Ruder ( 2 0 1 8) : slanted triangular learning rates , discriminative fine - tuning and gradual unfreezing .",
      "We aim to answer the following research questions : ( RQ 1 ) What is the performance of FinBERT in short sentence classification compared with the other transfer learning methods like ELMo and ULMFit ? [ 1 5 ] is a dataset that was created for WWW ' 1 8 conference financial opinion mining and question answering challenge 6 .",
      "For contrastive experiments , we consider baselines with three different methods : LSTM classifier with GLoVe embeddings , LSTM classifier with ELMo embeddings and ULMFit classifier .",
      "The difference between two models is that one uses GLoVe embeddings , while the other uses ELMo embeddings .",
      "As it was explained in section 3. 1 . 3 , classification with ULMFit consists of three steps .",
      "We first further pre - train AWD - LSTM language model on TRC 2 - financial corpus for 3 epochs .",
      "After that , we fine - tune the model for classification on Financial 6 Data can be found here : https://sites.google.com/view/fiqa/home PhraseBank dataset , by adding a fully - connected layer to the output of pre - trained language model ."
    ],
    "relations": [
      [
        "LSTM:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "GLoVe:Method",
        "Synonym-Of",
        "Global Vectors for Word Representation:Method"
      ],
      [
        "LSTM:Method",
        "Part-Of",
        "ELMo:Method"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "transfer learning:Task"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "ELMo:Method",
        "Compare-With",
        "ULMFit:Method"
      ],
      [
        "AWD - LSTM:Method",
        "Part-Of",
        "ULMFit:Method"
      ],
      [
        "dropout tuning strategies:Method",
        "Used-For",
        "AWD - LSTM:Method"
      ],
      [
        "LSTM:Method",
        "Part-Of",
        "AWD - LSTM:Method"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "linear layers:Method",
        "Part-Of",
        "AWD - LSTM:Method"
      ],
      [
        "Transformer:Method",
        "SubClass-Of",
        "recurrent neural networks:Method"
      ],
      [
        "Transformer encoders:Method",
        "Part-Of",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "next sentence prediction:Task"
      ],
      [
        "next sentence prediction:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "BERT - base:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "BERT - large:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "multi - head attention heads:Method",
        "Part-Of",
        "BERT - base:Method"
      ],
      [
        "multi - head attention:Method",
        "Part-Of",
        "BERT - large:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "regression:Task"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "ELMo:Method"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "ULMFit:Method"
      ],
      [
        "GLoVe embeddings:Method",
        "Part-Of",
        "LSTM:Method"
      ],
      [
        "ELMo embeddings:Method",
        "Part-Of",
        "LSTM:Method"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "AWD - LSTM:Method",
        "Trained-With",
        "TRC 2 - financial corpus:Dataset"
      ],
      [
        "Financial PhraseBank:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "dropout:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "warm - up:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "FinBERT:Method",
        "Evaluated-With",
        "Financial PhraseBank:Dataset"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "classification:Task"
      ]
    ]
  },
  {
    "doc_id": "201646244",
    "chunk_id": 3,
    "content": [
      "Since our data , Financial PhraseBank suffers from label imbalance ( almost 6 0 % of all sentences are neutral ) , this gives another good measure of the classification performance .",
      "For our implementation BERT , we use a dropout probability of p = 0. 1 , warm - up proportion of 0. 2 , maximum sequence length of 6 4 tokens , a learning rate of 2e − 5 and a mini - batch size of 6 4 .",
      "The results of FinBERT , the baseline methods and state - of - the - art on Financial PhraseBank dataset classification task can be seen on table 2 .",
      "For all of the measured metrics , FinBERT performs clearly the best among both the methods we implemented ourselves ( LSTM and ULMFit ) and the models reported by other papers ( LPS [ 1 7 ] , HSC [ 8 ] , FinSSLX [ 1 4 ] ) .",
      "In terms of accuracy , it is close to LPS and HSC , ( even better than LPS for examples with full agreement ) , however it produces a low F 1 - score .",
      "LSTM classifier with ELMo embeddings improves upon LSTM with static embeddings in all of the measured metrics .",
      "But it 's performance is comparable with LPS and HSC , besting them in accuracy .",
      "It also handily beats the machine learning based models LPS and HSC .",
      "ULMFit also outperforms FinSSLX , which has a text simplification step as well as pre - training of word embeddings on a large financial corpus with sentiment labels .",
      "FinBERT outperforms ULMFit , and consequently all of the other methods in all metrics .",
      "In order to measure the performance of the models on different sizes of labeled training datasets , we ran LSTM classifiers , ULMFit and FinBERT on 5 different configurations .",
      "However , once the training size becomes 2 5 0 , ULMFit and FinBERT starts to successfully differentiate between labels , with an accuracy as high as 8 0 % for FinBERT .",
      "All of the methods consistently get better with more data , but ULMFit and FinBERT does better with 2 5 0 examples than LSTM classifiers do with the whole dataset .",
      "We compare three models : 1 ) No further pre - training ( denoted by Vanilla BERT ) , 2 ) Further pre - training on classification training set ( denoted by FinBERT - task ) , 3 ) Further pre - training on domain corpus , TRC 2 - financial ( denoted by FinBERT - domain ) .",
      "We think that the last explanation is the likeliest , because for the subset of Financial Phrasebank that all of the annotators agree on the result , accuracy of Vanilla BERT is already 0. 9 6 .",
      "For measuring the performance of the techniques against catastrophic forgetting , we try four different settings : No adjustment ( NA ) , only with slanted triangular learning rate ( STL ) , slanted triangular learning rate and gradual unfreezing ( STL+GU ) and the techniques in the previous one , together with discriminative finetuning .",
      "We see from table 5 that using only discriminative fine - tuning with slanted triangular learning rates performs worse than using the slanted triangular learning rates alone .",
      "BERT has 1 2 Transformer encoder layers .",
      "In this paper , we implemented BERT for the financial domain by further pre - training it on a financial corpus and fine - tuning it for sentiment analysis ( FinBERT ) .",
      "In addition to BERT , we also implemented other pre - training language models like ELMo and ULMFit for comparison purposes .",
      "ULMFit , further pre - trained on a financial corpus , beat the previous state - of - the art for the classification task , only to a smaller degree than BERT .",
      "Another possible extension can be using FinBERT for other natural language processing tasks such as named entity recognition or question answering in financial domain .",
      "I am grateful to NIST , for sharing Reuters TRC - 2 corpus with me and to Malo et al. for making the excellent Financial PhraseBank publicly available ."
    ],
    "relations": [
      [
        "Financial PhraseBank:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "LSTM:Method"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "ULMFit:Method"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "LPS:Method"
      ],
      [
        "LSTM classifier with ELMo embeddings:Method",
        "Compare-With",
        "LSTM:Method"
      ],
      [
        "LPS:Method",
        "SubClass-Of",
        "machine learning based models:Method"
      ],
      [
        "HSC:Method",
        "SubClass-Of",
        "machine learning based models:Method"
      ],
      [
        "ULMFit:Method",
        "Compare-With",
        "FinSSLX:Method"
      ],
      [
        "text simplification:Task",
        "Used-For",
        "FinSSLX:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "FinSSLX:Method"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "ULMFit:Method"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "ULMFit:Method"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "FinBERT:Method"
      ],
      [
        "ULMFit:Method",
        "Compare-With",
        "FinBERT:Method"
      ],
      [
        "ULMFit:Method",
        "Compare-With",
        "FinBERT:Method"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "FinBERT:Method"
      ],
      [
        "ULMFit:Method",
        "Compare-With",
        "LSTM:Method"
      ],
      [
        "FinBERT:Method",
        "Compare-With",
        "LSTM:Method"
      ],
      [
        "FinBERT - domain:Method",
        "Trained-With",
        "TRC 2 - financial:Dataset"
      ],
      [
        "Vanilla BERT:Method",
        "Evaluated-With",
        "Financial Phrasebank:Dataset"
      ],
      [
        "STL:Method",
        "Synonym-Of",
        "slanted triangular learning rate:Method"
      ],
      [
        "STL+GU:Method",
        "Synonym-Of",
        "slanted triangular learning rate and gradual unfreezing:Method"
      ],
      [
        "discriminative fine - tuning with slanted triangular learning rates:Method",
        "Compare-With",
        "slanted triangular learning rates:Method"
      ],
      [
        "Transformer encoder:Method",
        "Part-Of",
        "BERT:Method"
      ],
      [
        "FinBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "ULMFit:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "ULMFit:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "named entity recognition:Task"
      ],
      [
        "FinBERT:Method",
        "Used-For",
        "question answering:Task"
      ]
    ]
  },
  {
    "doc_id": "54457603",
    "chunk_id": 1,
    "content": [
      "Beyond single - image activity recognition , a new temporal feature extractor was proposed to achieve video - based activity recognition [ 1 6 ] .",
      "Existing research simply modifies single - activity recognition models with a sigmoid output layer or uses multiple single - activity recognizers for concurrent activity recognition ; these have failed to achieve satisfactory performance [ 1 8 ] .",
      "Firstly , single - activity recognition focuses on selecting the most representative features associated with a certain activity ; many attentionbased methods were proposed to improve this feature extraction .",
      "Lastly , both types of activity recognition use probabilistic inference ( softmax ) for decision making .",
      "However , simply applying softmax with a threshold or using a parallel softmax layer for concurrent activity recognition would ignore the associations between activities .",
      "To address these challenges , we introduce a concurrent activity recognition model with a feature - to - activity attention for feature extraction and a tri - axial self - attention encoder - decoder for multi - label prediction .",
      "The proposed feature - to - activity attention maintains unaggregated temporal information to preserve temporal ordering ; different activities may occur at different times and require different temporal attentions , which would be Tri - axial Self - Attention for Concurrent Activity Recognition An overview of proposed concurrent activity recognition system that generates separated spatial - temporal features for independent activities . ignored by simple aggregation .",
      "We tested our system with published datasets : the hockey dataset [ 5 ] ( 1 2 activities ) , the volleyball dataset [ 1 5 ] ( 9 activities ) , and the charades dataset [ 2 5 ] ( 1 5 7 activities ) .",
      "Our contributions are : • A three level feature extraction strategy with a novel feature - to - activity attention for concurrent activity recognition . • A triaxial self - attention that learns independent temporal associations for different activities and makes concurrent activity predictions while being aware of possible activity combinations . • An activity association mask that helps the self - attentionbased decoder better capture the associations between activities .",
      "Traditional research was mainly based on hand - crafted features such as Histogram of Oriented Gradients ( HOG ) and Histogram of Optical Flow ( HOF ) [ 8 ] .",
      "Recently , deep learning has been commonly applied to activity recognition , initially using image recognition approaches with temporal feature fusion [ 1 6 ] .",
      "However , 2D CNNs originally used for image classification do not properly model spatio - temporal associations , which are critical for video - based activity recognition .",
      "CNN - LSTM strategies benefited from LSTM temporal association modeling ."
    ],
    "relations": [
      [
        "temporal feature extractor:Method",
        "Used-For",
        "video - based activity recognition:Task"
      ],
      [
        "sigmoid:Method",
        "Part-Of",
        "single - activity recognition models:Method"
      ],
      [
        "multiple single - activity recognizers:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "single - activity recognition models:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "probabilistic inference:Method",
        "Used-For",
        "activity recognition:Task"
      ],
      [
        "softmax:Method",
        "SubClass-Of",
        "probabilistic inference:Method"
      ],
      [
        "activity recognition:Task",
        "Used-For",
        "decision making:Task"
      ],
      [
        "parallel softmax:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "softmax with a threshold:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "feature - to - activity attention:Method",
        "Part-Of",
        "concurrent activity recognition model:Method"
      ],
      [
        "concurrent activity recognition model:Method",
        "Used-For",
        "feature extraction:Task"
      ],
      [
        "tri - axial self - attention encoder - decoder:Method",
        "Used-For",
        "multi - label prediction:Task"
      ],
      [
        "Tri - axial Self - Attention:Method",
        "SubClass-Of",
        "temporal attentions:Method"
      ],
      [
        "Tri - axial Self - Attention:Method",
        "Used-For",
        "Concurrent Activity Recognition:Task"
      ],
      [
        "proposed concurrent activity recognition system:Method",
        "Used-For",
        "generates separated spatial - temporal features:Task"
      ],
      [
        "feature - to - activity attention:Method",
        "Part-Of",
        "feature extraction strategy:Method"
      ],
      [
        "feature extraction strategy:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "HOG:Method",
        "Synonym-Of",
        "Histogram of Oriented Gradients:Method"
      ],
      [
        "HOF:Method",
        "Synonym-Of",
        "Histogram of Optical Flow:Method"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "activity recognition:Task"
      ]
    ]
  },
  {
    "doc_id": "54457603",
    "chunk_id": 2,
    "content": [
      "Spatial attention ( or region - based methods ) as well as temporal attention were proposed to help networks better focus on activity - associated features [ 2 4 ] [ 1 9 ] .",
      "The iDT [ 3 1 ] and TDD [ 3 4 ] works showed that manually - crafted ( as opposed to learned ) spatio - temporal descriptors can achieve good activity recognition performance .",
      "Most existing approaches simply modify single - activity recognition strategies for concurrent activity recognition .",
      "A CNN - LSTM structure was tested for concurrent activity recognition with limited success [ 1 8 ] .",
      "More effective feature extraction frameworks were proposed in recent years , including a multi - stream network [ 1 4 ] , a 3D convolution network [ 7 ] [ 3 7 ] , and a non - local neural network [ 3 2 ] .",
      "The features of visual objects are first extracted by the pre - trained VGG net on the ImageNet dataset .",
      "This is because unlike single - activity recognition , multiple activities might be distributed through the entire video clip ; the unspecific temporal attention will not be able to highlight the activityspecific features in time .",
      "Previous research modeled temporal features with a 3D ConvNet , LSTMs with attention , or 3D descriptors [ 1 8 ] [ 7 ] .",
      "Additionally , many previous works simply used the top N results or a sigmoid layer for concurrent activity recognition , which ignores the possible associations between activities .",
      "Previous decoder based on fully - connected layers with sigmoid [ 1 0 ] [ 7 ] ignored the feature associations between different activities .",
      "Batch normalization was used after each convolutional layer , and dropout ( rate= 0 . 5 ) was used after each dense layer to avoid overfitting .",
      "We used the Keras built - in VGG parameters for feature extraction , and set the last block as tunable .",
      "For the tunable VLAD descriptors , we made changes based on the action VLAD source code [ 3 6 ] .   We tested our system with three commonly used concurrent activity recognition datasets : Hockey Dataset [ 5 ] : This was collected from real university - level hockey matches using two fixed cameras positioned at both ends of the rink on the spectator 's side ."
    ],
    "relations": [
      [
        "image recognition approaches:Method",
        "Used-For",
        "activity recognition:Task"
      ],
      [
        "temporal feature fusion:Method",
        "Used-For",
        "image recognition approaches:Method"
      ],
      [
        "2D CNNs:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "LSTM:Method",
        "Part-Of",
        "CNN - LSTM:Method"
      ],
      [
        "Spatial attention:Method",
        "SubClass-Of",
        "region - based methods:Method"
      ],
      [
        "Spatial attention:Method",
        "Used-For",
        "activity - associated features:Task"
      ],
      [
        "temporal attention:Method",
        "Used-For",
        "activity - associated features:Task"
      ],
      [
        "TDD:Method",
        "Used-For",
        "activity recognition:Task"
      ],
      [
        "iDT:Method",
        "Used-For",
        "activity recognition:Task"
      ],
      [
        "single - activity recognition strategies:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "CNN - LSTM:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ],
      [
        "multi - stream network:Method",
        "SubClass-Of",
        "feature extraction frameworks:Method"
      ],
      [
        "3D convolution network:Method",
        "SubClass-Of",
        "feature extraction frameworks:Method"
      ],
      [
        "non - local neural network:Method",
        "SubClass-Of",
        "feature extraction frameworks:Method"
      ],
      [
        "VGG:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "3D descriptors:Method",
        "Part-Of",
        "3D ConvNet:Method"
      ],
      [
        "attention:Method",
        "Part-Of",
        "3D ConvNet:Method"
      ],
      [
        "3D descriptors:Method",
        "Part-Of",
        "LSTMs:Method"
      ],
      [
        "attention:Method",
        "Part-Of",
        "LSTMs:Method"
      ],
      [
        "sigmoid:Method",
        "Used-For",
        "concurrent activity recognition:Task"
      ]
    ]
  },
  {
    "doc_id": "54457603",
    "chunk_id": 3,
    "content": [
      "We compared our proposed method on the hockey and volleyball dataset with several baselines including : 1 .",
      "AlexNet for framewise activity recognition . 2 .",
      "CNN - LSTM with sigmoid layer [ 1 8 ] .",
      "CNNs over time [ 2 8 ] , and various SVMs .",
      "We also compared our method on the charades dataset ( Table 2 ) , the result shows our method outperformed most of the researches except the i 3 D and non - local neural network [ 3 2 ] .",
      "These two methods have much better performance on charades dataset , but they pre - trained their model on kinetics dataset ( 5 0 times larger ) and the time complexity of their methods are 5 times larger than our proposed method .",
      "Our model achieved comparable performance with only 2 0 % time complexity compared with i 3 D in charades .",
      "F 1 VGG Net [ 2 7 ] 9 1 System MAP complexity ( MACC ) AlexNet [ 1 7 ] 1 1 . 2 1. 2 B C 3 D [ 2 5 ] 1 0 . 7 8 0 B * Two - Stream + IDT [ 2 5 ] 1 8 . 6 / CoViAR [ 3 5 ] 2 1 . 9 / Asynchronous Temporal Fields [ 2 6 ] 2 2 . 4 / i 3 D [ 7 ] 3 4 . 4 1 6 5 B Action VLAD [ 1 0 ] 1 7 . 6 3 4 B Non - local Neural Network [ 3 2 ] 3 7 . 5 1 6 5 B+ Our Model 2 2 . 6 3 4 B Table 2 : Experiment results on charades .",
      "We tried to replace the triaxial self - attention with a traditional LSTM encoder - decoder .",
      "Although we only demonstrated our proposed system on concurrent activity recognition , the core idea of feature - toactivity attention and tri - axial self - attention can be extended to general multi - label classification .",
      "Many recent works have demonstrated that non - local neural networks and i 3 D have good performance on activity recognition .",
      "We designed a novel deep learning architecture for recognizing concurrent activities that outperformed stateof - the - art mechanisms in three published datasets ( charades , volleyball , and hockey ) .",
      "We hope this paper delivers the following contribution to the society : • A modified VLAD feature extractor and novel activity mapping layer that extracts independent features for each activity while preserving the temporal information . • A novel triaxial self - attention that learns independent temporal associations for different activities and makes concurrent activity predictions aware of possible activity combinations . • A novel use of the self - attention decoder that helps extract hidden associations between activities and avoid redundant weights . • An implemented network architecture that can serve as a reference for any multi - label classification problem given sequential input ."
    ],
    "relations": [
      [
        "fully - connected layers:Method",
        "Part-Of",
        "decoder:Method"
      ],
      [
        "sigmoid:Method",
        "Part-Of",
        "fully - connected layers:Method"
      ],
      [
        "VGG:Method",
        "Used-For",
        "feature extraction:Task"
      ],
      [
        "Hockey:Dataset",
        "Benchmark-For",
        "concurrent activity recognition:Task"
      ],
      [
        "AlexNet:Method",
        "Used-For",
        "framewise activity recognition:Task"
      ],
      [
        "sigmoid:Method",
        "Part-Of",
        "CNN - LSTM:Method"
      ],
      [
        "non - local neural network:Method",
        "Evaluated-With",
        "charades:Dataset"
      ],
      [
        "i 3 D:Method",
        "Evaluated-With",
        "charades:Dataset"
      ],
      [
        "i 3 D:Method",
        "Evaluated-With",
        "charades:Dataset"
      ],
      [
        "tri - axial self - attention:Method",
        "Used-For",
        "multi - label classification:Task"
      ],
      [
        "feature - toactivity attention:Method",
        "Used-For",
        "multi - label classification:Task"
      ],
      [
        "non - local neural networks:Method",
        "Used-For",
        "activity recognition:Task"
      ],
      [
        "i 3 D:Method",
        "Used-For",
        "activity recognition:Task"
      ],
      [
        "charades:Dataset",
        "Benchmark-For",
        "recognizing concurrent activities:Task"
      ],
      [
        "volleyball:Dataset",
        "Benchmark-For",
        "recognizing concurrent activities:Task"
      ],
      [
        "hockey:Dataset",
        "Benchmark-For",
        "recognizing concurrent activities:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "recognizing concurrent activities:Task"
      ],
      [
        "triaxial self - attention:Method",
        "Used-For",
        "concurrent activity predictions:Task"
      ]
    ]
  },
  {
    "doc_id": "121101928",
    "chunk_id": 1,
    "content": [
      "In this work , we tackle the problem of efficient keypoint - based object detection and introduce CornerNet - Lite .",
      "CornerNet - Lite is a combination of two efficient variants of CornerNet : CornerNet - Saccade , which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image , and CornerNet - Squeeze , which introduces a new compact backbone architecture .",
      "CornerNet - Saccade is suitable for offline processing , improving the efficiency of CornerNet by 6. 0 x and the AP by 1. 0 % on COCO .",
      "CornerNet - Squeeze is suitable for real - time detection , improving both the efficiency and accuracy of the popular real - time detector YOLOv 3 ( 3 4 . 4 % AP at 3 0 ms for CornerNet - Squeeze compared to 3 3 . 0 % AP at 3 9 ms for YOLOv 3 on COCO ) .",
      "CornerNet [ 2 6 ] , the stateof - the - art among them , detects and groups the top - left and bottom - right corners of bounding boxes ; it uses a stacked hourglass network [ 3 9 ] to predict the heatmaps of the corners and then uses associate embeddings [ 3 8 ] to group them .",
      "CornerNet allows a simplified design that eliminates the need for anchor boxes [ 4 6 ] , and has achieved state - ofthe - art accuracy on COCO [ 3 2 ] among single - stage detectors .",
      "However , a major drawback of CornerNet is its inference    CornerNet [ Law & Deng ' 1 8 ] YOLOv 3 [ Redmon & Farhadi ' 1 8 ] Figure 1 : We introduce CornerNet - Saccade and CornerNetSqueeze ( collectively as CornerNet - Lite ) , two efficient object detectors based on CornerNet [ 2 6 ] , a state - of - the - art keypoint based object detector .",
      "CornerNet - Saccade speeds up the original CornerNet by 6. 0 x with a 1% increase in AP .",
      "CornerNet - Squeeze is faster and more accurate than YOLOv 3 [ 4 5 ] , the state - of - the - art real time detector .",
      "For example , single - scale processing combined with reducing the input resolution can speed up the inference of CornerNet to 4 2 ms per image , comparable to the 3 9 ms of the popular fast detector YOLOv 3 [ 4 5 ] , but would decrease the AP to 2 5 . 6 % which is much lower than YOLOv 3 's 3 3 . 0 % .",
      "We explore both directions and introduce two efficient variants of CornerNet : CornerNet - Saccade and CornerNet - Squeeze , which we refer to collectively as CornerNet - Lite .",
      "Experiments on COCO show that CornerNet - Saccade achieves an AP of 4 3 . 2 % at 1 9 0 ms per image , a 1% increase in AP and a 6. 0 x speed - up over the original CornerNet .",
      "It incorporates ideas from SqueezeNet [ 1 9 ] and MobileNets [ 1 5 ] , and introduces a new , compact hourglass backbone that makes extensive use of 1 × 1 convolution , bottleneck layer , and depth - wise separable convolution .",
      "With the new hourglass backbone , CornerNet - Squeeze achieves an AP of 3 4 . 4 % on COCO at 3 0 ms , simultaneously more accurate and faster than YOLOv 3 ( 3 3 . 0 % at 3 9 ms ) .",
      "Somewhat surprisingly , our experiments give a negative answer : CornerNet - Squeeze - Saccade turns out slower and less accurate than CornerNet - Squeeze .",
      "Significance and novelty : Collectively , these two variants of CornerNet - Lite make the keypoint - based approach competitive , covering two popular use cases : CornerNetSaccade for offline processing , improving efficiency without sacrificing accuracy , and CornerNet - Squeeze for realtime processing , improving accuracy without sacrificing efficiency .",
      "CornerNet - Saccade is the first to integrate saccades with keypoint - based object detection .",
      "Prior work that employs saccade - like mechanisms either detects a single object per crop ( e.g. Faster R - CNN [ 4 6 ] ) or produces multiple detections per crop with a two - stage network involving additional sub - crops ( e.g. AutoFocus [ 3 7 ] ) .",
      "CornerNet - Squeeze is the first to integrate SqueezeNet with the stacked hourglass architecture and to apply such a combination on object detection .",
      "Contributions Our contributions are three - fold : ( 1 ) We propose CornerNet - Saccade and CornerNet - Squeeze , two novel approaches to improving the efficiency of keypointbased object detection ; ( 2 ) On COCO , we improve the efficiency of state - of - the - art keypoint based detection by 6 fold and the AP from 4 2 . 2 % to 4 3 . 2 % , ( 3 ) On COCO , we improve both the accuracy and efficiency of state - of - the art real - time object detection ( to 3 4 . 4 % at 3 0 ms from 3 3 . 0 % at 3 9 ms of YOLOv 3 ) .",
      "Saccades in R - CNN [ 1 1 ] , Fast R - CNN [ 1 0 ] , and Faster R - CNN [ 4 6 ] take the form of crops representing potential objects .",
      "After processing , each crop is either rejected or converted to a single labeled box through classification and regression .",
      "Cascade R - CNN [ 4 ] extends Faster R - CNN by using a cascade of classifiers and regressors to iteratively reject or refine each proposal ."
    ],
    "relations": [
      [
        "CornerNet - Lite:Method",
        "Used-For",
        "keypoint - based object detection:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Part-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Part-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Used-For",
        "offline processing:Task"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Used-For",
        "real - time detection:Task"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "YOLOv 3:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "stacked hourglass network:Method",
        "Part-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNetSqueeze:Method",
        "SubClass-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNetSqueeze:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet:Method",
        "SubClass-Of",
        "keypoint based object detector:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "SubClass-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Saccade:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "1 × 1 convolution:Method",
        "Part-Of",
        "hourglass backbone:Method"
      ],
      [
        "bottleneck layer:Method",
        "Part-Of",
        "hourglass backbone:Method"
      ],
      [
        "depth - wise separable convolution:Method",
        "Part-Of",
        "hourglass backbone:Method"
      ],
      [
        "hourglass backbone:Method",
        "Part-Of",
        "CornerNet - Squeeze:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet - Squeeze - Saccade:Method",
        "Compare-With",
        "CornerNet - Squeeze:Method"
      ],
      [
        "CornerNetSaccade:Method",
        "Used-For",
        "offline processing:Task"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Used-For",
        "realtime processing:Task"
      ],
      [
        "CornerNet - Saccade:Method",
        "Used-For",
        "keypoint - based object detection:Task"
      ]
    ]
  },
  {
    "doc_id": "121101928",
    "chunk_id": 2,
    "content": [
      "AutoFocus [ 3 7 ] , which builds upon SNIPER [ 5 2 ] that improved R - CNN training , adds a branch to Faster R - CNN to predict the regions that are likely to contain small objects .",
      "Then it applies Faster R - CNN again to each of those regions Figure 2 : Overview of CornerNet - Saccade .",
      "In AutoFocus , there are two kinds of cropping , one that can produce multiple objects ( by calling Faster R - CNN as a subroutine ) , and the other that can produce at most a single object ( cropping done within Faster R - CNN ) .",
      "This means that the number of crops processed by CornerNet - Saccade can be much smaller than the number of objects , whereas for R - CNN variants and AutoFocus the number of crops must be no smaller than the number of objects .",
      "Other than accuracy [ 3 , 4 9 , 1 8 , 3 0 , 8 , 6 4 , 1 2 , 4 1 , 5 1 , 5 9 , 5 7 , 5 , 2 1 ] , many recent works have improved upon the efficiency of detectors since the introduction of R - CNN [ 1 1 ] , which applies a ConvNet [ 2 4 ] to 2 0 0 0 RoIs .",
      "SPP [ 1 3 ] and Fast R - CNN [ 1 0 ] address this by applying a ConvNet fully convolutionally on the image and extracting features directly from the feature maps for each RoI. Faster R - CNN [ 4 6 ] further improves efficiency by replacing the low - level vision algorithm with a region proposal network .",
      "R - FCN [ 7 ] replaces the expensive fully connected sub - detection network with a fully convolutional network , and Light - Head R - CNN [ 2 8 ] reduces the cost in R - FCN by applying separable convolution to reduce the number of channels in the feature maps before RoI pooling .",
      "SqueezeNet [ 1 9 ] proposes a fire module to reduce the number of parameters in AlexNet [ 2 4 ] by 5 0 x , while achieving similar performance .",
      "MobileNets [ 1 5 ] are a class of lightweight networks that use depth - wise separable convolutions [ 6 ] , and proposes strategies to achieve a good tradeoff between accuracy and latency .",
      "PeleeNet [ 5 5 ] , in contrast , demonstrates the effectiveness of standard convolutions by introducing an efficient variant of DenseNet [ 1 7 ] consisting of two - way dense layers and a stem block .",
      "YOLOv 2 [ 4 4 ] incorporates ideas from NIN [ 2 9 ] to design a new variant of VGG [ 5 0 ] .",
      "YOLOv 3 [ 4 5 ] further improves DarkNet - 1 9 by making the network deeper and introducing skip connections .",
      "Each hourglass module in the network applies several convolution and downsampling layers to downsize the input feature maps .",
      "We predict the attention maps by applying a 3 × 3 ConvReLU module followed by a 1 × 1 Conv - Sigmoid module to each feature map .",
      "The new hourglass network consists of 3 hourglass modules and has a depth of 5 4 layers , while Hourglass - 1 0 4 in CornerNet consists of 2 hourglass modules and has a depth of 1 0 4 .",
      "Each hourglass module in Hourglass - 5 4 has fewer parameters and is shallower than the one in Hourglass - 1 0 4 .",
      "We use Adam [ 2 2 ] to optimize both the losses for the attention maps and object detection , and use the same training hyperparameters found in CornerNet .",
      "In order to avoid over - fitting , we adopt the data augmentation techniques used in CornerNet .",
      "This ensures that training and testing are consistent as the network detects objects within the crops centered at object locations .   In contrast to CornerNet - Saccade , which focuses on a subset of the pixels to reduce the amount of processing , CornerNet - Squeeze explores an alternative approach of reducing the amount of processing per pixel .",
      "In CornerNet , most of the computational resources are spent on Hourglass - 1 0 4 .",
      "Hourglass - 1 0 4 is built from residual blocks which consists of two 3 × 3 convolution layers and a skip connection .",
      "To reduce the complexity of Hourglass - 1 0 4 , we incorporate ideas from SqueezeNet [ 1 9 ] and MobileNets [ 1 5 ] to design a lightweight hourglass architecture .",
      "The building block of SqueezeNet , the fire module , encapsulates the first two ideas ."
    ],
    "relations": [
      [
        "SqueezeNet:Method",
        "Part-Of",
        "CornerNet - Squeeze:Method"
      ],
      [
        "hourglass:Method",
        "Part-Of",
        "SqueezeNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "CornerNet - Saccade:Method",
        "Used-For",
        "keypointbased object detection:Task"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Used-For",
        "keypointbased object detection:Task"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "keypoint based detection:Task"
      ],
      [
        "YOLOv 3:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "real - time object detection:Task"
      ],
      [
        "YOLOv 3:Method",
        "Used-For",
        "real - time object detection:Task"
      ],
      [
        "Saccades:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "Saccades:Method",
        "Part-Of",
        "Fast R - CNN:Method"
      ],
      [
        "Saccades:Method",
        "Part-Of",
        "Faster R - CNN:Method"
      ],
      [
        "cascade of classifiers and regressors:Method",
        "Part-Of",
        "Cascade R - CNN:Method"
      ],
      [
        "Cascade R - CNN:Method",
        "SubClass-Of",
        "Faster R - CNN:Method"
      ],
      [
        "Faster R - CNN:Method",
        "Part-Of",
        "AutoFocus:Method"
      ],
      [
        "AutoFocus:Method",
        "SubClass-Of",
        "SNIPER:Method"
      ],
      [
        "SNIPER:Method",
        "Used-For",
        "R - CNN:Method"
      ],
      [
        "Faster R - CNN:Method",
        "Part-Of",
        "AutoFocus:Method"
      ],
      [
        "Faster R - CNN:Method",
        "Part-Of",
        "AutoFocus:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Compare-With",
        "R - CNN variants:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Compare-With",
        "AutoFocus:Method"
      ],
      [
        "ConvNet:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "ConvNet:Method",
        "Part-Of",
        "SPP:Method"
      ],
      [
        "ConvNet:Method",
        "Part-Of",
        "Fast R - CNN:Method"
      ],
      [
        "region proposal network:Method",
        "Part-Of",
        "Faster R - CNN:Method"
      ],
      [
        "fully convolutional network:Method",
        "Part-Of",
        "R - FCN:Method"
      ],
      [
        "separable convolution:Method",
        "Part-Of",
        "Light - Head R - CNN:Method"
      ],
      [
        "Light - Head R - CNN:Method",
        "Compare-With",
        "R - FCN:Method"
      ],
      [
        "fire module:Method",
        "Part-Of",
        "SqueezeNet:Method"
      ],
      [
        "SqueezeNet:Method",
        "Compare-With",
        "AlexNet:Method"
      ],
      [
        "depth - wise separable convolutions:Method",
        "Part-Of",
        "MobileNets:Method"
      ],
      [
        "DenseNet:Method",
        "Part-Of",
        "PeleeNet:Method"
      ],
      [
        "two - way dense layers:Method",
        "Part-Of",
        "DenseNet:Method"
      ],
      [
        "stem block:Method",
        "Part-Of",
        "DenseNet:Method"
      ],
      [
        "skip connections:Method",
        "Part-Of",
        "YOLOv 3:Method"
      ],
      [
        "YOLOv 3:Method",
        "Compare-With",
        "DarkNet - 1 9:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "hourglass module:Method"
      ],
      [
        "downsampling layers:Method",
        "Part-Of",
        "hourglass module:Method"
      ],
      [
        "hourglass:Method",
        "Part-Of",
        "hourglass network:Method"
      ],
      [
        "hourglass:Method",
        "Part-Of",
        "Hourglass - 1 0 4:Method"
      ],
      [
        "Hourglass - 1 0 4:Method",
        "Part-Of",
        "CornerNet:Method"
      ],
      [
        "hourglass module:Method",
        "Part-Of",
        "Hourglass - 5 4:Method"
      ],
      [
        "Hourglass - 5 4:Method",
        "Compare-With",
        "Hourglass - 1 0 4:Method"
      ]
    ]
  },
  {
    "doc_id": "121101928",
    "chunk_id": 3,
    "content": [
      "Based on the insights provided by SqueezeNet , we use the fire module in CornerNet - Squeeze instead of the residual block .",
      "Furthermore , inspired by the success of MobileNets , we replace the 3 × 3 standard convolution in the second layer with a 3 × 3 depth - wise separable convolution , which further improves inference time .",
      "Tab . 1 shows a detail comparison between the residual block in CornerNet and the new fire module in CornerNet - Squeeze .",
      "CornerNet - Squeeze correspondingly downsizes the image three times before the hourglass module , whereas CornerNet downsizes the image twice .",
      "We use the same training losses and hyperparameters of CornerNet to train CornerNet - Squeeze .",
      "We use COCO [ 3 2 ] to evaluate CornerNet - Lite and compare it with other detectors .",
      "We compare the accuracy - efficiency trade - offs of CornerNet - Lite with three state - of - the - art object detectors , including YOLOv 3 [ 4 5 ] , RetinaNet 2 [ 3 1 ] and CornerNet [ 2 6 ] , on the validation set .",
      "For RetinaNet , we evaluate at different single scale settings , including 3 0 0 , 4 0 0 , 5 0 0 , 6 0 0 , Following the default settings of YOLOv 3 , we evaluate YOLOv 3 at 3 single image scales ( 3 2 0 , 4 1 6 and 6 0 8) .",
      "CornerNet - Squeeze achieves a better accuracy and efficiency ( 3 4 . 4 % at 3 0 ms ) trade - off than YOLOv 3 ( 3 2 . 4 % at 3 9 ms ) .",
      "We are able to train CornerNet - Saccade on only four 1 0 8 0 Ti GPUs with a total of 4 4 GB GPU memory , while CornerNet requires ten Titan X ( PASCAL ) GPUs with a total of 1 2 0 GB GPU memory .",
      "Neither CornerNet nor CornerNet - Saccade uses mixed precision training [ 3 6 ] .",
      "Table 6 : Ablation study on CornerNet - Squeeze . * Note that CornerNet is trained with a much smaller batch size .",
      "However , we find that CornerNet - Squeeze - Saccade does not outperform CornerNet - Squeeze .",
      "On the validation set , CornerNetSqueeze achieves an AP of 3 4 . 4 % , while CornerNetSqueeze - Saccade with k max = 3 0 achieves 3 2 . 7 % .",
      "If we replace the predicted attention map with the ground - truth attention map ( i.e. the object locations are known ) , we improve the AP of CornerNet - Squeeze - Saccade to 3 8 . 0 % , outperforming CornerNet - Squeeze .",
      "Furthermore , CornerNetSqueeze only operates on single scale images , which provides much less room for CornerNet - Squeeze - Saccade to save .",
      "CornerNet - Squeeze - Saccade may process more number of pixels than CornerNet - Squeeze , slowing down the inference time .",
      "Table 7 : CornerNet - Squeeze - Saccade runs slower and is less accurate than Cornernet - Squeeze .",
      "We also compare CornerNet - Lite with CornerNet and YOLOv 3 on COCO test set in Tab . 8 .",
      "CornerNet - Squeeze is faster and more accurate than YOLOv 3 .",
      "CornerNetSaccade is more accurate than CornerNet at multi - scales and 6 times faster .   We propose CornerNet - Lite which is a combination of two efficient variant of CornerNet : CornerNet - Saccade and CornerNet - Squeeze ."
    ],
    "relations": [
      [
        "Adam:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "CornerNet:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "data augmentation:Method",
        "Used-For",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "CornerNet - Saccade:Method"
      ],
      [
        "Hourglass - 1 0 4:Method",
        "Part-Of",
        "CornerNet:Method"
      ],
      [
        "residual blocks:Method",
        "Part-Of",
        "Hourglass - 1 0 4:Method"
      ],
      [
        "3 × 3 convolution:Method",
        "Part-Of",
        "residual blocks:Method"
      ],
      [
        "skip connection:Method",
        "Part-Of",
        "residual blocks:Method"
      ],
      [
        "fire module:Method",
        "SubClass-Of",
        "building block:Method"
      ],
      [
        "building block:Method",
        "Part-Of",
        "SqueezeNet:Method"
      ],
      [
        "fire module:Method",
        "Part-Of",
        "SqueezeNet:Method"
      ],
      [
        "fire module:Method",
        "Part-Of",
        "CornerNet - Squeeze:Method"
      ],
      [
        "3 × 3 depth - wise separable convolution:Method",
        "Part-Of",
        "MobileNets:Method"
      ],
      [
        "residual block:Method",
        "Part-Of",
        "CornerNet:Method"
      ],
      [
        "residual block:Method",
        "Compare-With",
        "fire module:Method"
      ],
      [
        "fire module:Method",
        "Part-Of",
        "CornerNet - Squeeze:Method"
      ],
      [
        "hourglass module:Method",
        "Part-Of",
        "CornerNet - Squeeze:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Lite:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "Compare-With",
        "RetinaNet:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Squeeze - Saccade:Method",
        "Compare-With",
        "CornerNet - Squeeze:Method"
      ],
      [
        "CornerNetSqueeze:Method",
        "Compare-With",
        "CornerNetSqueeze - Saccade:Method"
      ],
      [
        "CornerNet - Squeeze - Saccade:Method",
        "Compare-With",
        "CornerNet - Squeeze:Method"
      ],
      [
        "CornerNetSqueeze:Method",
        "Compare-With",
        "CornerNet - Squeeze - Saccade:Method"
      ],
      [
        "CornerNet - Squeeze - Saccade:Method",
        "Compare-With",
        "CornerNet - Squeeze:Method"
      ],
      [
        "CornerNet - Squeeze - Saccade:Method",
        "Compare-With",
        "Cornernet - Squeeze:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "Compare-With",
        "YOLOv 3:Method"
      ],
      [
        "CornerNet - Lite:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "YOLOv 3:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Compare-With",
        "YOLOv 3 .:Method"
      ],
      [
        "CornerNetSaccade:Method",
        "Compare-With",
        "CornerNet:Method"
      ],
      [
        "CornerNet:Method",
        "Part-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "Part-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "Part-Of",
        "CornerNet - Lite:Method"
      ],
      [
        "CornerNet - Squeeze:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ],
      [
        "CornerNet - Saccade:Method",
        "SubClass-Of",
        "CornerNet:Method"
      ]
    ]
  },
  {
    "doc_id": "202719327",
    "chunk_id": 1,
    "content": [
      "Language model pre - training , such as BERT , has significantly improved the performances of many natural language processing tasks .",
      "To accelerate inference and reduce model size while maintaining accuracy , we first propose a novel Transformer distillation method that is specially designed for knowledge distillation ( KD ) of the Transformer - based models .",
      "By leveraging this new KD method , the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny - BERT .",
      "Then , we introduce a new two - stage learning framework for TinyBERT , which performs Transformer distillation at both the pretraining and task - specific learning stages .",
      "This framework ensures that TinyBERT can capture he general - domain as well as the task - specific knowledge in BERT .",
      "TinyBERT with 4 layers is empirically effective and achieves more than 9 6 . 8 % the performance of its teacher BERTBASE on GLUE benchmark , while being 7. 5 x smaller and 9. 4 x faster on inference .",
      "TinyBERT with 4 layers is also significantly better than 4 - layer state - of - the - art baselines on BERT distillation , with only about 2 8 % parameters and about 3 1 % inference time of them .",
      "Moreover , TinyBERT with 6 layers performs on - par with its teacher BERTBASE .",
      "Pre - training language models then fine - tuning on downstream tasks has become a new paradigm for natural language processing ( NLP ) .",
      "Pre - trained language models ( PLMs ) , such as BERT ( Devlin et al. , 2 0 1 8) , XLNet ( Yang et al. , 2 0 1 9 ) , RoBERTa and SpanBERT , have achieved great success in many NLP tasks ( e.g. , the GLUE benchmark ( Wang et al. , 2 0 1 8) and the challenging multi - hop reasoning task ( Ding et al. , 2 0 1 9 ) ) .",
      "The most commonly used techniques include quantization ( Gong et al. , 2 0 1 4 ) , weights pruning ( Han et al. , 2 0 1 5 b ) , and knowledge distillation ( KD ) ( Romero et al. , 2 0 1 4 ) .",
      "Based on the framework , we propose a novel distillation method specifically for Transformer - based models ( Vaswani et al. , 2 0 1 7 ) , and use BERT as an example to investigate the KD methods for large scale PLMs .",
      "Under review KD has been extensively studied in NLP ( Kim & Rush , 2 0 1 6 ; Hu et al. , 2 0 1 8) , while designing KD methods for BERT has been less explored .",
      "The pre - training - then - fine - tuning paradigm firstly pre - trains BERT on a large scale unsupervised text corpus , then fine - tunes it on task - specific dataset , which greatly increases the difficulty of BERT distillation .",
      "To build a competitive TinyBERT , we firstly propose a new Transformer distillation method to distill the knowledge embedded in teacher BERT .",
      "Specifically , we design several loss functions to fit different representations from BERT layers : 1 ) the output of the embedding layer ; 2 ) the hidden states and attention matrices derived from the Transformer layer ; 3 ) the logits output by the prediction layer .",
      "The attention based fitting is inspired by the recent findings ( Clark et al. , 2 0 1 9 ) that the attention weights learned by BERT can capture substantial linguistic knowledge , which encourages that the linguistic knowledge can be well transferred from teacher BERT to student TinyBERT .",
      "However , it is ignored in existing KD methods of BERT , such as Distilled BiLSTM SOFT , BERT - PKD ( Sun et al. , 2 0 1 9 ) and DistilBERT 2 .",
      "The student TinyBERT learns to mimic the teacher 's behavior by executing the proposed Transformer distillation on the large scale corpus from general domain .",
      "At the task - specific distillation stage , we perform the data augmentation to provide more task - related materials for teacherstudent learning , and then re - execute the Transformer distillation on the augmented data .",
      "TinyBERT ( our method ) The main contributions of this work are as follows : 1 ) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be well transferred to TinyBERT . 2 ) We propose a novel two - stage learning framework with performing the proposed Transformer distillation at both the pre - training and fine - tuning stages , which ensures that Tiny - BERT can capture both the general - domain and task - specific knowledge of the teacher BERT . 3 ) We show experimentally that our TinyBERT can achieve comparable results with teacher BERT on GLUE tasks , while having much fewer parameters ( ∼ 1 3 . 3 % ) and less inference time ( ∼ 1 0 . 6 % ) , and significantly outperforms other state - of - the - art baselines on BERT distillation .",
      "We firstly describe the formulation of Transformer ( Vaswani et al. , 2 0 1 7 ) and Knowledge Distillation ( Hinton et al. , 2 0 1 5 ) .",
      "Our proposed Transformer distillation is a specially designed KD method for Transformer - based models .",
      "Most of recent pre - trained language models ( e.g. , BERT , XLNet and RoBERTa ) are built with Transformer layers , which can capture long - term dependencies between input tokens by self - attention mechanism .",
      "Specifically , a standard Transformer layer includes two main sub - layers : multi - head attention ( MHA ) and fully connected feed - forward network ( FFN ) .",
      "Multi - Head Attention ( MHA ) .",
      "The final function output is calculated as a weighted sum of values V , and the weight is computed by applying softmax ( ) operation on the matrix A. The attention matrices A of BERT can capture substantial linguistic knowledge and plays an essential role in our proposed distillation method .",
      "Position - wise Feed - Forward Network ( FFN ) .",
      "Transformer layer also contains a fully connected feed - forward network , which is formulated as follows : We can see that the FFN contains two linear transformations and one ReLU activation .",
      "In the context of Transformer distillation , the output of MHA layer or FFN layer , or some intermediate representations ( e.g. the attention matrix A ) can be used as behavior function .",
      "Different from previous KD methods , we also need to consider how to perform KD at the pre - training stage of BERT , which further increases the difficulty of KD for BERT ."
    ],
    "relations": [
      [
        "BERT:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "KD:Method",
        "Synonym-Of",
        "knowledge distillation:Method"
      ],
      [
        "Transformer distillation:Method",
        "SubClass-Of",
        "knowledge distillation:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "Transformer:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "Tiny - BERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERTBASE:Method"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "GLUE:Dataset"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT distillation:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERTBASE:Method"
      ],
      [
        "NLP:Task",
        "Synonym-Of",
        "natural language processing:Task"
      ],
      [
        "PLMs:Method",
        "Synonym-Of",
        "Pre - trained language models:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "Pre - trained language models:Method"
      ],
      [
        "XLNet:Method",
        "SubClass-Of",
        "Pre - trained language models:Method"
      ],
      [
        "RoBERTa:Method",
        "SubClass-Of",
        "Pre - trained language models:Method"
      ],
      [
        "SpanBERT:Method",
        "SubClass-Of",
        "Pre - trained language models:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "XLNet:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "RoBERTa:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "SpanBERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "Pre - trained language models:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "GLUE:Dataset",
        "Benchmark-For",
        "NLP:Task"
      ],
      [
        "Pre - trained language models:Method",
        "Used-For",
        "multi - hop reasoning:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "multi - hop reasoning:Task"
      ],
      [
        "XLNet:Method",
        "Used-For",
        "multi - hop reasoning:Task"
      ],
      [
        "RoBERTa:Method",
        "Used-For",
        "multi - hop reasoning:Task"
      ],
      [
        "SpanBERT:Method",
        "Used-For",
        "multi - hop reasoning:Task"
      ],
      [
        "KD:Method",
        "Synonym-Of",
        "knowledge distillation:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "PLMs:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "PLMs:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "KD:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "Distilled BiLSTM SOFT:Method",
        "SubClass-Of",
        "KD:Method"
      ],
      [
        "BERT - PKD:Method",
        "SubClass-Of",
        "KD:Method"
      ],
      [
        "DistilBERT 2:Method",
        "SubClass-Of",
        "KD:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "data augmentation:Method",
        "Used-For",
        "task - specific distillation:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "task - specific distillation:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "Tiny - BERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "GLUE:Dataset"
      ],
      [
        "BERT:Method",
        "Evaluated-With",
        "GLUE:Dataset"
      ],
      [
        "BERT distillation:Method",
        "Evaluated-With",
        "GLUE:Dataset"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT distillation:Method"
      ],
      [
        "Transformer distillation:Method",
        "SubClass-Of",
        "KD:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "Transformer - based models:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "pre - trained language models:Method"
      ],
      [
        "XLNet:Method",
        "SubClass-Of",
        "pre - trained language models:Method"
      ],
      [
        "RoBERTa:Method",
        "SubClass-Of",
        "pre - trained language models:Method"
      ],
      [
        "Transformer:Method",
        "Part-Of",
        "pre - trained language models:Method"
      ],
      [
        "self - attention mechanism:Method",
        "Part-Of",
        "Transformer:Method"
      ],
      [
        "multi - head attention:Method",
        "Part-Of",
        "Transformer layer:Method"
      ],
      [
        "fully connected feed - forward network:Method",
        "Part-Of",
        "Transformer layer:Method"
      ],
      [
        "MHA:Method",
        "Synonym-Of",
        "multi - head attention:Method"
      ]
    ]
  },
  {
    "doc_id": "202719327",
    "chunk_id": 2,
    "content": [
      "In this section , we propose a novel distillation method for Transformer - based models , then present a two - stage learning framework of TinyBERT .",
      "The proposed Transformer distillation is a specially designed KD method for Transformer networks .",
      "Assuming that the student model has M Transformer layers and teacher model has N Transformer layers , we choose M layers from the teacher model for the Transformerlayer distillation .",
      "The embedding - layer distillation and the prediction - layer distillation are also considered .",
      "Thus we propose the attention based distillation to ensure that the linguistic knowledge is transferred from teacher BERT to student TinyBERT .",
      "In this work , the attention matrix A i is used as the fitting target instead of its softmax output softmax(A i ) , since our experiments show that this setting has faster convergence and better performances .",
      "Using the above distillation objectives ( i.e. Equations 7 , 8 , 9 and 1 0 ) , we can unify the loss of model layer : In our experiments , we firstly perform intermediate layer distillation ( M ≥ m ≥ 0 ) , then perform the prediction - layer distillation ( m = M + 1 ) .",
      "General distillation helps student TinyBERT learn the rich knowledge embedded in teacher BERT , which plays an important role in improving the generalization capability of TinyBERT .",
      "By performing the Transformer distillation 5 on the text from general domain , we obtain a general TinyBERT that can be fine - tuned for downstream tasks .",
      "However , due to the big reductions in the hidden/embedding size and the layer number , general TinyBERT performs relatively worse than the original BERT .",
      "To this end , we propose to derive competitive fine - tuned TinyBERTs through the task - specific distillation .",
      "In the task - specific distillation , we re - perform the proposed Transformer distillation on augmented task - specific dataset ( as shown in Figure 2 ) .",
      "Specifically , the fine - tuned big BERT is used as the teacher and a new data augmentation method is proposed to extend the task - specific training set .",
      "In this work , we combine a pre - trained language model BERT and GloVe ( Pennington et al. , 2 0 1 4 ) word embeddings to do word - level replacement for data augmentation .",
      "More details of data augmentation procedure are discussed in Appendix A. The above two learning stages are complementary to each other : the general distillation provides a good initialization for the task - specific distillation , while the task - specific distillation further improves TinyBERT by focusing on learning the task - specific knowledge .",
      "Although there is a big gap between BERT and TinyBERT in model size , by performing the proposed two - stage distillation , the TinyBERT can achieve comparable performances as large BERT in various NLP tasks .",
      "We use g(m ) = 3 × m as the layer mapping function , so TinyBERT learns from every 3 layers of BERT BASE .",
      "We evaluate TinyBERT on the General Language Understanding Evaluation ( GLUE ) ( Wang et al. , 2 0 1 8) benchmark , which is a collection of diverse natural language understanding tasks .",
      "The experiment results demonstrate that : 1 ) There is a large performance gap between BERT SMALL and BERT BASE due to the big reduction in model size . 2 ) TinyBERT is consistently better than BERT SMALL in all the GLUE tasks and achieves a large improvement of 6. 3 % on average .",
      "This indicates that the proposed KD learning framework can effectively improve the performances of small models regardless of downstream tasks . 3 ) TinyBERT significantly outperforms the state - ofthe - art KD baselines ( i.e. , BERT - PKD and DistillBERT ) by a margin of at least 3. 9 % , even with only ∼ 2 8 % parameters and ∼ 3 1 % inference time of baselines ( see Table 3 ) . 4 ) Compared with the teacher BERT BASE , TinyBERT is 7. 5 x smaller and 9. 4 x faster in the model efficiency , while maintaining comparable performances . 5 ) TinyBERT has a comparable model efficiency ( slightly larger in size but faster in inference ) with Distilled BiLSTM SOFT and obtains substantially better performances in all tasks reported by the BiLSTM baseline . 6 ) For the challenging CoLA dataset ( the task of predicting linguistic acceptability judgments ) , all the distilled small models have a relatively bigger performance gap with teacher model .",
      "Moreover , BERT - PKD and DistillBERT initialize their student models with some layers of well pre - trained teacher BERT ( see Table 1 ) , which makes the student models have to keep the same size settings of Transformer layer ( or embedding layer ) as their teacher BERT .",
      "We evaluate how much improvement can be achieved when increasing the model size of TinyBERT on several typical GLUE tasks , where MNLI and MRPC are used in the ablation studies of Devlin et al. ( 2 0 1 8) , and CoLA is the most difficult task in GLUE .",
      "We can observe that : 1 ) All the three TinyBERT variants can consistently outperform the original smallest TinyBERT , which indicates that the proposed KD method works for the student models of various model sizes . 2 ) For the CoLA task , the improvement is slight when only increasing the number of layers ( from 4 9 . 7 to 5 0 . 6 ) or hidden size ( from 4 9 . 7 to 5 0 . 5 ) .",
      "To achieve more dramatic improve - ments , the student model should become deeper and wider ( from 4 9 . 7 to 5 4 . 0 ) . 3 ) Another interesting observation is that the smallest 4 - layer TinyBERT can even outperform the 6 - layers baselines , which further confirms the effectiveness of the proposed KD method .",
      "The proposed two - stage TinyBERT learning framework ( see Figure 2 ) consists of three key procedures : TD ( Task - specific Distillation ) , GD ( General Distillation ) and DA ( Data Augmentation ) .",
      "The TD and DA has comparable effects in all the four tasks .",
      "We can also find the task - specific procedures ( TD and DA ) are more helpful than the pre - training procedure ( GD ) in all the four tasks .",
      "Another interesting observation is that GD has more effect on CoLA than on MNLI and MRPC .",
      "We conjecture that the ability of linguistic generalization ( Warstadt et al. , 2 0 1 8) learned by GD plays a more important role in the downstream CoLA task ( linguistic acceptability judgments ) .",
      "Several baselines are proposed including the TinyBERT learning without the Transformer - layer distillation ( No Trm ) , embedding - layer distillation ( No Emb ) and predictionlayer distillation ( No Pred ) 6 respectively .",
      "The performance drops significantly from 7 5 . 3 to 5 6 . 3 under the setting ( No Trm ) , which indicates Transformer - layer distillation is the key for TinyBERT learning ."
    ],
    "relations": [
      [
        "FFN:Method",
        "Synonym-Of",
        "fully connected feed - forward network:Method"
      ],
      [
        "MHA:Method",
        "Synonym-Of",
        "Multi - Head Attention:Method"
      ],
      [
        "FFN:Method",
        "Synonym-Of",
        "Position - wise Feed - Forward Network:Method"
      ],
      [
        "fully connected feed - forward network:Method",
        "Part-Of",
        "Transformer layer:Method"
      ],
      [
        "linear transformations:Method",
        "Part-Of",
        "FFN:Method"
      ],
      [
        "ReLU activation:Method",
        "Part-Of",
        "FFN:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "Transformer distillation:Method",
        "SubClass-Of",
        "KD:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "Transformer:Method"
      ],
      [
        "attention based distillation:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "attention based distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "task - specific distillation:Method",
        "Used-For",
        "TinyBERTs:Method"
      ],
      [
        "Transformer distillation:Method",
        "Used-For",
        "task - specific distillation:Method"
      ],
      [
        "data augmentation:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "pre - trained language model:Method"
      ],
      [
        "GloVe:Method",
        "Used-For",
        "data augmentation:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "data augmentation:Method"
      ],
      [
        "task - specific distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "BERT:Method",
        "Compare-With",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "TinyBERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT BASE:Method"
      ],
      [
        "GLUE:Dataset",
        "Synonym-Of",
        "General Language Understanding Evaluation:Dataset"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "General Language Understanding Evaluation:Dataset"
      ],
      [
        "General Language Understanding Evaluation:Dataset",
        "Benchmark-For",
        "natural language understanding:Task"
      ],
      [
        "TinyBERT:Method",
        "Used-For",
        "natural language understanding:Task"
      ],
      [
        "BERT SMALL:Method",
        "Compare-With",
        "BERT BASE:Method"
      ],
      [
        "GLUE:Dataset",
        "Evaluated-With",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT SMALL:Method"
      ],
      [
        "GLUE:Dataset",
        "Evaluated-With",
        "BERT SMALL:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT - PKD:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "BERT - PKD:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "DistillBERT:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "DistillBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT BASE:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "Distilled BiLSTM SOFT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BiLSTM:Method"
      ],
      [
        "Transformer layer:Method",
        "Part-Of",
        "BERT - PKD:Method"
      ],
      [
        "Transformer layer:Method",
        "Part-Of",
        "DistillBERT:Method"
      ],
      [
        "DistillBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "BERT - PKD:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "Transformer layer:Method",
        "Part-Of",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "GLUE:Dataset"
      ],
      [
        "MNLI:Dataset",
        "SubClass-Of",
        "GLUE:Dataset"
      ],
      [
        "MRPC:Dataset",
        "SubClass-Of",
        "GLUE:Dataset"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "MNLI:Dataset"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "MRPC:Dataset"
      ],
      [
        "TinyBERT:Method",
        "Evaluated-With",
        "CoLA:Dataset"
      ],
      [
        "CoLA:Dataset",
        "SubClass-Of",
        "GLUE:Dataset"
      ],
      [
        "KD:Method",
        "Used-For",
        "TinyBERT variants:Method"
      ],
      [
        "TinyBERT variants:Method",
        "Compare-With",
        "TinyBERT:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "4 - layer TinyBERT:Method"
      ],
      [
        "TD:Method",
        "Part-Of",
        "two - stage TinyBERT learning framework:Method"
      ]
    ]
  },
  {
    "doc_id": "202719327",
    "chunk_id": 3,
    "content": [
      "Meanwhile , these two kinds of knowledge distillation are complementary to each other , which makes TinyBERT obtain the competitive results .",
      "We find that the top - strategy performs better than the bottom - strategy in MNLI , while being worse in MRPC and CoLA tasks , which confirms the observations that different tasks depend on the different kinds of knowledge from BERT layers .",
      "We also evaluate TinyBERT on the question answering tasks , and study whether we can use BERT SMALL as the initialization of the general TinyBERT .",
      "The experiments are detailed in Appendix C and D. In this paper , we firstly introduce a new KD method for Transformer - based distillation , then we further propose a two - stage framework for TinyBERT learning .",
      "Extensive experiments show that the TinyBERT achieves competitive performances meanwhile significantly reducing the model size and shortening the inference time of original BERT BASE , which provides an effective way to deploy BERT - based NLP applications on the edge devices .",
      "In future work , we would study how to effectively transfer the knowledge from wider and deeper teachers ( e.g. , BERT LARGE and XLNet LARGE ) to student TinyBERT .",
      "The joint learning of distillation and quantization/pruning would be another promising direction to further compress the pre - trained language models .   In this section , we explain the proposed data augmentation method .",
      "TinyBERT learning includes the general distillation and the task - specific distillation .",
      "For the general distillation , we use English Wikipedia ( 2, 5 0 0 M words ) as the text corpus and perform the intermediate layer distillation for 3 epochs with the supervision from a pre - trained BERT BASE teacher and keep other hyper - parameters same as BERT pre - training ( Devlin et al. , 2 0 1 8) .",
      "For task - specific distillation , we firstly perform intermediate layer distillation on the augmented dataset for 1 0 epochs with batch size 3 2 and learning rate 5e - 5 under the supervision of a fine - tuned BERT teacher , and then perform prediction layer distillation for 3 epochs with batch size 3 2 and learning rate 3e - 5 .",
      "For tasks like MNLI , QQP and QNLI which have ≥ 1 0 0 K training examples , we distill intermediate layer knowledge for 5 epochs with batch size 2 5 6 on the augmented dataset .",
      "Besides , for CoLA task , we perform 5 0 epochs of intermediate layer distillation .",
      "We use BERT - PKD and DistilBERT as our baselines .",
      "For a fair comparison , we firstly re - implemented the results of BERT - PKD and DistilBERT reported in their papers to ensure our implementation procedure is correct .",
      "Then following the verified implementation procedure , we trained a 4 - layer BERT - PKD and a 4 - layer DistilBERT as the baselines .",
      "The BERT SMALL learning strictly follows the same learning strategy as described in the original BERT work ( Devlin et al. , 2 0 1 8) .",
      "The GLUE datasets are described as follows : MNLI .",
      "The Stanford Sentiment Treebank is a binary single - sentence classification task , where the goal is to predict the sentiment of movie reviews ( Socher et al. , 2 0 1 3 ) .",
      "C SQUAD 1. 1 AND 2. 0 We also demonstrate the effectiveness of TinyBERT on the question answering ( QA ) tasks : SQuAD v 1 . 1 ( Rajpurkar et al. , 2 0 1 6 ) and v 2 . 0 ( Rajpurkar et al. , 2 0 1 8) .",
      "We follow the settings of task - specific distillation in GLUE tasks , except with 3 running epochs and a learning rate of 5e - 5 for the prediction - layer distillation on the original training dataset .",
      "Compared with sequence - level GLUE tasks , the question answering tasks depends on more subtle knowledge to infer the correct answer , which increases the difficulty of knowledge distillation .",
      "Initializing general TinyBERT with BERT SMALL is a straightforward idea .",
      "However , BERT SMALL would derive mismatched distributions in intermediate representations ( e.g. , attention matrices and hidden states ) with the teacher BERT BASE model , if without imitating the teacher 's behaviors at the pre - training stage .",
      "Further task - specific distillation under the supervision of BERT BASE will disturb the learned distribution/knowledge of BERT SMALL , finally leading to poor performances in some less - data tasks .",
      "The results in Table 9 , show that the BERT SMALL ( MLM&NSP+TD ) performs worse than the BERT SMALL in MRPC and CoLA tasks , which validates our hypothesis .",
      "For the intensive - data task ( e.g. MNLI ) , TD learning has enough learning materials to make BERT SMALL acquire the task - specific knowledge very well , although the pre - trained distributions have already been disturbed .",
      "To make TinyBERT effectively work for all tasks , we propose General Distillation ( GD ) for initialization , where the TinyBERT learns the knowledge from intermediate layers of teacher BERT at the pre - training stage .",
      "From the results of Table 9 , we find that GD can effectively transfer the knowledge from the teacher BERT to the student TinyBERT and achieve comparable results with BERT SMALL ( 6 1 . 1 vs 6 3 . 9 ) , even without performing the MLM and NSP tasks .",
      "Furthermore , the task - specific distillation boosts the performances of TinyBERT by continuing on learning the taskspecific knowledge of fine - tuned teacher BERT BASE ."
    ],
    "relations": [
      [
        "GD:Method",
        "Part-Of",
        "two - stage TinyBERT learning framework:Method"
      ],
      [
        "DA:Method",
        "Part-Of",
        "two - stage TinyBERT learning framework:Method"
      ],
      [
        "TD:Method",
        "Synonym-Of",
        "Task - specific Distillation:Method"
      ],
      [
        "GD:Method",
        "Synonym-Of",
        "General Distillation:Method"
      ],
      [
        "DA:Method",
        "Synonym-Of",
        "Data Augmentation:Method"
      ],
      [
        "TD:Method",
        "Compare-With",
        "DA:Method"
      ],
      [
        "TD:Method",
        "SubClass-Of",
        "task - specific procedures:Method"
      ],
      [
        "DA:Method",
        "SubClass-Of",
        "task - specific procedures:Method"
      ],
      [
        "GD:Method",
        "SubClass-Of",
        "pre - training procedure:Method"
      ],
      [
        "task - specific procedures:Method",
        "Compare-With",
        "pre - training procedure:Method"
      ],
      [
        "GD:Method",
        "Evaluated-With",
        "CoLA:Dataset"
      ],
      [
        "GD:Method",
        "Evaluated-With",
        "MNLI:Dataset"
      ],
      [
        "GD:Method",
        "Evaluated-With",
        "MRPC:Dataset"
      ],
      [
        "GD:Method",
        "Evaluated-With",
        "CoLA:Dataset"
      ],
      [
        "CoLA:Dataset",
        "Benchmark-For",
        "linguistic acceptability judgments:Task"
      ],
      [
        "GD:Method",
        "Used-For",
        "linguistic acceptability judgments:Task"
      ],
      [
        "Transformer - layer distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "knowledge distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Used-For",
        "question answering:Task"
      ],
      [
        "BERT SMALL:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "KD:Method",
        "Used-For",
        "Transformer - based distillation:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT BASE:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT LARGE:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "XLNet LARGE:Method"
      ],
      [
        "task - specific distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "general distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "intermediate layer distillation:Method",
        "Trained-With",
        "English Wikipedia:Dataset"
      ],
      [
        "intermediate layer distillation:Method",
        "Used-For",
        "BERT BASE:Method"
      ],
      [
        "intermediate layer distillation:Method",
        "Used-For",
        "task - specific distillation:Method"
      ],
      [
        "prediction layer distillation:Method",
        "Used-For",
        "task - specific distillation:Method"
      ],
      [
        "intermediate layer distillation:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "intermediate layer distillation:Method",
        "Trained-With",
        "CoLA:Dataset"
      ],
      [
        "BERT - PKD:Method",
        "Compare-With",
        "DistilBERT:Method"
      ],
      [
        "BERT SMALL:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "MNLI:Dataset",
        "SubClass-Of",
        "GLUE:Dataset"
      ],
      [
        "Stanford Sentiment Treebank:Dataset",
        "Benchmark-For",
        "binary single - sentence classification:Task"
      ],
      [
        "TinyBERT:Method",
        "Trained-With",
        "SQUAD 1. 1 AND 2. 0:Dataset"
      ],
      [
        "QA:Task",
        "Synonym-Of",
        "question answering:Task"
      ],
      [
        "SQUAD 1. 1 AND 2. 0:Dataset",
        "Benchmark-For",
        "question answering:Task"
      ],
      [
        "TinyBERT:Method",
        "Used-For",
        "question answering:Task"
      ],
      [
        "BERT SMALL:Method",
        "SubClass-Of",
        "BERT BASE:Method"
      ],
      [
        "task - specific distillation:Method",
        "Used-For",
        "BERT BASE:Method"
      ],
      [
        "BERT SMALL:Method",
        "SubClass-Of",
        "BERT BASE:Method"
      ],
      [
        "BERT SMALL:Method",
        "Used-For",
        "MLM&NSP+TD:Task"
      ],
      [
        "BERT SMALL:Method",
        "Compare-With",
        "BERT SMALL:Method"
      ],
      [
        "BERT SMALL:Method",
        "Evaluated-With",
        "MRPC:Dataset"
      ],
      [
        "BERT SMALL:Method",
        "Evaluated-With",
        "MRPC:Dataset"
      ],
      [
        "BERT SMALL:Method",
        "Evaluated-With",
        "CoLA:Dataset"
      ],
      [
        "BERT SMALL:Method",
        "Evaluated-With",
        "CoLA:Dataset"
      ],
      [
        "TD:Method",
        "Used-For",
        "BERT SMALL:Method"
      ],
      [
        "General Distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "GD:Method",
        "Synonym-Of",
        "General Distillation:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT:Method"
      ],
      [
        "GD:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "GD:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "Compare-With",
        "BERT SMALL:Method"
      ],
      [
        "task - specific distillation:Method",
        "Used-For",
        "TinyBERT:Method"
      ],
      [
        "TinyBERT:Method",
        "SubClass-Of",
        "BERT BASE:Method"
      ]
    ]
  },
  {
    "doc_id": "199668729",
    "chunk_id": 1,
    "content": [
      "An end - to - end multi - task network ( MTN ) is designed to perform human detection , pose estimation , and person re - identification ( Re - ID ) tasks simultaneously .",
      "To alleviate the performance bottleneck caused by scale variation problem , a paradigm which exploits scale - normalized image and feature pyramids ( SIFP ) is proposed to boost both performance and speed .",
      "Given the results of MTN , we adopt an occlusion - aware Re - ID feature strategy in the pose tracking module , where pose information is utilized to infer the occlusion state to make better use of Re - ID feature .",
      "In experiments , we demonstrate that the pose estimation and tracking performance improves steadily utilizing SIFP through different backbones .",
      "Using ResNet - 1 8 and ResNet - 5 0 as backbones , the overall pose tracking framework achieves competitive performance with 2 9 . 4 FPS and 1 2 . 2 FPS , respectively .",
      "Methods involved are PoseTrack [ 2 8 ] , JointFlow [ 1 6 ] , PoseFlow [ 5 3 ] , Detect - and - Track [ 2 1 ] , FlowTrack [ 5 0 ] , and our FastPose framework with various backbones .",
      "Previous pose estimation systems address single prelocated person , which exploit pictorial structures model [ 4 , 1 8 ] and following deep convolutional neural networks ( DC - NNs ) approaches [ 4 7 , 4 6 , 4 8 , 3 6 , 5 6 ] .",
      "Motivated by practical applications in video surveillance , human - computer interaction and action recognition , researchers now focus on the multi - person pose estimation in unconstrained environment .",
      "Generally , two - stages methods achieve the state - of - theart results both on pose estimation and pose tracking tasks , beyond the performance of unified approach .",
      "Based on the detection result of the first stage , the second stage only focuses on the task of keypoint detection on a fixed scale .",
      "Despite the leading performance , these methods ca n't perform real - time inference as their complex procedures , including human detection , cropping and scaling images , and pose estimation .",
      "Although many methods [ 3 3 , 2 3 ] have been proposed to alleviate scale variation problem in face detection or object detection area , there are few researches focusing on dealing with the scale variation in unified multi - person pose estimation .",
      "Different from multiple object tracking that focuses on instance identification assignment , pose tracking aims to address a more complex problem of articulated multi - person pose tracking in videos .",
      "Based on the top - down pose estimation methods , [ 5 0 ] exploits flow - based pose similarity as metric and solves the matching problem in a greedy fashion . [ 2 1 ] proposes a 3D extension of Mask R - CNN , which predicts the location of person tubes and corresponding poses simultaneously .",
      "Based on the above analyses , this paper develops FastPose , a pose tracking framework which can perform pose estimation and tracking towards real - time speed .",
      "Specifically , we first build a multi - task network ( MTN ) which jointly optimizes three tasks simultaneously , including human detection , pose estimation , and person Re - ID .",
      "The main contributions of this paper can be described as follows : ( 1 ) Taking the person Re - ID features into account , we design an end - to - end multi - task network which performs human detection , pose estimation , and person Re - ID simultaneously .",
      "The network 's outputs provide the necessary informations for the following pose tracking strategy . ( 2 ) We propose a paradigm named scale - normalized image and feature pyramid ( SIFP ) for alleviating scale variation problem which is the performance bottleneck of unified top - down pose estimation methods .",
      "Combining feature pyramid networks ( FPN ) with the scale distribution can help the network to avoid multi - scale testing . ( 3 ) Utilizing the outputs of our multi - task network , an occlusion - aware strategy is exploited to perform articulated multi - person pose tracking in videos .",
      "Specifically , the pose information is utilized to infer occlusion state and achieve the occlusion - aware Re - ID strategy which dramatically reduce the identification ( ID ) switches during tracking .    Pose estimation has underwent a long way as a basic research topic of computer vision .",
      "CPN [ 1 2 ] is the leading method on COCO 2 0 1 7 keypoint challenge .",
      "It involves skip layer feature concatenation and an online hard keypoint mining step . [ 5 0 ] adopts FPN - DCN as the human detector and adds a few deconvolutional layers on single - person pose estimation network to improve the performance .",
      "Based on the multi - person pose estimation approaches described above , it is natural to extend them to multiperson pose tracking in video .",
      "In [ 2 8 , 2 6 ] , authors firstly estimate human pose with a bottom - up method , and then transform the problem into solving an energy minimizing function over a spatiotemporal graph constructed on the detected joints . [ 1 6 ] proposes a model to predict Temporal Flow Fields ( TTF ) to formulate a similarity measure of detected joints .",
      "Based on the top - down pose estimation methods , [ 2 1 ] proposes an extended Mask R - CNN and solves the bipartite graph matching problem based on IoU. [ 5 0 ] exploits flow - based pose similarity as metric and solves the matching problem in a greedy fashion .",
      "Based on the obtained pose of single person , [ 5 3 ] proposes to construct pose flow and perform pose flow non maximum suppression ( NMS ) to eliminate issues like ID switches .",
      "Multi - task learning [ 9 , 6 0 , 2 0 , 3 2 ] has been used successfully in applications of natural language processing [ 1 3 , 4 2 ] , speech recognition [ 1 5 ] , computer vision [ 2 2 , 6 1 , 5 2 ] ."
    ],
    "relations": [
      [
        "MTN:Method",
        "Synonym-Of",
        "multi - task network:Method"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "human detection:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "person re - identification:Task"
      ],
      [
        "Re - ID:Task",
        "Synonym-Of",
        "person re - identification:Task"
      ],
      [
        "SIFP:Method",
        "Synonym-Of",
        "scale - normalized image and feature pyramids:Method"
      ],
      [
        "MTN:Method",
        "Used-For",
        "Re - ID:Task"
      ],
      [
        "pose tracking module:Method",
        "Used-For",
        "Re - ID:Task"
      ],
      [
        "SIFP:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "SIFP:Method",
        "Used-For",
        "tracking:Task"
      ],
      [
        "ResNet - 5 0:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "ResNet - 1 8:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "deep convolutional neural networks:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "DC - NNs:Method",
        "Synonym-Of",
        "deep convolutional neural networks:Method"
      ],
      [
        "pose tracking:Task",
        "Compare-With",
        "multiple object tracking:Task"
      ],
      [
        "FastPose:Method",
        "SubClass-Of",
        "pose tracking framework:Method"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "tracking towards real - time speed:Task"
      ],
      [
        "MTN:Method",
        "Synonym-Of",
        "multi - task network:Method"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "human detection:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "human detection:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "SIFP:Method",
        "Synonym-Of",
        "scale - normalized image and feature pyramid:Method"
      ],
      [
        "scale - normalized image and feature pyramid:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "FPN:Method",
        "Synonym-Of",
        "feature pyramid networks:Method"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "multi - person pose tracking:Task"
      ],
      [
        "occlusion - aware strategy:Method",
        "Used-For",
        "multi - person pose tracking:Task"
      ],
      [
        "Pose estimation:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "CPN:Method",
        "Evaluated-With",
        "COCO 2 0 1 7:Dataset"
      ],
      [
        "FPN - DCN:Method",
        "SubClass-Of",
        "human detector:Method"
      ],
      [
        "deconvolutional layers:Method",
        "Part-Of",
        "single - person pose estimation network:Method"
      ],
      [
        "multi - person pose estimation approaches:Method",
        "Used-For",
        "multiperson pose tracking:Task"
      ],
      [
        "TTF:Method",
        "Synonym-Of",
        "Temporal Flow Fields:Method"
      ],
      [
        "Mask R - CNN:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "NMS:Method",
        "Synonym-Of",
        "non maximum suppression:Method"
      ],
      [
        "Multi - task learning:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "Multi - task learning:Method",
        "Used-For",
        "speech recognition:Task"
      ],
      [
        "Multi - task learning:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "multi - task learning:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "Mask R - CNN:Method",
        "Used-For",
        "detect objects:Task"
      ],
      [
        "face detection:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "object detection:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "pose estimation:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "small object detection:Task"
      ],
      [
        "dilated/atrous convolutions:Method",
        "Part-Of",
        "object detector:Method"
      ],
      [
        "MTN:Method",
        "Synonym-Of",
        "multi - task network:Method"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "predict the bounding boxes:Task"
      ],
      [
        "multi - task network:Method",
        "Used-For",
        "keypoint coordinates:Task"
      ]
    ]
  },
  {
    "doc_id": "199668729",
    "chunk_id": 2,
    "content": [
      "Especially in many computer vision tasks , the effectiveness of multi - task learning has been proved .",
      "Fast R - CNN [ 2 2 ] and Faster R - CNN [ 4 0 ] jointly predict the class and the coordinates of objects in an image .",
      "Mask R - CNN [ 2 4 ] can efficiently detect objects in an image while simultaneously generating a high - quality segmentation mask for each instance .",
      "Large scale variation is one of major factors to influence the performance of many computer vision tasks like face detection , object detection and pose estimation .",
      "To address the problem that large strides of deep CNNs make small object detection very difficult , object detector [ 1 0 , 1 4 ] use dilated/atrous convolutions to increase the resolution of the feature map .",
      "SDP [ 5 4 ] , SSH [ 3 5 ] and MS - CNN [ 7 ] make predictions of small objects on the lower layer and big objects on the higher layers respectively .",
      "Furthermore , methods like FPN [ 3 3 ] and Mask - RCNN [ 2 4 ] propose a pyramidal representation and fuse adjacent scale feature maps to combine features which have semantic and detail informations .",
      "Besides , some methods , like SNIP [ 4 4 ] and SNIPER [ 4 5 ] , propose advanced and efficient data argumentation methods to illustrate the scale variation problem .",
      "Given an original image as input , the multi - task network ( MTN ) can predict the bounding boxes , keypoint coordinates and Re - ID features in the scene .",
      "A scale - normalized image pyramid and feature pyramid ( SIFP ) paradigm is exploited to alleviate the scale variation problem of MTN .",
      "The MTN adopts the similar unified procedure as Mask R - CNN .",
      "A fully convolutional network , called a Region Proposal Network ( RPN ) , is built upon these feature maps to propose candidate human bounding boxes .",
      "Based on the candidate boxes and their corresponding features extracted from the sharing feature maps , Mask R - CNN has two branches , one branch performs classification and bounding - box regression .",
      "Network Architecture : Similar with Mask R - CNN , our proposed network can be instantiated with multiple architectures : ( i ) the backbone network used for feature extraction over an entire image , and ( ii ) the head networks for human detection ( bounding - box classification and regression ) , pose estimation and person Re - ID that are applied separately to each RoI. For the backbone network , deeper architecture gains the effectiveness of extracted features , but brings longer training and inference time .",
      "To provide a trade - off between accuracy and speed when MTN is adopted in practical applications , we evaluate MobileNet - v 2 [ 4 3 ] and ResNet [ 2 5 ] with FPN [ 3 3 ] of depth 1 8 , 5 0 and 1 0 1 layers .",
      "For the pose estimation head network , Mask R - CNN adopts a straightforward structure , which limits the precision of keypoints localization .",
      "In Mask R - CNN , 1 4 × 1 4 feature maps of 5 1 2 channels are extracted by RoIAlign for each proposal .",
      "In MTN , we utilize a padding operation to maintaining the ratio of the person in the 2 2 × 1 6 feature maps extracted by RoIAlign .",
      "As this head network is based upon the backbone and RPN , so it need the training data composed by images within multi - person and corresponding ID annotation , like some person search datasets [ 5 1 , 6 2 ] .",
      "The MTN can provide necessary informations to the occlusion - aware strategy introduced in Sec. 3. 3 to perform pose tracking .",
      "As described above , MTN performs human detection , pose estimation and person Re - ID simultaneously .",
      "However , MTN , a unified network , builds all the head networks upon the RoIs generated by RPN .",
      "So inspired by [ 4 4 ] , we develop a scalenormalized paradigm exploiting both image pyramid and feature pyramid ( SIFP ) to achieve enhanced scale invariance capability of MTN .",
      "To maxmize the inference speed without reducing performance , SIFP exploits FPN to tackle this dilemma .",
      "Due to FPN , MTN enhances scale invariance capability to alleviate the domain - shift brought by single scale testing .",
      "In conclusion , SIFP is a modified version of SNIP .",
      "Combining with FPN helps SNIP to avoid slower inference speed brought by multi - scale testing ."
    ],
    "relations": [
      [
        "multi - task network:Method",
        "Used-For",
        "Re - ID:Task"
      ],
      [
        "SIFP:Method",
        "Synonym-Of",
        "scale - normalized image pyramid and feature pyramid:Method"
      ],
      [
        "Region Proposal Network:Method",
        "SubClass-Of",
        "fully convolutional network:Method"
      ],
      [
        "RPN:Method",
        "Synonym-Of",
        "Region Proposal Network:Method"
      ],
      [
        "Mask R - CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Mask R - CNN:Method",
        "Used-For",
        "bounding - box regression:Task"
      ],
      [
        "bounding - box classification and regression:Method",
        "Used-For",
        "human detection:Task"
      ],
      [
        "head networks:Method",
        "Used-For",
        "human detection:Task"
      ],
      [
        "head networks:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "head networks:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "FPN:Method",
        "Part-Of",
        "MobileNet - v 2:Method"
      ],
      [
        "FPN:Method",
        "Part-Of",
        "ResNet:Method"
      ],
      [
        "Mask R - CNN:Method",
        "Part-Of",
        "pose estimation head network:Method"
      ],
      [
        "RoIAlign:Method",
        "Part-Of",
        "Mask R - CNN:Method"
      ],
      [
        "RoIAlign:Method",
        "Part-Of",
        "MTN:Method"
      ],
      [
        "padding operation:Method",
        "Part-Of",
        "MTN:Method"
      ],
      [
        "RPN:Method",
        "Part-Of",
        "head network:Method"
      ],
      [
        "head network:Method",
        "Used-For",
        "person search:Task"
      ],
      [
        "MTN:Method",
        "Used-For",
        "occlusion - aware strategy:Method"
      ],
      [
        "MTN:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "occlusion - aware strategy:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "MTN:Method",
        "Used-For",
        "human detection:Task"
      ],
      [
        "MTN:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "MTN:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "head networks:Method",
        "Part-Of",
        "MTN:Method"
      ],
      [
        "RPN:Method",
        "Part-Of",
        "head networks:Method"
      ],
      [
        "SIFP:Method",
        "Synonym-Of",
        "scalenormalized paradigm exploiting both image pyramid and feature pyramid:Method"
      ],
      [
        "FPN:Method",
        "Part-Of",
        "SIFP:Method"
      ],
      [
        "FPN:Method",
        "Part-Of",
        "MTN:Method"
      ],
      [
        "SIFP:Method",
        "SubClass-Of",
        "SNIP:Method"
      ],
      [
        "FPN:Method",
        "Part-Of",
        "SNIP:Method"
      ],
      [
        "MTN:Method",
        "Used-For",
        "detection box:Task"
      ],
      [
        "MTN:Method",
        "Used-For",
        "keypoints:Task"
      ],
      [
        "MTN:Method",
        "Used-For",
        "Re - ID:Task"
      ],
      [
        "occlusion - aware strategy:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "pose estimation head:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "Re - ID head:Method"
      ],
      [
        "MPII:Dataset",
        "Benchmark-For",
        "pose estimation:Task"
      ],
      [
        "PoseTrack:Dataset",
        "Benchmark-For",
        "pose estimation:Task"
      ],
      [
        "SSM:Dataset",
        "Benchmark-For",
        "Person search:Task"
      ],
      [
        "PRW:Dataset",
        "Benchmark-For",
        "Person search:Task"
      ],
      [
        "PRW:Dataset",
        "Benchmark-For",
        "person Re - ID:Task"
      ],
      [
        "SSM:Dataset",
        "Benchmark-For",
        "person Re - ID:Task"
      ],
      [
        "non - maximum suppression:Method",
        "Used-For",
        "detection boxes:Task"
      ],
      [
        "non - maximum suppression:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "non - maximum suppression:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "non - maximum suppression:Method",
        "Used-For",
        "Re - ID:Task"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "PoseTrack:Dataset",
        "Benchmark-For",
        "pose estimation:Task"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "PoseTrack:Dataset",
        "Benchmark-For",
        "pose tracking:Task"
      ]
    ]
  },
  {
    "doc_id": "199668729",
    "chunk_id": 3,
    "content": [
      "Based on the detection box , keypoints and Re - ID feature provided by MTN , pose tracking is performed by an occlusion - aware strategy .",
      "As described in Sec. 3. 1 , using a deeper backbone network ( ResNet - 5 0 or ResNet - 1 0 1 ) , the numbers of convolutional layers in pose estimation head and Re - ID head are 8 and 4 respectively .",
      "When using a smaller backbone ( ResNet - 1 8 or MobileNet - v 2 ) , they are changed to 4 and 2 .",
      "MPII [ 3 ] and PoseTrack [ 2 ] datasets are utilized for training pose estimation task .",
      "Person search datasets including SSM [ 5 1 ] and PRW [ 6 2 ] datasets are for training person Re - ID task .",
      "Utilizing non - maximum suppression , the highest scoring 1 0 0 detection boxes are fed into pose estimation and person Re - ID branches to obtain the heat maps of K keypoints and 1 2 8 - d Re - ID feature for each human boxes .",
      "In this section , we perform thorough ablation experiments for both pose estimation and pose tracking tasks , and compare our FastPose framework with the state - of - theart methods on PoseTrack [ 2 ] dataset .",
      "Pose estimation task is evaluated on 5k validation images ( minival ) of COCO [ 3 4 ] dataset and PoseTrack [ 2 ] dataset .",
      "Backbone Architecture and SIFP for FastPose : As shown in Table 1 ( b ) , our proposed FastPose also shows steady improvement by using deeper backbone models . mAP and MOTA are two main metrics on PoseTrack dataset .",
      "Using MobileNet - v 2 or ResNet 1 8 as the backbone , FastPose can achieve real - time pose tracking .",
      "Although FastPose - MobileNet - v 2 has lower metric ( 6 2 . 1 on mAP and 5 5 . 6 on MOTA ) than FastPose - 1 8 ( 6 3 . 1 and 5 6 . 8) , its properties make it particularly suitable for mobile applications .",
      "It proves SIFP can stably improve the performance of pose estimation and tracking on PoseTrack dataset .",
      "Pose estimation performance of FastPose on COCO dataset is reported in the supplementary material due to the page limit .",
      "Besides , we evaluate MTN on the person Re - ID dataset SSM and the mAP on SSM test is 8 9 . 3 8 , which suggests that it is feasible to extract Re - ID features in MTN .",
      "More complex designs have the potential to improve performance but are not the focus of this work .   we compare our FastPose framework with the state - ofthe - art methods on PoseTrack Dataset [ 2 ] , including PoseTrack [ 2 ] , JointFlow [ 1 6 ] , PoseFlow [ 5 3 ] , Detect - and - Track [ 2 1 ] and FlowTrack [ 5 0 ] . top - down or bottom - up approach , are all have lower mAP and slower speed than FastPose - 5 0 or FastPose - 1 0 1 .",
      "On PoseTrack val , Only FlowTrack - 1 5 2 with Flow has 6 5 . 4 MOTA higher than 6 3 . 2 of our FastPose - 1 0 1 .",
      "But its slower detector FPN - DCN and the optical flow estimation take much inference time , which causes the speed of FlowTrack - 1 5 2 is only 0. 2 FPS .",
      "Although using Flow and adopting FPN - DCN as human detector , FlowTrack - 5 0 achieves MOTA of 6 2 . 9 which is still caught up by our FastPose - 5 0 with MOTA of 6 2 . 8 .",
      "On PoseTrack test , FastPose - 5 0 and FastPose - 1 0 1 achieve MOTA of 5 6 . 6 and 5 7 . 4 , which are close to the state - of - the - art performance .",
      "The inference time of FastPose comes from two aspects : MTN and tracking strategy .",
      "In this paper , we present FastPose , a fast and unified pose estimation and tracking framework , which utilizes a multi - task network ( MTN ) to integrates three tasks together .",
      "An occlusion - aware strategy following MTN performs pose tracking .",
      "Besides , a paradigm named Scale - normalized Image and Feature Pyramid ( SIFP ) is designed to deal with severe scale variation widely existed in unified pose approaches .",
      "In ablation studies , we prove the stable improvements brought by MTN , SIFP and occlusion - aware strategy .",
      "Moreover , with different configurations , FastPose can achieve real - time inference or competitive performance , which is helpful to adopt pose tracking in actual scenarios ."
    ],
    "relations": [
      [
        "FastPose:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "PoseTrack:Dataset",
        "Benchmark-For",
        "Pose estimation:Task"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "Pose estimation:Task"
      ],
      [
        "SIFP:Method",
        "Part-Of",
        "FastPose:Method"
      ],
      [
        "FastPose:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "ResNet 1 8:Method",
        "Part-Of",
        "FastPose:Method"
      ],
      [
        "MobileNet - v 2:Method",
        "Part-Of",
        "FastPose:Method"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "real - time pose tracking:Task"
      ],
      [
        "FastPose - MobileNet - v 2:Method",
        "Compare-With",
        "FastPose - 1 8:Method"
      ],
      [
        "SIFP:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "PoseTrack:Dataset",
        "Benchmark-For",
        "pose estimation:Task"
      ],
      [
        "SIFP:Method",
        "Used-For",
        "tracking:Task"
      ],
      [
        "PoseTrack:Dataset",
        "Benchmark-For",
        "tracking:Task"
      ],
      [
        "SIFP:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "Pose estimation:Task"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "Pose estimation:Task"
      ],
      [
        "FastPose:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "MTN:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "SSM:Dataset",
        "Benchmark-For",
        "person Re - ID:Task"
      ],
      [
        "SSM:Dataset",
        "Benchmark-For",
        "person Re - ID:Task"
      ],
      [
        "MTN:Method",
        "Evaluated-With",
        "SSM:Dataset"
      ],
      [
        "MTN:Method",
        "Evaluated-With",
        "SSM:Dataset"
      ],
      [
        "MTN:Method",
        "Used-For",
        "Re - ID:Task"
      ],
      [
        "FastPose:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "PoseTrack:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "JointFlow:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "PoseFlow:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "Detect - and - Track:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "FlowTrack:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "FastPose:Method",
        "Compare-With",
        "PoseTrack:Method"
      ],
      [
        "FastPose:Method",
        "Compare-With",
        "JointFlow:Method"
      ],
      [
        "FastPose:Method",
        "Compare-With",
        "PoseFlow:Method"
      ],
      [
        "FastPose:Method",
        "Compare-With",
        "Detect - and - Track:Method"
      ],
      [
        "FastPose:Method",
        "Compare-With",
        "FlowTrack:Method"
      ],
      [
        "FlowTrack - 1 5 2 with Flow:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "FlowTrack - 1 5 2 with Flow:Method",
        "Compare-With",
        "FastPose - 1 0 1:Method"
      ],
      [
        "FPN - DCN:Method",
        "Compare-With",
        "FlowTrack - 1 5 2:Method"
      ],
      [
        "optical flow estimation:Method",
        "Compare-With",
        "FlowTrack - 1 5 2:Method"
      ],
      [
        "Flow:Method",
        "Part-Of",
        "FPN - DCN:Method"
      ],
      [
        "FPN - DCN:Method",
        "SubClass-Of",
        "human detector:Method"
      ],
      [
        "FlowTrack - 5 0:Method",
        "Compare-With",
        "FastPose - 5 0:Method"
      ],
      [
        "FastPose - 5 0:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "FastPose - 1 0 1:Method",
        "Evaluated-With",
        "PoseTrack:Dataset"
      ],
      [
        "multi - task network:Method",
        "Part-Of",
        "FastPose:Method"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "pose estimation:Task"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "tracking:Task"
      ],
      [
        "MTN:Method",
        "Synonym-Of",
        "multi - task network:Method"
      ],
      [
        "MTN:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "occlusion - aware strategy:Method",
        "Used-For",
        "pose tracking:Task"
      ],
      [
        "SIFP:Method",
        "Synonym-Of",
        "Scale - normalized Image and Feature Pyramid:Method"
      ],
      [
        "FastPose:Method",
        "Used-For",
        "pose tracking:Task"
      ]
    ]
  },
  {
    "doc_id": "8238530",
    "chunk_id": 1,
    "content": [
      "However , in the field of semantic segmenta - tion , where we need to perform dense per - pixel prediction , we find that the large kernel ( and effective receptive field ) plays an important role when we have to perform the clas - sification and localization tasks simultaneously .",
      "Following our design principle , we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation .",
      "Our approach achieves state - of - art perfor - mance on two public benchmarks and significantly outper - forms previous results , 8 2 . 2 % ( vs 8 0 . 2 % ) on PASCAL VOC 2 0 1 2 dataset and 7 6 . 9 % ( vs 7 1 . 8 % ) on Cityscapes dataset .",
      "Semantic segmentation can be considered as a per - pixel classification problem .",
      "There are two challenges in this task : 1 ) classification : an object associated to a specific semantic concept should be marked correctly ; 2 ) localization : the classification label for a pixel must be aligned to the appropriate coordinates in output score map .",
      "The conventional semantic segmentation algorithms mainly target for the localization issue , as shown in Figure 1 B. But this might decrease the In this paper , we propose an improved net architecture , called Global Convolutional Network ( GCN ) , to deal with the above two challenges simultaneously .",
      "We follow two design principles : 1 ) from the localization view , the model structure should be fully convolutional to retain the localization performance and no fully - connected or global pooling layers should be used as these layers will discard the localization information ; 2 ) from the classification view , large kernel size should be adopted in the network architecture to enable densely connections between feature maps and per - pixel classifiers , which enhances the capability to handle different transformations .",
      "These two principles lead to our GCN , as in Figure 2 A. The FCN [ 2 5 ] -like structure is employed as our basic framework and our GCN is used to generate semantic score maps .",
      "To further improve the localization ability near the object boundaries , we introduce boundary refinement block to model the boundary alignment as a residual structure , shown in Figure 2 C. Unlike the CRF - like post - process [ 6 ] , our boundary refinement block is integrated into the network and trained end - to - end .",
      "Our contributions are summarized as follows : 1 ) we propose Global Convolutional Network for semantic segmentation which explicitly address the \" classification \" and \" localization \" problems simultaneously ; 2 ) a Boundary Refinement block is introduced which can further improve the localization performance near the object boundaries ; 3 ) we achieve state - of - art results on two standard benchmarks , with 8 2 . 2 % on PASCAL VOC 2 0 1 2 and 7 6 . 9 % on the Cityscapes .",
      "One of the most popular CNN based work is the Fully Convolutional Network ( FCN ) [ 2 5 ] .",
      "By converting the fully - connected layers into convolutional layers and concatenating the intermediate score maps , FCN has outperformed a lot of traditional methods on semantic segmentation .",
      "Following the structure of FCN , there are several works trying to improve the semantic segmentation task based on the following three aspects .",
      "Further , Dilated - Net [ 3 7 ] appends several layers after the score map to embed the multi - scale context , and Deeplab - V 2 [ 7 ] uses the Atrous Spatial Pyramid Pooling , which is a combination of convolutions , to embed the context directly from feature map .",
      "Initially , FCN [ 2 5 ] proposes the deconvolution ( i.e. inverse of convolution ) operation to increase the resolution of small score map .",
      "Further , Deconv - Net [ 2 7 ] and SegNet [ 3 ] introduce the unpooling operation ( i.e. inverse of pooling ) and a glass - like network to learn the upsampling process .",
      "Instead of learning the upsampling process , Deeplab [ 2 4 ] and Dilated - Net [ 3 7 ] propose a special dilated convolution to directly increase the spatial size of small feature maps , resulting in a larger score map .",
      "Among the many methods , Conditional Random Field ( CRF ) is often employed here because of its good mathematical formation .",
      "Deeplab [ 6 ] directly employs denseCRF [ 1 8 ] , which is a CRF - variant built on fully - connected graph , as a post - processing method after CNN .",
      "Then CRFAsRNN [ 3 8 ] models the denseCRF into a RNN - style operator and proposes an end - to - end pipeline , yet it involves too much CPU computation on Permutohedral Lattice [ 1 ] .",
      "DPN [ 2 4 ] makes a different approxima - tion on denseCRF and put the whole pipeline completely on GPU .",
      "Furthermore , Adelaide [ 2 1 ] deeply incorporates CRF and CNN where hand - crafted potentials is replaced by convolutions and nonlinearities .",
      "Besides , there are also some alternatives to CRF . [ 4 ] presents a similar model to CRF , called Bilateral Solver , yet achieves 1 0 x speed and comparable performance . [ 1 6 ] introduces the bilateral filter to learn the specific pairwise potentials within CNN .",
      "In contrary to previous works , we argues that semantic segmentation is a classification task on large feature map and our Global Convolutional Network could simultaneously fulfill the demands of classification and localization .",
      "In this section , we first propose a novel Global Convolutional Network ( GCN ) to address the contradictory aspects -classification and localization in semantic segmentation .",
      "Then using GCN we design a fully - convolutional framework for semantic segmentation task ."
    ],
    "relations": [
      [
        "clas - sification:Task",
        "SubTask-Of",
        "semantic segmenta - tion:Task"
      ],
      [
        "localization:Task",
        "SubTask-Of",
        "semantic segmenta - tion:Task"
      ],
      [
        "semantic segmenta - tion:Task",
        "SubTask-Of",
        "dense per - pixel prediction:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "localization:Task",
        "SubTask-Of",
        "semantic segmentation:Task"
      ],
      [
        "classification:Task",
        "SubTask-Of",
        "semantic segmentation:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Semantic segmentation:Task",
        "SubTask-Of",
        "per - pixel classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Global Convolutional Network:Method"
      ],
      [
        "boundary refinement block:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "CRF:Method",
        "Compare-With",
        "boundary refinement block:Method"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "Fully Convolutional Network:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "FCN:Method",
        "Synonym-Of",
        "Fully Convolutional Network:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "FCN:Method"
      ],
      [
        "fully - connected layers:Method",
        "Part-Of",
        "FCN:Method"
      ],
      [
        "FCN:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Atrous Spatial Pyramid Pooling:Method",
        "Part-Of",
        "Deeplab - V 2:Method"
      ],
      [
        "convolutions:Method",
        "Part-Of",
        "Atrous Spatial Pyramid Pooling:Method"
      ],
      [
        "deconvolution:Method",
        "Part-Of",
        "FCN:Method"
      ],
      [
        "unpooling operation:Method",
        "Part-Of",
        "Deconv - Net:Method"
      ],
      [
        "unpooling operation:Method",
        "Part-Of",
        "SegNet:Method"
      ],
      [
        "inverse of pooling:Method",
        "Synonym-Of",
        "unpooling operation:Method"
      ],
      [
        "dilated convolution:Method",
        "Part-Of",
        "Deeplab:Method"
      ],
      [
        "dilated convolution:Method",
        "Part-Of",
        "Dilated - Net:Method"
      ],
      [
        "CRF:Method",
        "Synonym-Of",
        "Conditional Random Field:Method"
      ],
      [
        "denseCRF:Method",
        "Part-Of",
        "Deeplab:Method"
      ],
      [
        "denseCRF:Method",
        "SubClass-Of",
        "CRF:Method"
      ],
      [
        "denseCRF:Method",
        "Part-Of",
        "CRFAsRNN:Method"
      ],
      [
        "denseCRF:Method",
        "SubClass-Of",
        "RNN - style operator:Method"
      ],
      [
        "CRF:Method",
        "Part-Of",
        "Adelaide:Method"
      ],
      [
        "CNN:Method",
        "Part-Of",
        "Adelaide:Method"
      ]
    ]
  },
  {
    "doc_id": "8238530",
    "chunk_id": 2,
    "content": [
      "The task of semantic segmentation , or pixel - wise classification , requires to output a score map assigning each pixel from the input image with semantic label .",
      "As mentioned in Introduction section , this task implies two challenges : classification and localization .",
      "However , we find that the requirements of classification and localization problems are naturally contradictory : ( 1 ) For classification task , models are required invariant to transformation on the inputs -objects may be shifted , rotated or rescaled but the classification results are expected to be unchanged . ( 2 ) While for localization task , models should be transformation - sensitive because the localization results depend on the positions of inputs .",
      "In deep learning , the differences between classification and localization lead to different styles of models .",
      "For classification , most modern frameworks such as AlexNet [ 2 0 ] , VGG Net [ 3 1 ] , GoogleNet [ 3 2 , 3 3 ] or ResNet [ 1 4 ] employ the \" Cone - shaped \" networks shown in Figure 1 A : features are extracted from a relatively small hidden layer , which is coarse on spatial dimensions , and classifiers are densely connected to entire feature map via fullyconnected layer [ 2 0 , 3 1 ] or global pooling layer [ 3 2 , 3 3 , 1 4 ] , which makes features robust to locally disturbances and allows classifiers to handle different types of input transformations .",
      "That is why most semantic segmentation frameworks , such as FCN [ 2 5 , 3 0 ] , U - Net [ 2 8 ] , DeepLab [ 6 , 7 ] , Deconv - Net [ 2 7 ] , adopt \" Barrel - shaped \" networks shown in Figure 1 B .",
      "We notice that current state - of - the - art semantic segmentation models [ 2 5 , 6 , 2 7 ] mainly follow the design principles for localization , however , which may be suboptimal for classification .",
      "At first , the valid receptive filed ( VRF ) 1 is large enough to hold the entire object .",
      "However , if the input object is resized to a large scale , then VRF can only cover a part of the object , which may be harmful for classification .",
      "It will be even worse if larger feature maps are used , because the gap between classification and localization becomes larger .",
      "First from the localization view , the structure must be fully - convolutional without any fully - connected layer or global pooling layer that used by many classification networks , since the latter will 1 Feature maps from modern networks such as GoolgeNet or ResNet usually have very large receptive field because of the deep architecture .",
      "However , studies [ 3 9 ] show that network tends to gather information mainly from a much smaller region in the receptive field , which is called valid receptive field ( VRF ) in this paper . discard localization information .",
      "Second from the classification view , motivated by the densely - connected structure of classification models , the kernel size of the convolutional structure should be as large as possible .",
      "Based on these two principles , we propose a novel Global Convolutional Network ( GCN ) in Figure 2 B. Instead of directly using larger kernel or global convolution , our GCN module employs a combination of 1 × k + k × 1 and k × 1 + 1 × k convolutions , which enables densely connections within a large k × k region in the feature map .",
      "We use pretrained ResNet [ 1 4 ] as the feature network and FCN 4 [ 2 5 , 3 6 ] as the segmentation framework .",
      "For traditional segmentation model , even though the receptive field is as large as the input image , however , the VRF just covers the bird ( A ) and fails to hold the entire object if the input resized to a larger scale ( B ) .",
      "As a comparison , our Global Convolution Network significantly enlarges the VRF ( C ) . class .",
      "The details can be referred to Figure 2   We evaluate our approach on the standard benchmark PASCAL VOC 2 0 1 2 [ 1 1 , 1 0 ] and Cityscapes [ 8 ] .",
      "We choose the state - of - the - art network ResNet 1 5 2 [ 1 4 ] ( pretrained on ImageNet [ 2 9 ] ) as our base model for fine tuning .",
      "During the training time , we use standard SGD [ 2 0 ] with batch size 1 , momentum 0. 9 9 and weight decay 0.0 0 0 5 .",
      "Then we will report the full results on PASCAL VOC 2 0 1 2 and Cityscapes .",
      "For all succeeding experiments , we pad each input image into 5 1 2 × 5 1 2 so that the top - most feature map is 1 6 × 1 6 .   In Section 3. 1 we propose Global Convolutional Network ( GCN ) to enable densely connections between classifiers and features .",
      "Further Discussion : In the experiments in Table 1 , since there are other differences between baseline and different versions of GCN , it seems not so confirmed to attribute the improvements to large kernels or GCN .",
      "The score is measured under standard mean IoU(% ) , and the 3rd and 4th rows show number of parameters of GCN and trivial Convolution after res - 5 . larger kernel will result in better performance if k ≤ 5 , yet for k ≥ 7 the performance drops .",
      "Thus the actual reason still needs further study . ( 2 ) GCN vs. Stack of small convolutions .",
      "Instead of GCN , another trivial approach to form a large kernel is to use stack of small kernel convolutions(for example , stack of 3 × 3 kernels in Figure 4 D ) , , which is very common in modern CNN architectures such as VGG - net [ 3 1 ] ."
    ],
    "relations": [
      [
        "convolutions:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "nonlinearities:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "Bilateral Solver:Method",
        "SubClass-Of",
        "CRF:Method"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "semantic segmentation:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Global Convolutional Network:Method"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "-classification:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "Global Convolutional Network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "-classification:Task",
        "SubTask-Of",
        "semantic segmentation:Task"
      ],
      [
        "localization:Task",
        "SubTask-Of",
        "semantic segmentation:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "semantic segmentation:Task",
        "SubTask-Of",
        "pixel - wise classification:Task"
      ],
      [
        "classification:Task",
        "Compare-With",
        "localization:Task"
      ],
      [
        "classification:Task",
        "Compare-With",
        "localization:Task"
      ],
      [
        "AlexNet:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "VGG Net:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "GoogleNet:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "ResNet:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "FCN:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "U - Net:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "DeepLab:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Deconv - Net:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "VRF:Method",
        "Synonym-Of",
        "valid receptive filed:Method"
      ],
      [
        "VRF:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "classification:Task",
        "Compare-With",
        "localization:Task"
      ],
      [
        "fully - convolutional:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "global pooling layer:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fully - connected layer:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "VRF:Method",
        "Synonym-Of",
        "valid receptive field:Method"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Global Convolutional Network:Method"
      ],
      [
        "k × 1 + 1 × k convolutions:Method",
        "Part-Of",
        "GCN:Method"
      ],
      [
        "ResNet:Method",
        "SubClass-Of",
        "feature network:Method"
      ]
    ]
  },
  {
    "doc_id": "8238530",
    "chunk_id": 3,
    "content": [
      "Comparison Experiments between Global Convolutional Network and the equivalent stack of small kernel convolutions .",
      "The score is measured under standard mean IoU. GCN outperforms the convolutional stack design with less parameters . ( 3 ) How GCN contributes to the segmentation results ?",
      "In Section 3. 1 , we claim that GCN improves the classification capability of segmentation model by introducing densely connections to the feature map , which is helpful to handle large variations of transformations .",
      "Based on this , we can infer that pixels lying in the center of large objects may benefit more from GCN because it is very close to \" pure \" classification problem .",
      "We evaluate our segmentation model ( GCN with k = 1 5 ) in both regions .",
      "In contrary to GCN structure , BF mainly improves the accuracy in boundary region , which also confirms its effectiveness .   In the above subsection our segmentation models are finetuned from ResNet - 1 5 2 network .",
      "Since large kernel plays a critical role in segmentation tasks , it is nature to apply the idea of GCN also on the pretrained model .",
      "Thus we propose a new ResNet - GCN structure , as shown in Figure 5 .",
      "We remove the first two layers in the original bottleneck structure used by ResNet , and replace them with a GCN module .",
      "In order to keep consistent with the original , we also apply Batch Normalization [ 1 5 ] and ReLU after each of the convolution layers .",
      "We compare our ResNet - GCN structure with the original ResNet model .",
      "For fair comparison , sizes for ResNet - GCN are carefully selected so that both network have similar computation cost and number of parameters .",
      "We first pretrain ResNet - GCN on ImageNet 2 0 1 5 [ 2 9 ] and fine tune on PASCAL VOC 2 0 1 2 segmentation dataset .",
      "Note that we take ResNet 5 0 model ( with or without GCN ) for comparison because the training of large ResNet 1 5 2 is very costly .",
      "From the results we can see that our GCNbased ResNet is slightly poorer than original ResNet as an ImageNet classification model .",
      "However , after finetuning on segmentation dataset ResNet - GCN model outperforms original ResNet significantly by 5. 5 % .",
      "With the application of GCN and boundary refinement , the gain of GCNbased pretrained model becomes minor but still prevails .",
      "We can safely conclude that GCN mainly helps to improve segmentation performance , no matter in pretrained model or segmentation - specific structures .",
      "ResNet 5 0 ResNet 5 0 - GCN ImageNet cls err ( % ) 7. 7 7. 9 Seg .",
      "Score ( GCN + BR ) 7 2 . 3 7 2 . 5   In this section we discuss our practice on PASCAL VOC 2 0 1 2 dataset .",
      "COCO has 8 0 classes and here we only retain the images including the same 2 0 classes in PASCAL VOC 2 0 1 2 .",
      "The training phase is split into three stages : ( 1 ) Our GCN + BR model clearly prevails , meanwhile the post - processing multi - scale and denseCRF [ 1 8 ] also bring benefits .",
      "Our work has outperformed all the previous state - of - the - arts . mean - IoU(% ) FCN - 8 s - heavy [ 3 0 ] 6 7 . 2 TTI zoomout v 2 [ 2 6 ] 6 9 . 6 MSRA BoxSup [ 9 ] 7 1 . 0 DeepLab - MSc - CRF - LargeFOV [ 6 ] 7 1 . 6 Oxford TVG CRF RNN COCO [ 3 8 ] 7 4 . 7 CUHK DPN COCO [ 2 4 ] 7 7 . 5 Oxford TVG HO CRF [ 2 ] 7 7 . 9 CASIA IVA OASeg [ 3 4 ] 7 8 . 3 Adelaide VeryDeep FCN VOC [ 3 5 ] 7 9 . 1 LRR 4x ResNet COCO [ 1 2 ] 7 9 . 3 Deeplabv 2 - CRF [ 7 ] 7 9 . 7 CentraleSupelec Deep G - CRF [ 5 ] 8 0 . 2 Our approach 8 2 . 2   Cityscapes [ 8 ] is a dataset collected for semantic segmentation on urban street scenes .",
      "According to our analysis on classification and segmentation , we find that large kernels is crucial to relieve the contradiction between classification and localization .",
      "Our best model achieves state - of - the - art on two public benchmarks : PASCAL VOC 2 0 1 2 ( 8 2 . 2 % ) and Cityscapes ( 7 6 . 9 % ) ."
    ],
    "relations": [
      [
        "FCN 4:Method",
        "SubClass-Of",
        "segmentation framework:Method"
      ],
      [
        "Global Convolution Network:Method",
        "Compare-With",
        "VRF ( C ):Method"
      ],
      [
        "ResNet 1 5 2:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "momentum:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "weight decay:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Global Convolutional Network:Method"
      ],
      [
        "Convolution:Method",
        "Part-Of",
        "GCN:Method"
      ],
      [
        "convolutions:Method",
        "Part-Of",
        "GCN:Method"
      ],
      [
        "convolutions(for:Method",
        "Part-Of",
        "GCN:Method"
      ],
      [
        "3 × 3 kernels:Method",
        "Part-Of",
        "convolutions(for:Method"
      ],
      [
        "VGG - net:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "convolutions:Method",
        "Part-Of",
        "Global Convolutional Network:Method"
      ],
      [
        "GCN:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "GCN:Method",
        "Compare-With",
        "BF:Method"
      ],
      [
        "ResNet - 1 5 2:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "ResNet - GCN:Method",
        "Compare-With",
        "ResNet:Method"
      ],
      [
        "ResNet - GCN:Method",
        "Trained-With",
        "ImageNet 2 0 1 5:Dataset"
      ],
      [
        "ResNet - GCN:Method",
        "Trained-With",
        "PASCAL VOC 2 0 1 2 segmentation:Dataset"
      ],
      [
        "GCN:Method",
        "Part-Of",
        "ResNet 5 0:Method"
      ],
      [
        "ResNet 5 0:Method",
        "Compare-With",
        "ResNet 1 5 2:Method"
      ],
      [
        "GCNbased ResNet:Method",
        "Compare-With",
        "ResNet:Method"
      ],
      [
        "GCNbased ResNet:Method",
        "Used-For",
        "ImageNet classification:Task"
      ],
      [
        "ResNet:Method",
        "Used-For",
        "ImageNet classification:Task"
      ],
      [
        "ResNet - GCN:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "ResNet:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "ResNet - GCN:Method",
        "Compare-With",
        "ResNet:Method"
      ],
      [
        "boundary refinement:Method",
        "Part-Of",
        "GCNbased:Method"
      ],
      [
        "GCN:Method",
        "Part-Of",
        "GCNbased:Method"
      ],
      [
        "GCN:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "ResNet 5 0 - GCN:Method",
        "Evaluated-With",
        "ImageNet:Dataset"
      ],
      [
        "Cityscapes:Dataset",
        "Benchmark-For",
        "semantic segmentation:Task"
      ]
    ]
  },
  {
    "doc_id": "201307511",
    "chunk_id": 1,
    "content": [
      "Specifically , we extract four types of visual co - occurrences between object and attribute words from large - scale , textually - annotated visual databases like VisualGenome and ImageNet .",
      "Through unsupervised clustering , supervised partitioning , and a zero - shot - like generalization analysis we show that our word embeddings complement text - only embeddings like GloVe by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone .",
      "These word embeddings , e.g. , GloVe and word 2 vec , are typically learned from large - scale text corpora by modeling textual co - occurrences .",
      "While no visual dataset exists with such exhaus - tive annotations ( many non - annotated words may still be applicable to an image region ) , large scale datasets like Vi - sualGenome [ 1 7 ] and ImageNet [ 8 ] along with their Word - Net [ 3 2 ] synset annotations provide a good starting point .",
      "We use ImageNet annotations augmented with WordNet hypernyms to compute Object - Hypernym co - occurrences while the remaining types of co - occurrence are computed from VisualGenome 's object and attribute annotations .",
      "To learn ViCo , i.e. , word embeddings from Visual Cooccurrences , we could concatenate GloVe - like embeddings trained separately for each co - occurrence type via a logbilinear model .",
      "To test ViCo 's ability to capture similarities and differences between visual concepts , we analyze performance in an unsupervised clustering , supervised partitioning ( see supplementary material ) , and a zero - shot - like visual generalization setting .",
      "In both cases , ViCo augmented GloVe outperforms GloVe , random vectors , vis - w 2 v , or their combinations .",
      "Through a qualitative analogy question answering evaluation , we also find ViCo embedding space to better capture relations between visual concepts than GloVe .",
      "The latter includes Caption - Image Retrieval , VQA , Referring Expression Comprehension , and Image Captioning .",
      "Systems using ViCo outperform those using GloVe for almost all tasks and metrics .",
      "To summarize our contributions : ( 1 ) We develop a multitask method to learn a word embedding from multiple types of co - occurrences ; ( 2 ) We show that the embeddings learned from multiple visual co - occurrences , when com - bined with GloVe , outperform GloVe alone in unsupervised clustering , supervised partitioning , and zero - shot - like analysis , as well as on multiple vision - language tasks ; ( 3 ) We find that performance of supervised vision - language models is relatively insensitive to word embeddings , with even random embeddings leading to nearly the same performance as learned embeddings .",
      "Semantic Differential ( SD ) [ 3 4 ] is among the earliest attempts to obtain vector representations of words .",
      "Another approach involved acquiring word similarity annotations followed by applying Multidimensional Scaling ( MDS ) [ 2 1 ] to obtain low dimensional ( typically 2 - 4 ) embeddings and then identifying meaningful clusters or interpretable dimensions [ 4 5 ] .",
      "Like SD , the MDS approach lacked representation power , and embeddings and their interpretations varied based on words ( e.g. , food names [ 4 5 ] , animals [ 4 4 ] , etc . ) to which MDS was applied .",
      "Recent neural approaches like the Continuous Bag - of - Words ( CBOW ) and the Skip - Gram models [ 2 9 , 3 1 , 3 0 ] learn from co - occurrences in local context windows as opposed to global co - occurrence statistics .",
      "We show loss computation of different approaches for learning word embeddings wi and wj for words i and j. The embeddings are denoted by colored vertical bars . ( i ) shows GloVe 's log - bilinear model . ( ii ) is our multi - task extension to learn from multiple co - occurrence matrices .",
      "There is some work on incorporating image representations into word embeddings . vis - w 2 v [ 1 8 ] uses abstract ( synthetic ) scenes to learn visual relatedness .",
      "ViEW [ 1 3 ] is another approach to visually enhance existing word embeddings .",
      "An autoencoder is trained on pre - trained word embeddings while matching intermediate representations to visual features extracted from a convolutional network trained on ImageNet ."
    ],
    "relations": [
      [
        "zero - shot - like generalization analysis:Method",
        "Used-For",
        "word embeddings:Method"
      ],
      [
        "supervised partitioning:Method",
        "Used-For",
        "word embeddings:Method"
      ],
      [
        "unsupervised clustering:Method",
        "Used-For",
        "word embeddings:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "text - only embeddings:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "WordNet:Dataset",
        "Used-For",
        "ImageNet:Dataset"
      ],
      [
        "word embeddings:Method",
        "Part-Of",
        "Visual Cooccurrences:Method"
      ],
      [
        "ViCo augmented GloVe:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "ViCo augmented GloVe:Method",
        "Compare-With",
        "random vectors:Method"
      ],
      [
        "ViCo augmented GloVe:Method",
        "Compare-With",
        "vis - w 2 v ,:Method"
      ],
      [
        "ViCo:Method",
        "Used-For",
        "question answering:Task"
      ],
      [
        "GloVe:Method",
        "Used-For",
        "question answering:Task"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "supervised partitioning:Method",
        "Used-For",
        "GloVe alone:Method"
      ],
      [
        "unsupervised clustering:Method",
        "Used-For",
        "GloVe alone:Method"
      ],
      [
        "zero - shot - like analysis:Method",
        "Used-For",
        "GloVe alone:Method"
      ],
      [
        "com - bined with GloVe:Method",
        "Compare-With",
        "GloVe alone:Method"
      ],
      [
        "GloVe alone:Method",
        "Used-For",
        "vision - language:Task"
      ],
      [
        "SD:Method",
        "Synonym-Of",
        "Semantic Differential:Method"
      ],
      [
        "MDS:Method",
        "Synonym-Of",
        "Multidimensional Scaling:Method"
      ],
      [
        "MDS:Method",
        "Compare-With",
        "SD:Method"
      ],
      [
        "CBOW:Method",
        "Synonym-Of",
        "Continuous Bag - of - Words:Method"
      ],
      [
        "ViEW:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ]
    ]
  },
  {
    "doc_id": "201307511",
    "chunk_id": 2,
    "content": [
      "The past year has seen several advances in contextualized word representations through pre - training on language models such as ELMo [ 3 9 ] , OpenAI GPT [ 4 2 ] , and BERT [ 9 ] .",
      "GloVe learns d - dimensional embeddings w i ∈ R d for all words i by optimizing where f : R → R is a weighting function that assigns lower weight to less frequent , noisy co - occurrences and b i is a learnable bias term for word i. Intuitively , the program in Eq. ( 1 ) learns word embeddings such that for any word pair with non - zero cooccurrence , the dot product w T i w j approximates the log co - occurrence count up to an additive constant .",
      "Note the slight difference between the objective in Eq. ( 1 ) and the original GloVe objective : GloVe replaces w j and b j withw j ( context vector ) andb j which are also trainable .",
      "We learn ViCo embeddings w i ∈ R d for all words i by minimizing the following loss function Here φ t : R d → R dt is a co - occurrence type - specific transformation function that maps ViCo embeddings to a type - specialized embedding space . b t i is a learned bias term for word i and type t. We set function f ( X ) in Eq. ( 1 ) to the constant 1 for all X. Next , we discuss the transformations φ t , benefits of capturing different types of co - occurrences , use of the second term in Eq. ( 2 ) , and training details .",
      "Pennington et al. [ 3 7 ] report Adagrad to work best for GloVe .",
      "We use Visual Genome and ImageNet for estimating visual co - occurrence counts .",
      "ImageNet synsets and their ancestors in WordNet are used to compute Object - Hypernym ( oh ) counts .",
      "For each region in Vi - sualGenome , we increment X oa ij by 1 , for each word pair ( i , j ) ∈ S o ×S a , and for all synset pairs ( S o , S a ) ∈ O × A. X oa ji is also incremented unless i = j. • For each region in VisualGenome , we increment X aa ij by 1 , for each word pair ( i , j ) ∈ S a 1 × S a 2 , and for all synset pairs ( S a 1 , S a 2 ) ∈ A × A. • Let C be the union of all object synsets annotated in an image .",
      "For each image in VisualGenome , X c ij is incremented by 1 , for each word pair ( i , j ) ∈ S c 1 × S c 2 , and for all synset pairs ( S c 1 , S c 2 ) ∈ C × C. • Let H be a set of object synsets annotated for an image in ImageNet and its ancestors in WordNet .",
      "For each each image in ImageNet , X oh ij is incremented by 1 , for each word pair ( i , j ) ∈ S h 1 × S h 2 , and for all synset pairs ( S h 1 , S h 2 ) ∈ H × H. We analyze ViCo embeddings with respect to the following properties : ( 1 ) 1 Data for clustering analysis .",
      "We hypothesize that ViCo represents similarities and differences between visual categories that are missing from GloVe .",
      "Qualitative evidence to support this hypothesis can be found in t - SNE plots shown in Fig. 4 , where concatenation of GloVe and ViCo embeddings leads to tighter , more homogenous clusters of the 1 3 coarse categories than GloVe .",
      "Plots ( c , d ) in Fig. 4 compare random vectors , GloVe , variants of ViCo and their combinations ( concatenation ) for different number of clusters using V - Measure .",
      "Tab . 3 shows that ViCo alone outperforms GloVe , random , and vis - w 2 v based embeddings .",
      "WordNet is not the sole contributor to strong performance of ViCo .",
      "To verify that ViCo 's gains are not simply due to the hierarchical nature of WordNet , we evaluate a version of ViCo trained on co - occurrences computed without using WordNet , i.e. , using raw word annotations in VisualGenome instead of synset annotations and without Object - Hypernym co - occurrences .",
      "Tab . 3 shows that GloVe+ViCo(linear, 1 0 0 ,w/o WordNet ) outperforms GloVe for both coarse and fine categories on both metrics .",
      "ViCo outperforms existing visual word embeddings .",
      "Tab . 3 evaluates performance of existing visual word embeddings which are learned from abstract scenes [ 1 8 ] . wiki and coco are different versions of vis - w 2 v depending on the dataset ( Wikipedia or MS - COCO [ 2 5 , 5 ] ) used for training word 2 vec for initialization .",
      "GloVe+vis - w 2 v - wiki performs similarly to GloVe and GloVe+vis - w 2 v - wiki - coco performs only slightly better than GloVe , showing that the majority of the information captured by vis - w 2 v may already be present in GloVe ."
    ],
    "relations": [
      [
        "word embeddings:Method",
        "Used-For",
        "autoencoder:Method"
      ],
      [
        "convolutional network:Method",
        "Used-For",
        "autoencoder:Method"
      ],
      [
        "convolutional network:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "ELMo:Method",
        "SubClass-Of",
        "pre - training on language models:Method"
      ],
      [
        "GPT:Method",
        "SubClass-Of",
        "pre - training on language models:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "pre - training on language models:Method"
      ],
      [
        "Adagrad:Method",
        "Part-Of",
        "GloVe:Method"
      ],
      [
        "random vectors:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "random vectors:Method",
        "Compare-With",
        "variants of ViCo:Method"
      ],
      [
        "GloVe:Method",
        "Compare-With",
        "variants of ViCo:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "random:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "vis - w 2 v:Method"
      ],
      [
        "WordNet:Dataset",
        "Used-For",
        "ViCo:Method"
      ],
      [
        "ViCo:Method",
        "Trained-With",
        "VisualGenome:Dataset"
      ],
      [
        "GloVe+ViCo(linear,:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "visual word embeddings:Method"
      ],
      [
        "vis - w 2 v:Method",
        "Trained-With",
        "Wikipedia:Dataset"
      ],
      [
        "vis - w 2 v:Method",
        "Trained-With",
        "MS - COCO:Dataset"
      ],
      [
        "GloVe+vis - w 2 v - wiki:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+vis - w 2 v - wiki - coco:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+random:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "CIFAR - 1 0 0:Dataset",
        "Benchmark-For",
        "object classification:Task"
      ],
      [
        "VisualGenome:Dataset",
        "Used-For",
        "learning word embeddings:Task"
      ]
    ]
  },
  {
    "doc_id": "201307511",
    "chunk_id": 3,
    "content": [
      "GloVe+random performs similarly to GloVe or worse .",
      "This implies that gains of GloVe+ViCo over GloVe are not just an artifact of increased dimensionality .",
      "To assess this ability , we evaluate embeddings on their zero - shot - like object classification performance using the CIFAR - 1 0 0 dataset .",
      "Note that our zero - shot - like setup is slightly different from a typical zero - shot setup because even though the visual classifier is not trained on unseen class images in CIFAR , annotations associated with images of unseen categories in VisualGenome or ImageNet may be used to compute word co - occurrences while learning word embeddings .",
      "Let f ( I ) ∈ R n be the features extracted from image I using a CNN and let w c ∈ R m denote the word embedding for class c ∈ C. Let g : R m → R n denote a function that projects word embeddings into the space of image features .",
      "In our experiments , f ( I ) is a 6 4 - dimensional feature vector produced by the last linear layer of a 3 4 - layer ResNet ( modified to accept 3 2 × 3 2 CIFAR images ) and g is a linear transformation .",
      "Fig. 5 compares chance performance ( 1/|U| ) , random vectors , GloVe , and GloVe+ViCo on four seen/unseen splits .",
      "The key conclusions are as follows : ViCo generalizes to unseen classes better than GloVe .",
      "ViCo based embeddings , especially 2 0 0 - dim . select and linear variants show healthy gains over GloVe .",
      "Note that this is not just due to higher dimensions of the embeddings since GloVe+random( 2 0 0 ) performs worse than GloVe .",
      "However , GloVe+ViCo(linear, 1 0 0 ) still outperforms GloVe in 3 out of 4 splits .",
      "On all tasks GloVe+ViCo outpeforms GloVe and GloVe+random .",
      "Comparing ViCo to GloVe and random vectors .",
      "GloVe+ViCo(linear ) outperforms GloVe and GloVe+random for all tasks and outperforms random for all tasks except Image Captioning .",
      "Let w 1 , w 2 , and a be the word embeddings ( GloVe or ViCo ) for the two concept words and the attribute word .",
      "We compute the scores s g and s v for GloVe and ViCo using function s(a , w 1 , w 2 ) = cosine(a , w 1 ) − cosine(a , w 2 ) , where cosine ( · ) is the cosine similarity .",
      "We then learn a linear SVM over s g for the GloVe only model and over s g and s v for the GloVe+ViCo model .",
      "Image features are then fused with a question representation using a GRU operating on word embeddings and fed into an answer classifier .",
      "We use the open source implementation of MAt - tNet [ 5 4 ] to compare localization accuracy with different embeddings on the RefCOCO+ dataset using the UNC split .",
      "Out of 3 0 analogy pairings tested , we found both GloVe and ViCo to be correct 1 9 times , only ViCo was correct 8 times , and only Glove was correct 3 times ."
    ],
    "relations": [
      [
        "ImageNet:Dataset",
        "Used-For",
        "learning word embeddings:Task"
      ],
      [
        "CIFAR:Dataset",
        "Used-For",
        "learning word embeddings:Task"
      ],
      [
        "linear layer:Method",
        "Part-Of",
        "ResNet:Method"
      ],
      [
        "random vectors:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "random vectors:Method",
        "Compare-With",
        "GloVe+ViCo:Method"
      ],
      [
        "GloVe:Method",
        "Compare-With",
        "GloVe+ViCo:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+random(:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+ViCo(linear,:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+ViCo:Method",
        "Compare-With",
        "GloVe+random:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "ViCo:Method",
        "Compare-With",
        "random vectors:Method"
      ],
      [
        "GloVe+ViCo(linear ):Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "GloVe+ViCo(linear ):Method",
        "Compare-With",
        "GloVe+random:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "ViCo:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "linear SVM:Method",
        "Part-Of",
        "GloVe only model:Method"
      ],
      [
        "linear SVM:Method",
        "Part-Of",
        "GloVe+ViCo:Method"
      ],
      [
        "MAt - tNet:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "RefCOCO+:Dataset",
        "Benchmark-For",
        "localization:Task"
      ],
      [
        "MAt - tNet:Method",
        "Evaluated-With",
        "RefCOCO+:Dataset"
      ]
    ]
  }
]