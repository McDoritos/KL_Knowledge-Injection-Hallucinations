[
  {
    "doc_id": "192546007",
    "chunk_id": 1,
    "content": [
      "Specifically , we investigate the attention and feature extraction mechanisms of state - of - the - art recurrent neural networks and self - attentive architectures for sentiment analysis , entailment and machine translation under adversarial attacks .",
      "Self - attentive neural models have recently become a prominent component that achieves state - of - theart performances on many natural language processing ( NLP ) tasks such as text classification and machine translation ( MT ) .",
      "This type of models , including Transformer ( Vaswani et al. , 2 0 1 7 ) and \" Bidirectional Encoder Representations from Transformers , \" shortened as BERT ( Devlin et al. , 2 0 1 9 ) , rely on the attention mechanism ( Luong et al. , 2 0 1 5 ) to learn a context - dependent representation ; compared to recurrent neural networks ( RNN ) , these self - attention - based models have faster encoding speed and the capacity of modeling a wider context .",
      "Particularly , BERT is recently proposed to extend the directionality of the Transformer model , and \" pre - trained \" using multiple objectives to strengthen its encoding capability .",
      "BERT achieves state - of - the - art performance on several NLP tasks including classification and sequence - to - sequence problems , often outperforming task - specific feature engineering or model architecture ; therefore , BERT is poised to be a key component in almost every neural model for NLP tasks .",
      "Despite the superior performance , it remains unclear whether the self - attentive structure deployed by Transformer or BERT is robust to adversarial attacks compared with other neural networks .",
      "We conduct experiments on two mainstream self - attentive models : ( a ) Transformer for neural machine translation , and ( b ) BERT for sentiment and entailment classification .",
      "To the best of our knowledge , this paper brings the following contributions . • We propose novel algorithms to generate more natural adversarial examples that both preserve the semantics and mislead the classifiers . • We conduct comprehensive experiments to examine the robustness of RNN , Transformer , and BERT .",
      "This section describes the target neural architectures , LSTM and self - attentive models , and how to adapt these models for the downstream tasks : sentiment analysis , entailment and translation .",
      "For classification tasks including sentiment analysis and entailment detection , we use a Bidirectional LSTM with an attention ( Hochreiter and Schmidhuber , 1 9 9 7 ; Bahdanau et al. , 2 0 1 4 ) layer as the sentence encoder , and a fully connected layer for classification problems .",
      "For machine translation , we employ a common seq 2 seq model ( Sutskever et al. , 2 0 1 4 ) , in which both the encoder and decoder are a 2 - layer stacked Bi - LSTM with 5 1 2 hidden units .",
      "Self - attentive models are further distinguished into BERT and Transformers .",
      "The classification problems adopt the BERT model with an identical setup to the original paper ( Devlin et al. , 2 0 1 9 ) , in which BERT is used as an encoder that represents a sentence as a vector .",
      "We also experiment with a smaller BERT model without pre - training , denoted as BERT NOPT , in order to isolate the impact of pre - training .",
      "To the best of our knowledge , there is no prior work that uses pre - trained BERT for machine translation .",
      "Thus , the Transformer model is employed for neural machine translation task .",
      "Although the GS - GR method potentially achieves a high success rate , the adversarial examples formed by GS - GR are usually unnatural ; sometimes GS - GR completely changes the semantics of the original sentence by replacing the most important word with its antonym , for example : changing \" this is a good restaurant \" into \" this is a bad restaurant . \" This can not be treated as a successful attack , since humans will notice the change and agree with the model 's output .",
      "In the experimental results , we show that the GS - EC method achieves a similar success rate as GS - GR in misleading the model , while being able to generate more natural and semanticallyconsistent adversarial sentences .",
      "These methods are denoted as AS MIN -GR that replaces the word with the lowest score , and AS MAX -GR with the highest score ."
    ],
    "relations": [
      [
        "attention:Method",
        "Part-Of",
        "recurrent neural networks:Method"
      ],
      [
        "feature extraction mechanisms:Method",
        "Part-Of",
        "recurrent neural networks:Method"
      ],
      [
        "attention:Method",
        "Part-Of",
        "self - attentive architectures:Method"
      ],
      [
        "feature extraction mechanisms:Method",
        "Part-Of",
        "self - attentive architectures:Method"
      ],
      [
        "recurrent neural networks:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "self - attentive architectures:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "recurrent neural networks:Method",
        "Used-For",
        "entailment:Task"
      ],
      [
        "self - attentive architectures:Method",
        "Used-For",
        "entailment:Task"
      ],
      [
        "recurrent neural networks:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "self - attentive architectures:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "NLP:Task",
        "Synonym-Of",
        "natural language processing:Task"
      ],
      [
        "text classification:Task",
        "SubTask-Of",
        "natural language processing:Task"
      ],
      [
        "machine translation:Task",
        "SubTask-Of",
        "natural language processing:Task"
      ],
      [
        "Self - attentive neural models:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "Self - attentive neural models:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "MT:Task",
        "Synonym-Of",
        "machine translation:Task"
      ],
      [
        "Self - attentive neural models:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "attention mechanism:Method",
        "Part-Of",
        "Transformer:Method"
      ],
      [
        "BERT:Method",
        "Synonym-Of",
        "Bidirectional Encoder Representations from Transformers:Method"
      ],
      [
        "attention mechanism:Method",
        "Part-Of",
        "Bidirectional Encoder Representations from Transformers:Method"
      ],
      [
        "RNN:Method",
        "Synonym-Of",
        "recurrent neural networks:Method"
      ],
      [
        "Bidirectional Encoder Representations from Transformers:Method",
        "Compare-With",
        "recurrent neural networks:Method"
      ],
      [
        "Transformer:Method",
        "Compare-With",
        "recurrent neural networks:Method"
      ],
      [
        "self - attention - based models:Method",
        "Compare-With",
        "recurrent neural networks:Method"
      ],
      [
        "Bidirectional Encoder Representations from Transformers:Method",
        "SubClass-Of",
        "self - attention - based models:Method"
      ],
      [
        "Transformer:Method",
        "SubClass-Of",
        "self - attention - based models:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "self - attention - based models:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "Transformer:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "sequence - to - sequence problems:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "classification:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "sequence - to - sequence problems:Task"
      ],
      [
        "BERT:Method",
        "Compare-With",
        "task - specific feature engineering:Method"
      ],
      [
        "BERT:Method",
        "Part-Of",
        "neural model:Method"
      ],
      [
        "neural model:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "Transformer:Method",
        "Part-Of",
        "self - attentive structure:Method"
      ],
      [
        "BERT:Method",
        "Part-Of",
        "self - attentive structure:Method"
      ],
      [
        "self - attentive structure:Method",
        "Compare-With",
        "neural networks:Method"
      ],
      [
        "Transformer:Method",
        "SubClass-Of",
        "mainstream self - attentive models:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "mainstream self - attentive models:Method"
      ],
      [
        "Transformer:Method",
        "Used-For",
        "neural machine translation:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "sentiment and entailment classification:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "entailment:Task"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "entailment:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "translation:Task"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "translation:Task"
      ],
      [
        "sentiment analysis:Task",
        "SubTask-Of",
        "classification:Task"
      ]
    ]
  },
  {
    "doc_id": "192546007",
    "chunk_id": 2,
    "content": [
      "We evaluate the robustness of the classification models ( for sentiment analysis and entailment ) by the following three criteria : ( a ) the success rate of the attacks misleading the model , ( b ) readability , and ( c ) human accuracy .",
      "For the experiments on machine translation task , we evaluate the attack success rate and BLEU scores ( Papineni et al. , 2 0 0 2 ) for 2 0 0 sentence pairs in the WMT 1 7 Task ( Bojar et al. , 2 0 1 7 ) .",
      "We first evaluate the robustness of LSTM , BERT , and BERT NOPT on binary sentiment analysis using the Yelp dataset ( Zhang et al. , 2 0 1 5 ) .",
      "Models under attack have accuracies of 9 3 . 7 % , 8 7 . 3 % and 9 0 . 7 % for fine - tuned BERT model , BERT NOPT and LSTM , respectively , on the test set .",
      "Note that for attention - based attacks ( i.e. , AS MIN -GR , AS MAX -GR , AS MIN -EC , and AS MAX -EC ) , the average of the first ( i.e. , the one that is closest to the model input ) attention layer from all 1 2 heads in BERT and BERT NOPT are used for our attacks . 1 To illustrate how adversarial attacks work , Fig. 1 shows the results from AS MAX -EC and AS MIN -EC methods that select a word to change based on the attention scores of the original sentence .",
      "The proposed GS - EC method can achieve almost identical success rates with GS - GR while restricting the search space based on the embedding distances .",
      "GS - EC leads to higher quality adversarial examples in Section 4. 2 . • We found that using attention , especially AS MAX methods , can easily break the LSTM model .",
      "However , the same vulnerability does not exist in BERT or BERT NOPT models .",
      "Since different types of attention - based attacks are suitable for different models , we summarize the best attention - based attack performance as A * in the table , which takes the maximum over four different types of attention - based attacks . • Self - attentive models ( BERT and BERT NOPT ) consistently lead to lower attack successful rates compared with the LSTM model , under RANDOM , LIST , attention - based attacks and greedy - based attacks .",
      "We demonstrate the robustness of BERT model under GS - EC attack in Fig 2 .",
      "We can see that , GS - EC caused a substantial shift in the LSTM 's attention map while that of BERT remain stable .",
      "Table 2 : Adversarial examples for the BERT sentiment analysis model generated by GS - GR and GS - EC methods .",
      "First , Table 2 compares the quality of the results generated by GS - GR and GS - EC attacks on a BERT model .",
      "Here we see that constraints imposed by GS - EC make it superior than GS - GR in terms of retrieving words that are coherent with the context .",
      "Table 3 is a comparison of LSTM and BERT models using the GS - EC attack .",
      "It shows that the distance in embeddings space of BERT can better reflect semantic similarity and contribute to more natural adversarial examples .",
      "And , in Table 4 , we compare using GS - GR and GS - EC method on BERT model .",
      "Again , we see that the GS - EC method , which restricts the distance between sentence embeddings of original and adversarial inputs , can produce superior adversaries .",
      "Model Readability Human Accuracy LSTM 0. 6 5 2 . 1 % BERT 1. 0 6 8 . 8 % Table 3 : Comparison of LSTM and BERT models under human evaluations against GS - EC attack ."
    ],
    "relations": [
      [
        "entailment detection:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "entailment detection:Task",
        "Used-For",
        "classification:Task"
      ],
      [
        "Bidirectional LSTM:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "Bidirectional LSTM:Method",
        "Used-For",
        "entailment detection:Task"
      ],
      [
        "attention:Method",
        "Part-Of",
        "Bidirectional LSTM:Method"
      ],
      [
        "fully connected layer:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "seq 2 seq:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "2 - layer stacked Bi - LSTM:Method",
        "Part-Of",
        "seq 2 seq:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "Transformer:Method",
        "Used-For",
        "neural machine translation:Task"
      ],
      [
        "GS - EC:Method",
        "Compare-With",
        "GS - GR:Method"
      ],
      [
        "AS MIN -GR:Method",
        "Compare-With",
        "AS MAX -GR:Method"
      ],
      [
        "sentiment analysis:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "entailment:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "WMT 1 7 Task:Dataset",
        "Benchmark-For",
        "machine translation:Task"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "binary sentiment analysis:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "binary sentiment analysis:Task"
      ],
      [
        "BERT NOPT:Method",
        "Used-For",
        "binary sentiment analysis:Task"
      ],
      [
        "Yelp:Dataset",
        "Benchmark-For",
        "binary sentiment analysis:Task"
      ],
      [
        "LSTM:Method",
        "Evaluated-With",
        "Yelp:Dataset"
      ],
      [
        "BERT:Method",
        "Evaluated-With",
        "Yelp:Dataset"
      ],
      [
        "BERT NOPT:Method",
        "Evaluated-With",
        "Yelp:Dataset"
      ],
      [
        "AS MIN -GR:Method",
        "SubClass-Of",
        "attention - based attacks:Method"
      ],
      [
        "AS MAX -GR:Method",
        "SubClass-Of",
        "attention - based attacks:Method"
      ],
      [
        "AS MIN -EC:Method",
        "SubClass-Of",
        "attention - based attacks:Method"
      ],
      [
        "AS MAX -EC:Method",
        "SubClass-Of",
        "attention - based attacks:Method"
      ],
      [
        "attention layer:Method",
        "Part-Of",
        "1 2 heads:Method"
      ],
      [
        "1 2 heads:Method",
        "Part-Of",
        "BERT:Method"
      ],
      [
        "1 2 heads:Method",
        "Part-Of",
        "BERT NOPT:Method"
      ],
      [
        "GS - EC:Method",
        "Compare-With",
        "GS - GR:Method"
      ],
      [
        "attention:Method",
        "Part-Of",
        "AS MAX:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "Self - attentive models:Method"
      ],
      [
        "BERT NOPT:Method",
        "SubClass-Of",
        "Self - attentive models:Method"
      ],
      [
        "Self - attentive models:Method",
        "Compare-With",
        "LSTM:Method"
      ],
      [
        "attention map:Method",
        "Part-Of",
        "LSTM:Method"
      ],
      [
        "GS - GR:Method",
        "Compare-With",
        "GS - EC attacks:Method"
      ],
      [
        "GS - EC attacks:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "GS - GR:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "GS - EC:Method",
        "Compare-With",
        "GS - GR:Method"
      ],
      [
        "GS - EC attack:Method",
        "Used-For",
        "LSTM:Method"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "GS - EC attack:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "semantic similarity:Task"
      ],
      [
        "GS - GR:Method",
        "Compare-With",
        "GS - EC:Method"
      ],
      [
        "GS - GR:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "GS - EC:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "GS - GR:Method",
        "Compare-With",
        "GS - EC attacks:Method"
      ],
      [
        "GS - EC attacks:Method",
        "Used-For",
        "BERT:Method"
      ]
    ]
  },
  {
    "doc_id": "192546007",
    "chunk_id": 3,
    "content": [
      "GS - GR 0. 5 5 6 4 . 6 % GS - EC 1. 0 6 8 . 8 % Table 4 : Comparison of GS - GR and GS - EC attacks on BERT model for sentiment analysis .",
      "MultiNLI is one of the many datasets that see major improvements by BERT .",
      "The BERT model is trained to achieve 8 3 . 5 % accuracy and LSTM 7 6 % .",
      "Our findings are summarized as follows : • The entailment task is more difficult than single - sentence classification , as evidenced by the higher success rates of attacks among all models and attacks . • The greedy - based attacks consistently achieve higher success rates .   Samples illustrated in Table 6 show that the GS - EC method can find more coherent words for the attack , as opposed to GS - GR .",
      "We implement LSTM and Transformer machine translation models using OpenNMT - py 2 .",
      "Specifically , for the LSTM model , we train it with 4 5 3 thousand pairs from the Europarl corpus of German - English WMT 1 5 Task 3 , common crawl , and news - commentary .",
      "Unlike the classification tasks , in machine translation the attack goal is harder to define .",
      "First , we notice that the success rate of the attacks are below 3 0 % , presumably because translation is substantially more complex compared with the aforementioned text classification tasks .",
      "Nevertheless , the attacks on the Transformer model is significantly less successful than the LSTM - based one .",
      "We observe that the Transformer - based model always achieves a higher BLEU score over LSTM - based model , i.e. , have a better translation performance whether the sentences contain typos or not .",
      "We conclude that Transformer - based model exhibits a greater robustness over LSTM - based model in the case of machine translation .",
      "In addition , we present some successful adversarial examples in Table 9 , and see that the greedy attack can indeed generate natural examples for both models .   2 7 . 5 % 1 0 . 5 % Table 8 : BLEU scores using typo - based attack on LSTM and Transformer translation models .",
      "This is somewhat counter - intuitive - at the first glance one may assume that the self - attention layer is not robust   Original input And in this vein , he passed the prize money of 2 5, 0 0 0 euros on straight away Adv input And as this vein , he passed the prize money of 2 5, 0 0 0 euros on straight away Original output Und in diesem Sinne hat er sofort das Preis geld von 2 5. 0 0 0 Euro über wiesen Adv output Und als diese Art , ging er sofort das Preis geld von 2 5. 0 0 0 Euro weiter Table 9 : Adversarial examples for LSTM and Transformer ( shortened as TF ) models with the target keyword \" Art . \" in the output . since perturbation in one word can affect all the attention scores .",
      "For the GS - EC attack in the sentiment analysis task , embeddings from the LSTM model has an average R e of 0. 8 3 whereas for the BERT model it is 0. 5 6 under the same attack by changing one word .",
      "This supports our claim that the impact of an adversarial example is more severe on the LSTM model than BERT , which presumably plays an important role in the robustness of self - attentive models .   Robustness of neural network models has been a prominent research topic since Szegedy et al. ( 2 0 1 3 ) discovered that CNN - based image classification models are vulnerable to adversarial examples .",
      "Previous work on attacking neural NLP models include using Fast Gradient Sign Method ( Goodfellow et al. , 2 0 1 5 ) to perturb the embedding of RNN - based classifiers ( Papernot et al. , 2 0 1 6 ; , but they have difficulties mapping from continuous embedding space to discrete input space .",
      "Zhao et al. ( 2 0 1 8) utilize generative adversarial networks ( GAN ) to generate adversarial attacks against black - box models for applications including image classification , textual entailment , and machine translation .",
      "In terms of comparisons between LSTM and Transformers , Tang et al. ( 2 0 1 8) show that multiheaded attention is a critical factor in Transformer when learning long distance linguistic relations .",
      "We show that self - attentive models are more robust to adversarial attacks than recurrent networks under small input perturbations on three NLP tasks , i.e. , sentiment analysis , entailment , and translation ."
    ],
    "relations": [
      [
        "GS - GR:Method",
        "Used-For",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "BERT:Method",
        "Evaluated-With",
        "MultiNLI:Dataset"
      ],
      [
        "BERT:Method",
        "Compare-With",
        "LSTM:Method"
      ],
      [
        "entailment:Task",
        "Compare-With",
        "single - sentence classification:Task"
      ],
      [
        "GS - EC:Method",
        "Compare-With",
        "GS - GR:Method"
      ],
      [
        "OpenNMT - py 2:Method",
        "Used-For",
        "LSTM:Method"
      ],
      [
        "OpenNMT - py 2:Method",
        "Used-For",
        "Transformer machine translation:Method"
      ],
      [
        "LSTM:Method",
        "Trained-With",
        "German - English WMT 1 5 Task 3:Dataset"
      ],
      [
        "LSTM:Method",
        "Trained-With",
        "common crawl:Dataset"
      ],
      [
        "LSTM:Method",
        "Trained-With",
        "news - commentary:Dataset"
      ],
      [
        "machine translation:Task",
        "Compare-With",
        "classification:Task"
      ],
      [
        "translation:Task",
        "Compare-With",
        "text classification:Task"
      ],
      [
        "Transformer:Method",
        "Compare-With",
        "LSTM - based one:Method"
      ],
      [
        "Transformer - based model:Method",
        "Compare-With",
        "LSTM - based model:Method"
      ],
      [
        "LSTM - based model:Method",
        "Used-For",
        "translation:Task"
      ],
      [
        "Transformer - based model:Method",
        "Used-For",
        "translation:Task"
      ],
      [
        "Transformer - based model:Method",
        "Compare-With",
        "LSTM - based model:Method"
      ],
      [
        "Transformer - based model:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "LSTM - based model:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "TF:Method",
        "Synonym-Of",
        "Transformer:Method"
      ],
      [
        "GS - EC:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "self - attentive models:Method"
      ],
      [
        "LSTM:Method",
        "SubClass-Of",
        "self - attentive models:Method"
      ],
      [
        "CNN:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "Fast Gradient Sign:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "GAN:Method",
        "Synonym-Of",
        "generative adversarial networks:Method"
      ],
      [
        "generative adversarial networks:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "generative adversarial networks:Method",
        "Used-For",
        "textual entailment:Task"
      ],
      [
        "generative adversarial networks:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "LSTM:Method",
        "Compare-With",
        "Transformers:Method"
      ],
      [
        "multiheaded attention:Method",
        "Part-Of",
        "Transformer:Method"
      ],
      [
        "self - attentive models:Method",
        "Compare-With",
        "recurrent networks:Method"
      ],
      [
        "sentiment analysis:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "entailment:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "translation:Task",
        "SubTask-Of",
        "NLP:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "recurrent networks:Method",
        "Used-For",
        "NLP:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "recurrent networks:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "entailment:Task"
      ],
      [
        "recurrent networks:Method",
        "Used-For",
        "entailment:Task"
      ],
      [
        "self - attentive models:Method",
        "Used-For",
        "translation:Task"
      ],
      [
        "recurrent networks:Method",
        "Used-For",
        "translation:Task"
      ]
    ]
  },
  {
    "doc_id": "52169846",
    "chunk_id": 1,
    "content": [
      "However , the effectiveness of such techniques has not been assessed for the hierarchical text classification ( HTC ) yet .",
      "We trained classification models with prominent machine learning algorithm implementations --- fastText , XGBoost , SVM , and Keras ' CNN --- and noticeable word embeddings generation methods --- GloVe , word 2 vec , and fastText --- with publicly available data and evaluated them with measures specifically appropriate for the hierarchical context .",
      "FastText achieved an $ { } _ {LCA}F_ 1 $ of 0. 8 9 3 on a single - labeled version of the RCV 1 dataset .",
      "Text classification (TC) - a.k.a . text categorization , topic classification - is the field that studies solutions for this problem , and uses a combination of knowledge areas such as Information Retrieval , Artificial Intelligence , Natural Language Processing ( NLP ) , Data Mining , Machine Learning , and Statistics .",
      "TC tasks usually have two or a just few classes , for example , automatic email categorization , spam detection , customer request routing , etc .",
      "This is where the hierarchical classification ( HC ) arises : it is a particular type of structured classification problem , where the output of the classification algorithm must correspond to one or more nodes of a taxonomic hierarchy [ 3 8 ] .",
      "When applied to textual data , HC then obviously becomes hierarchical text classification ( HTC ) .",
      "Some examples of large hierarchical text repositories are web directories ( e.g. Best of the Web 1 , DMOZ 2 , Wikipedia topic classifications 3 ) , library and patent classification schemes ( e.g. Library of Congress Classification 4 , United States Patent Classification 5 ) , or the classification schemes used in medical applications ( e.g. Medical Subject Headings ( MeSH ) 6 ) .",
      "The HTC problem poses some particular challenges , and while many classification algorithms are likely to work well in problems with only two or a small number of well - separated categories , accurate classification over large sets of closely related classes is inherently difficult [ 2 7 ] .",
      "Moreover , in the recent years , some breakthroughs have been achieved in the machine learning and NLP fields , which have been improving the effectiveness of many TC systems .",
      "Such progress include two main topics : ( 1 ) efficient text representation in vector space models such as word embeddings [ 2 8 , 3 2 ] and ( 2 ) efficient classification algorithms implementations , e.g. softmax - based linear classifiers [ 1 5 ] , scalable tree boosting systems [ 3 ] , and neural network variations [ 2 3 ] .",
      "However , to the best of our knowledge , and despite the close relationship between TC and HTC , the impact of those recent advancements have not been fully explored with regards to HTC yet .",
      "The present work investigated whether and how some techniques that have recently shown to improve the results of TC tasks can be extended to have a positive impact on the HTC problem through empirical experimentation and analysis .",
      "More specifically , we have attempted to at least partially address the following main questions : • How do recently developed text representation methods - GloVe , word 2 vec , and fastText - and efficient classification algorithms implementations - fastText , XGBoost , and Keras ' CNN - that have recently boosted the flat text classification results improve the effectiveness of HTC ? • What are the classification models effectiveness difference when comparing traditional classification measures - e.g. flat F 1 -against measures created specifically for hierarchical classification - e.g. hF 1 and lcaF 1 ?",
      "The following three sections provide descriptions of formal HTC definitions ( section 2 ) , text representation schemes ( section 3 ) , and classification algorithms ( section 4 ) that we will use for experimentation .",
      "Section 5 reviews relevant advancements within the HTC research , and the impact of recent techniques onto similar classification tasks .",
      "If the classification problem allows for classes that are not mutually exclusive , i.e. if a text piece can belong to one , more than one , or no class at all , it is called an any - of , multi - value , or multi - label classification ; on the other hand , if the classes are mutually exclusive , i.e. each document belongs to exactly one class , it as then called an one - of , single - label , multinomial , polytomous , or multi - class classification [ 2 7 ] .",
      "As hierarchies were becoming ever more popular for the organization of text documents , researchers from the Institute of Informatics and Telecommunications -NCSR Demokritos in Athens , Greece and from the Laboratoire d'Informatique de Grenoble , France organized the Large Scale HTC ( LSHTC ) Challenge .",
      "LSHTC became a series of competitions to assess the effectiveness of classification systems in large - scale classification in a large number of classes , which occurred in four occasions ( 2 0 0 9 , 2 0 1 1 , 2 0 1 2 , and 2 0 1 4 ) , and set some benchmarks for the task [ 3 1 ] .",
      "The ( 2 ) task objective determines whether the classifier must always choose a leaf node - mandatory leaf node prediction (MLNP) - or can choose any node in any level - non - mandatory leaf node prediction ( NMLNP ) [ 3 8 ] .",
      "Many approaches have been proposed to exploit the hierarchical structure of the target categories during the classification processes , and Silla Jr. & Freitas [ 3 8 ] summarized them into three main clusters , as follows : 3.a flat : ignores the hierarchy by \" flattening \" it to the leaf nodes level and works any usual multi - class classification algorithm during training and testing phases , 3.b global ( a.k.a . big - bang approach ): trains a single classifier while taking the hierarchy into account and may use a top - down strategy at the testing phase 3.c local approaches : sometimes incorrectly referred as \" top - down \" approach , uses the hierarchy structure to build classifiers using local information , i.e. only the data that belongs to a particular node is considered to learn one or many classification models per each node .",
      "Silla Jr. & Freitas [ 3 8 ] subdivide the local classification approach further into three subgroups depending on the way local information is used at the training phase : 3.c.i local classifier per node ( LCN ) trains a binary classifier for each child node 3.c.ii local classifier per parent node ( LCPN ) trains a multi - class classifier for each parent node 3.c.iii local classifier per level ( LCL ) trains a multi - class classifier for the entire hierarchy level During the test phase , all systems built using this local classification approach use a top - down strategy , i.e. they predict a class at an uppermost level and then use that information to predict further under the candidates nodes from the previous step only in recursive manner until a leaf node is reached or the blocking criteria for a NMLNP is met .",
      "Their lcaF 1 measure was studied empirically on datasets used by LSHTC and BioASQ 8 HTC competitions to conclude that \" flat \" measures are indeed not adequate to evaluate HC systems .",
      "In most approaches , TC takes advantage of the techniques developed by the Information Retrieval community to address the document indexing problem in order to build such representation models .",
      "Some of the most popular schemes in this approach are the latent semantic indexing ( LSI ) and latent Dirichlet allocation ( LDA ) .",
      "LSI consists of a low - rank approximation of the document - term matrix built from it using singular value decomposition ( SVD ) , and can arguably capture some aspects of basic linguistic notions such as synonymy and polysemy;LDA is a generative probabilistic model in which each document is modeled as a finite mixture of latent topics with Dirichlet distribution [ 2 7 ] .",
      "Such methods include the continuous bag of words ( CBoW ) model , the continuous skip - gram model (CSG) - a.k.a . word 2 vec models 9 [ 2 8 ] - and the global vectors model ( GloVe ) [ 3 2 ] .",
      "While word 2 vec is based on predictive models , GloVe is based on count - based models [ 2 ] .",
      "Word 2 vec models are trained using a shallow feedforward neural network that aims to predict a word based on the context regardless of its position ( CBoW ) or predict the words that surround a given single word ( CSG ) [ 2 8 ] .",
      "GloVe is a log - bilinear regression model that combines global co - occurrence matrix factorization ( somehow similar to LSI ) and local context window methods [ 3 2 ] .",
      "FastText can be used as a word embedding generator as well ; as such , it is similar to the CBoW model with a few particularities , which we described with more details in subsection 4. 1 .",
      "Moreover , some recently proposed classification algorithms incorporate the principles used to compute those word vectors into the classification task itself [ 2 2 , 1 5 ] .",
      "The following sub - subsections provide an overview about some of them , namely , linear classifier , gradient tree boosting , and convolutional neural networks ( CNN ) , for future reference in section 6 .",
      "Rocchio 1 0 , Naïve Bayes 1 1 , and SVM 1 2 are examples of linear classifiers .",
      "On the other hand , in unsupervised mode , fastText simply generates word embeddings for general purposes , then not taking classes into account .",
      "In data mining , decision trees can be used as classification and regression models , and induced from labeled training tuples by recursively partitioning the data into smaller , purer subsets given a certain splitting criteria until either all remaining tuples belong to the same class , there are no remaining splitting attributes , or there are no remaining tuples for a given branch [ 1 1 ] . 1 0 Rocchio classification model uses centroids to calculate decision boundaries and classify a tuple according to the region it belongs to [ 2 7 ] .",
      "1 1 Naïve Bayes is a statistical model based on Bayes ' theorem that makes predictions based on the probability that a tuple belongs to a class given its feature values . [ 1 1 ] 1 2 Support Vector Machine is a classification model that tries to find the hypothesis that minimizes the classification error based on the structural risk minimization principle by looking for the decision boundary that maximizes the distance between itself and the tuples that belong to either class . [ 1 3 ] There are many ways to improve the tree induction algorithm by , for example , using different splitting criteria ( gain ratio , information gain , Gini index , χ 2 , etc . ) , pruning too specific branches , or using tree ensembles .",
      "This allowed DL to produce extremely promising results for various tasks in natural language understanding , particularly topic classification [ 2 3 ] .",
      "Besides the general feed - forward neural network ( FNN ) , a few specialized architectures are already used heavily in industry , including CNN and recurrent neural networks ( RNN ) , which can scale to , for example , high resolution images and long temporal sequences [ 8 ] .",
      "CNN is a specialization of FNN that employs convolution - a specialized kind of linear operation - rather then a matrix multiplication with connection weights .",
      "Its architecture usually consists of layers with three stages , namely , convolution , detection , and pooling .",
      "Finally , the pooling stage replaces the detection output with a summary statistic of nearby outputs , which might be used to make a final prediction or to connect to the next convolution layer [ 8 ] .",
      "At the same time , recent investigation on problems that bear some similarity with the HTC , such as binary TC ( sentiment analysis , spam detection , etc ) have experienced some rapid development with the usage of the representation and classification methods presented in sections 3 and 4 .",
      "This section aims to present past and current research status regarding HTC and some techniques used in related areas that can have an impact on this field as well , considering the similarity that it holds with other TC problems .",
      "Subsection 5. 1 provides an overview about recent HTC research , sections 5. 2 and 5. 3 describe the advancements that other TC problems have seen in recent years , and finally subsection 5. 4 critically analyzes all those studies and their relation to HTC .",
      "Experimental results using the Reuters - 2 2 1 7 3 1 5 showed a significantly higher then previous models due to ( 1 ) mainly the selection of features and ( 2 ) marginally the hierarchical disposition of the individual classifiers , as long as they are complex ones - i.e. the benefit was inconclusive while using Naïve Bayes model , but substantial with a more elaborated algorithm from the Bayesian family .",
      "Soon after that , Dumais & Chen [ 5 ] used SVM to build an HTC model with two levels .",
      "The classification is based on threshold , and considers parent and child nodes either in a Boolean decision function ( LCN approach ) or multiplying them ( LCPN approach ) .",
      "The authors used SVM because it was considered an effective , efficient algorithm for TC , and experimented on a web content data set with 3 5 0 K records , which was a considerable amount for the time ."
    ],
    "relations": [
      [
        "HTC:Task",
        "Synonym-Of",
        "hierarchical text classification:Task"
      ],
      [
        "machine learning:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word embeddings generation methods:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "XGBoost:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "SVM:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "GloVe:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word 2 vec:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "SubClass-Of",
        "machine learning:Method"
      ],
      [
        "XGBoost:Method",
        "SubClass-Of",
        "machine learning:Method"
      ],
      [
        "SVM:Method",
        "SubClass-Of",
        "machine learning:Method"
      ],
      [
        "CNN:Method",
        "SubClass-Of",
        "machine learning:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "word embeddings generation methods:Method"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "word embeddings generation methods:Method"
      ],
      [
        "fastText:Method",
        "SubClass-Of",
        "word embeddings generation methods:Method"
      ],
      [
        "FastText:Method",
        "Evaluated-With",
        "RCV 1:Dataset"
      ],
      [
        "(TC):Task",
        "Synonym-Of",
        "Text classification:Task"
      ],
      [
        "text categorization:Task",
        "SubTask-Of",
        "Text classification:Task"
      ],
      [
        "topic classification:Task",
        "SubTask-Of",
        "Text classification:Task"
      ],
      [
        "NLP:Task",
        "Synonym-Of",
        "Natural Language Processing:Task"
      ],
      [
        "automatic email categorization:Task",
        "SubTask-Of",
        "TC:Task"
      ],
      [
        "spam detection:Task",
        "SubTask-Of",
        "TC:Task"
      ],
      [
        "customer request routing:Task",
        "SubTask-Of",
        "TC:Task"
      ],
      [
        "HC:Task",
        "Synonym-Of",
        "hierarchical classification:Task"
      ],
      [
        "hierarchical classification:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "HTC:Task",
        "Synonym-Of",
        "hierarchical text classification:Task"
      ],
      [
        "Library of Congress Classification:Dataset",
        "Benchmark-For",
        "library and patent classification:Task"
      ],
      [
        "United States Patent Classification:Dataset",
        "Benchmark-For",
        "library and patent classification:Task"
      ],
      [
        "Medical Subject Headings:Dataset",
        "Benchmark-For",
        "classification schemes used in medical applications:Task"
      ],
      [
        "MeSH:Dataset",
        "Synonym-Of",
        "Medical Subject Headings:Dataset"
      ],
      [
        "classification algorithms:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "NLP:Task",
        "Used-For",
        "TC:Task"
      ],
      [
        "machine learning:Task",
        "Used-For",
        "TC:Task"
      ],
      [
        "softmax - based linear classifiers:Method",
        "SubClass-Of",
        "classification algorithms:Method"
      ],
      [
        "scalable tree boosting systems:Method",
        "SubClass-Of",
        "classification algorithms:Method"
      ],
      [
        "neural network variations:Method",
        "SubClass-Of",
        "classification algorithms:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "text representation methods:Method"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "text representation methods:Method"
      ],
      [
        "fastText:Method",
        "SubClass-Of",
        "text representation methods:Method"
      ],
      [
        "fastText:Method",
        "SubClass-Of",
        "classification algorithms:Method"
      ],
      [
        "XGBoost:Method",
        "SubClass-Of",
        "classification algorithms:Method"
      ],
      [
        "CNN:Method",
        "Used-For",
        "flat text classification:Task"
      ],
      [
        "XGBoost:Method",
        "Used-For",
        "flat text classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "flat text classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "flat text classification:Task"
      ],
      [
        "word 2 vec:Method",
        "Used-For",
        "flat text classification:Task"
      ],
      [
        "GloVe:Method",
        "Used-For",
        "flat text classification:Task"
      ],
      [
        "LSHTC:Dataset",
        "Synonym-Of",
        "Large Scale HTC:Dataset"
      ],
      [
        "classification systems:Method",
        "Evaluated-With",
        "LSHTC:Dataset"
      ],
      [
        "classification systems:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "LSHTC:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "(MLNP):Method",
        "Synonym-Of",
        "mandatory leaf node prediction:Method"
      ],
      [
        "NMLNP:Method",
        "Synonym-Of",
        "non - mandatory leaf node prediction:Method"
      ],
      [
        "LCN:Method",
        "Synonym-Of",
        "local classifier per node:Method"
      ],
      [
        "LCPN:Method",
        "Synonym-Of",
        "local classifier per parent node:Method"
      ],
      [
        "LCL:Method",
        "Synonym-Of",
        "local classifier per level:Method"
      ],
      [
        "BioASQ:Dataset",
        "Benchmark-For",
        "HTC:Task"
      ],
      [
        "LSHTC:Dataset",
        "Benchmark-For",
        "HTC:Task"
      ],
      [
        "Information Retrieval:Task",
        "Used-For",
        "TC:Task"
      ],
      [
        "LSI:Method",
        "Synonym-Of",
        "latent semantic indexing:Method"
      ],
      [
        "LDA:Method",
        "Synonym-Of",
        "latent Dirichlet allocation:Method"
      ],
      [
        "SVD:Method",
        "Synonym-Of",
        "singular value decomposition:Method"
      ],
      [
        "polysemy;LDA:Method",
        "SubClass-Of",
        "generative probabilistic model:Method"
      ],
      [
        "CBoW:Method",
        "Synonym-Of",
        "continuous bag of words:Method"
      ],
      [
        "(CSG):Method",
        "Synonym-Of",
        "continuous skip - gram model:Method"
      ],
      [
        "continuous skip - gram model:Method",
        "SubClass-Of",
        "word 2 vec:Method"
      ],
      [
        "GloVe:Method",
        "Synonym-Of",
        "global vectors model:Method"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "predictive models:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "count - based models:Method"
      ],
      [
        "feedforward neural network:Method",
        "Used-For",
        "Word 2 vec:Method"
      ],
      [
        "local context window methods:Method",
        "Part-Of",
        "GloVe:Method"
      ],
      [
        "global co - occurrence matrix factorization:Method",
        "Part-Of",
        "GloVe:Method"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "log - bilinear regression model:Method"
      ],
      [
        "FastText:Method",
        "SubClass-Of",
        "word embedding generator:Method"
      ],
      [
        "FastText:Method",
        "Compare-With",
        "CBoW:Method"
      ],
      [
        "classification algorithms:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNN:Method",
        "Synonym-Of",
        "convolutional neural networks:Method"
      ],
      [
        "Rocchio:Method",
        "SubClass-Of",
        "linear classifiers:Method"
      ],
      [
        "Naïve Bayes:Method",
        "SubClass-Of",
        "linear classifiers:Method"
      ],
      [
        "SVM:Method",
        "SubClass-Of",
        "linear classifiers:Method"
      ],
      [
        "fastText:Method",
        "Used-For",
        "generates word embeddings:Task"
      ],
      [
        "decision trees:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "decision trees:Method",
        "Used-For",
        "regression:Task"
      ],
      [
        "Bayes ' theorem:Method",
        "Part-Of",
        "Naïve Bayes:Method"
      ],
      [
        "Naïve Bayes:Method",
        "SubClass-Of",
        "statistical model:Method"
      ],
      [
        "Support Vector Machine:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "DL:Method",
        "Used-For",
        "natural language understanding:Task"
      ],
      [
        "topic classification:Task",
        "SubTask-Of",
        "natural language understanding:Task"
      ],
      [
        "DL:Method",
        "Used-For",
        "topic classification:Task"
      ]
    ]
  },
  {
    "doc_id": "52169846",
    "chunk_id": 2,
    "content": [
      "The researchers experimented the method on Reuters - 2 1 5 7 8 ( 9 0 classes , ∼ 1 1 K records ) , the RCV 1 1 7 ( 1 0 3 classes , ∼ 8 0 0 K records ) , and the ICCCFT 1 8 ( 7 9 classes , ∼ 1 K records ) , and considered the same F 1 function variation as Ruiz & Srinivasan [ 3 4 ] did for an effectiveness measure .",
      "The editions of the LSHTC Challenge brought many diverse approaches into the HTC area .",
      "The report summarizes the results by saying that 1 4 Zipf 's Law is an empirical rulethat states that the collection frequency cf i of the ith most common term is proportional to 1/i [ 2 7 , flat classification approaches were competitive with the hierarchical ones , and highlights only a few that seem noteworthy , such as some models built upon k -Nearest Neighbor ( k NN ) 2 0 and Rocchio improvements .",
      "The 4th edition winning submission , in particular , consisted of an ensemble of sparse generative models extending Multinomial Naïve Bayes that combined document , label , and hierarchy level multinomials with feature pre - processing using variants of TF - IDF and BM 2 5 [ 3 3 ] .",
      "Balikas & Amini [ 1 ] elaborated an empirical study that employed word embeddings as features for large scale TC .",
      "The word embeddings were generated using the skip - gram model of word 2 vec based on 1 0 million PubMed 2 1 abstracts plus 2. 5 M Wikipedia documents in four sizes : 5 0 , 1 0 0 , 2 0 0 , and 4 0 0 elements .",
      "Besides the way the word embeddings are combined , the vector size has proportional effect on the classification effectiveness as well .",
      "The results , however , do not reach the baseline model , which is TF - IDF based SVM model .",
      "Nevertheless , when combining TF - IDF to the concatenated document distributed representations , the results are better than the TF - IDF alone by a small , but statistically significant margin .",
      "While no breakthrough has occurred with the HTC task over the last years , other TC problems on the other hand have benefited from the recent great improvements on text representation using distributed vector space models .",
      "Over the recent years , many researchers used methods based on word embeddings to improve the accuracy of classification tasks such as sentiment analysis and topic classification .",
      "This section provides some examples from these other TC tasks that are somehow similar to HTC and took advantage from those advancements .",
      "They collected a dataset 2 2 with 1 0 0 , 0 0 0 movie reviews from the Internet Movie Database (IMDb) - 2 5 , 0 0 0 labeled reviews for the classification task , 2 5 , 0 0 0 for the classification test , and 5 0 , 0 0 0 of unlabeled ones as additional data to build the semantic component of their model .",
      "Such paragraph vectors can be used as features for conventional machine learning techniques , so the authors took the data collected by Maas et al. [ 2 6 ] to calculate paragraph vectors , used them as inputs to a neural network to predict the sentiment , and compared the results against other approaches that used the same 2 0 Nearest Neighbors classifiers are labor intensive classification methods that are based on comparing a given test tuple with training tuples that are similar to it [ 1 1 , p. 4 2 3 ] 2 1 https://www.ncbi.nlm.nih.gov/pubmed/ 2 2 The so - called Large Movie Review Dataset v 1 . 0 has been widely used as a benchmark dataset for binary sentiment TC and is publicly available at http://ai.stanford.edu/~amaas/data/sentiment/ 2 3 A dataset with 2, 0 0 0 balanced , processed reviews from the IMDb archive publicly available at http://www.cs.cornell.edu/people/pabo/movie - review - data/ dataset .",
      "Still on the sentiment analysis topic , however on a slightly different scenario , Tang et al. [ 4 1 ] proposed the learning of Sentiment Specific Word Embedding ( SSWE ) by integrating the sentiment information into the loss function of the model and its application in a supervised learning framework for Twitter sentiment classification task .",
      "The authors used a partial version of a benchmark dataset used on SemEval 2 4 2 0 1 3 with 6, 2 5 1 positive/negative unbalanced records , and found that the SVM classification model built upon their SSWE has an effectiveness ( macro - F 1 ) comparable with models created from state - of - the - art , manually designed features .",
      "Furthermore , they compared their SSWE with three other word embeddings - C&W 2 5 [ 4 ] , word 2 vec [ 2 8 ] , and WVSA ( Word Vectors for Sentiment Analysis ) [ 2 6 ] -to conclude that the effectiveness of word embeddings that do not directly take advantage of the sentiment information in the text - C&W and word 2 vec - are considerably lower than the others .",
      "Their study is just the beginning of a clear strategy trend in this topic : 7 out of the 1 0 top - ranked solutions for the SemEval - 2 0 1 6 Sentiment Analysis in Twitter Task incorporated either general - purpose or task - specific word embeddings in their participating systems [ 3 0 ] .",
      "As an exponent of this trend , Vosoughi et al. [ 4 5 ] created a method to compute distributed representations for short texts using a long short - term memory ( LSTM ) 2 6 NN at a characterlevel .",
      "To evaluate the quality of the resulting vectors , the authors used them to perform a polarity classification on the dataset provided on SemEval - 2 0 1 5 Task 1 0 subtask B competition 2 7 .",
      "On the other hand , there are also examples of the usage of word embeddings on more general , multi category TC tasks .",
      "Huang et al. [ 1 2 ] propose a method to learn so called document embeddings directly in TC task , that aims to represent a document as a combination of the word embeddings of its words , which is learned using a neural network architecture .",
      "The authors use resulting network in two ways during the classification phase : the network itself as a classification model or the weights from one of its last hidden layers as the input for an SVM classifier .",
      "Ma et al. [ 2 5 ] use a Gaussian process approach to model the distribution of word embeddings according to their respective themes .",
      "Their results show that the proposed method has a 3. 3 % accuracy gain over two other approaches that used ( 1 ) classical TF - IDF and ( 2 ) topic models estimated using latent Dirichlet allocation ( LDA ) as representation methods connected to MaxEnt classifiers , and suggest the accuracy increase occurs because \" It is clear that using word embeddings 2 4 SemEval ( Semantic Evaluation ) is an ongoing series of evaluations of computational semantic analysis systems organized by the Association for Computational Linguistics ( ACL ) https://aclweb.org/aclwiki/SemEval_Portal 2 5",
      "C&W is a short that Tang et al. [ 4 1 ] used for the method reportedly introduced by Collobert et al. [ 4 ] , which was not formally named by the authors . 2 6 The long short - term memory ( LSTM ) uses \" a memory cell which can maintain its state over time and non - linear gating units which regulate the information flow into and out of the cell \" [ 1 0 ] , and is usually associated with the deep learning algorithms family [ 2 3 ] . 2 7 http://alt.qcri.org/semeval 2 0 1 5 /task 1 0 / which were trained from universal dataset mitigated the problem of unseen words . \" On another approach , Kusner et al. [ 2 0 ] take advantage of the word embeddings to create a distance metric between text documents .",
      "Their proposed metric aims to incorporate the semantic similarity between word pairs - the lowest \" traveling cost \" ( Euclidean distance ) from a word to another within the word 2 vec embedding space - into a document distance function .",
      "The minimum cumulative cost to move from a document to another - the so - called Word Mover 's Distance (WMD) - is then used to perform k NN document classification on eight real world document classification data sets .",
      "The resulting k NN classification model using WMD yields unprecedented low classification error rates when compared to other well established methods such as latent semantic indexing ( LSI ) and LDA .",
      "Joulin et al. [ 1 5 ] built a classification model called fastText , already presented in section 4 .",
      "For sentiment analysis comparison , they used the same eight datasets and evaluation protocol as Zhang et al. [ 5 0 ] , and found that fastText ( using 1 0 hidden units , trained 5 epochs , with bigram information ) accuracy is competitive with complex models , but needed only a fraction of time to process - the faster competitor took 2 4 minutes to train ( some took up to 7 hours ) , while the worst case for fastText took only 1 0 seconds .",
      "Nevertheless , at the same time that Sebastiani [ 3 7 ] reports that some NN - based models using logistic regression provide some good results , he observes that they have a relative effectiveness slightly worse than many other models known at the time , e.g. SVM .",
      "In the authors ' point of view , part - of - speech tagging ( POS ) , chunking , named entity recognition ( NER ) , and semantic role labeling ( SRL ) problems can be roughly seen as assigning labels to words , so they build an architecture capable of capturing feature vectors from words and higher level features through CNN to do that from raw text .",
      "Still on sentiment classification studies , Kim [ 1 6 ] reports on experiments with CNN trained upon distributed text representations .",
      "The experiment results show that the authors ' simple CNN with one convolution layer only performs remarkably well despite little tuning of hyperparameters , surpassing the state - of - the - art methods in 4 out of the 7 analyzed tasks/datasets .",
      "Johnson & Zhang [ 1 4 ] also use CNN architecture , but instead of word embeddings , their model works on high - dimensional one - hot encoding vectors , i.e. each document is seen as sequence of dictionary - sized , ordered vectors with a single true bit each that corresponds to a given word .",
      "For evaluation purposes , the authors executed experiments on sentiment classification - IMDb dataset [ 2 6 ] -and topic categorization - RCV 1 dataset [ 2 4 ] , disregarding the hierarchical structure - , and compared the results against SVM - based classifiers .",
      "Since such kind of model requires very large datasets , in order to perform meaningful experiments , the authors had to build 8 of them , which had from 2 to 1 4 classes , number of records ranging from 1 2 0 , 0 0 0 to 3. 6 million , and related to two main task , namely sentiment analysis and topic classification .",
      "To compare the models effectiveness , besides training their own new model , the authors also did so with models using ( 1 ) a multinomial logistic regression method built upon traditional text representation techniques and ( 2 ) a recurrent neural network using pre - trained word 2 vec word embeddings as input .",
      "At the model first level , a bi - directional recurrent structure captures the contextual information ; at its second level , a max - pooling layer finds the best features to execute the classification .",
      "The model input consists of word embeddings that were pre - trained using the word 2 vec skip - gram method on Wikipedia dumps .",
      "This is in part because of the variations within the HTC task itself ( single - or multi - label ) , but also in part because it seems it took a long time for the community to evolve to a point when a thorough study about hierarchical classification evaluation could have been done .",
      "In other words , comparing the effectiveness of hierarchical classification against flat classification is not only inadequate , but also inaccurate , as the problem is different in its own nature [ 1 9 ] .",
      "Many different methods have been applied to HTC , and the most representative ones have been referred , namely Bayesian models [ 1 8 , 3 3 ] , SVM [ 5 , 9 ] , NN [ 3 4 ] , boosting methods [ 6 ] , Rocchio , and k NN [ 3 1 ] .",
      "Such comparison , on the other hand , makes the fact that HTC researchers seem to have been paying little or no attention to the recent classification effectiveness improvements achieved using advances in other TC tasks also surprising .",
      "For example , considering the reports on TC competitions , while Partalas et al. [ 3 1 ] do not even refer to the usage of word embeddings in LSHTC Challenges , Nakov et al. [ 3 0 ] mention that most of the best systems performing sentiment analysis on Twitter used them in some way .",
      "It seems clear that word embeddings and other vector space models improve some TC schemes considerably .",
      "Nevertheless , the ideas behind word embeddings are undoubtedly advantageous for TC in many different ways , from calculating a distance metric for k -NN classifier [ 2 0 ] to transforming a word embedding learner into a classifier itself [ 1 5 ] .",
      "All in all , despite its promising results , the effect of using word embeddings in HTC remains a great unknown , as no empirical evidence has been reported on it ."
    ],
    "relations": [
      [
        "FNN:Method",
        "Synonym-Of",
        "feed - forward neural network:Method"
      ],
      [
        "RNN:Method",
        "Synonym-Of",
        "recurrent neural networks:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "CNN:Method",
        "SubClass-Of",
        "FNN:Method"
      ],
      [
        "convolution:Method",
        "SubClass-Of",
        "linear operation:Method"
      ],
      [
        "sentiment analysis:Task",
        "SubTask-Of",
        "binary TC:Task"
      ],
      [
        "spam detection:Task",
        "SubTask-Of",
        "binary TC:Task"
      ],
      [
        "Naïve Bayes:Method",
        "Evaluated-With",
        "Reuters - 2 2 1 7 3:Dataset"
      ],
      [
        "SVM:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "Boolean decision function:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "LCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "LCPN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "LCN:Method",
        "SubClass-Of",
        "Boolean decision function:Method"
      ],
      [
        "SVM:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "LSHTC:Dataset",
        "Benchmark-For",
        "HTC:Task"
      ],
      [
        "k NN:Method",
        "Synonym-Of",
        "k -Nearest Neighbor:Method"
      ],
      [
        "flat classification approaches:Method",
        "Compare-With",
        "k -Nearest Neighbor:Method"
      ],
      [
        "flat classification approaches:Method",
        "Compare-With",
        "Rocchio:Method"
      ],
      [
        "TF - IDF:Method",
        "Used-For",
        "sparse generative models:Method"
      ],
      [
        "BM 2 5:Method",
        "Used-For",
        "sparse generative models:Method"
      ],
      [
        "sparse generative models:Method",
        "SubClass-Of",
        "Multinomial Naïve Bayes:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "skip - gram model:Method",
        "Used-For",
        "word embeddings:Method"
      ],
      [
        "skip - gram model:Method",
        "Part-Of",
        "word 2 vec:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "TF - IDF:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "distributed vector space models:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "distributed vector space models:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "sentiment analysis:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "topic classification:Task",
        "SubTask-Of",
        "classification:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "topic classification:Task"
      ],
      [
        "TC:Task",
        "Compare-With",
        "HTC:Task"
      ],
      [
        "(IMDb):Dataset",
        "Synonym-Of",
        "Internet Movie Database:Dataset"
      ],
      [
        "Internet Movie Database:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "Internet Movie Database:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "Nearest Neighbors classifiers:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Large Movie Review Dataset v 1 . 0:Dataset",
        "Benchmark-For",
        "binary sentiment TC:Method"
      ],
      [
        "Sentiment Specific Word Embedding:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "SSWE:Method",
        "Synonym-Of",
        "Sentiment Specific Word Embedding:Method"
      ],
      [
        "Sentiment Specific Word Embedding:Method",
        "Used-For",
        "Twitter sentiment classification:Task"
      ],
      [
        "SVM classification:Method",
        "Evaluated-With",
        "SemEval 2 4 2 0 1 3:Dataset"
      ],
      [
        "SSWE:Method",
        "Part-Of",
        "SVM classification:Method"
      ],
      [
        "C&W:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "WVSA:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "SSWE:Method",
        "Compare-With",
        "word embeddings:Method"
      ],
      [
        "SSWE:Method",
        "Compare-With",
        "C&W:Method"
      ],
      [
        "SSWE:Method",
        "Compare-With",
        "word 2 vec:Method"
      ],
      [
        "Word Vectors for Sentiment Analysis:Method",
        "Synonym-Of",
        "WVSA:Method"
      ],
      [
        "SSWE:Method",
        "Compare-With",
        "WVSA:Method"
      ],
      [
        "long short - term memory:Method",
        "Used-For",
        "compute distributed representations for short texts:Task"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long short - term memory:Method"
      ],
      [
        "SemEval - 2 0 1 5 Task 1 0 subtask B competition 2 7:Dataset",
        "Benchmark-For",
        "polarity classification:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "multi category TC:Task"
      ],
      [
        "document embeddings:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "neural network architecture:Method",
        "Used-For",
        "word embeddings:Method"
      ],
      [
        "SVM:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Gaussian process:Method",
        "Used-For",
        "word embeddings:Method"
      ],
      [
        "latent Dirichlet allocation:Method",
        "Used-For",
        "topic models:Task"
      ],
      [
        "LDA:Method",
        "Synonym-Of",
        "latent Dirichlet allocation:Method"
      ],
      [
        "SemEval:Dataset",
        "Synonym-Of",
        "Semantic Evaluation:Dataset"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long short - term memory:Method"
      ],
      [
        "Euclidean distance:Method",
        "Used-For",
        "semantic similarity:Task"
      ],
      [
        "(WMD):Method",
        "Synonym-Of",
        "Word Mover 's Distance:Method"
      ],
      [
        "Word Mover 's Distance:Method",
        "Used-For",
        "k NN document classification:Task"
      ],
      [
        "WMD:Method",
        "Used-For",
        "k NN classification:Task"
      ],
      [
        "WMD:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "LSI:Method",
        "Synonym-Of",
        "latent semantic indexing:Method"
      ],
      [
        "WMD:Method",
        "Compare-With",
        "latent semantic indexing:Method"
      ],
      [
        "WMD:Method",
        "Compare-With",
        "LDA:Method"
      ],
      [
        "fastText:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "logistic regression:Method",
        "Part-Of",
        "NN - based models:Method"
      ],
      [
        "logistic regression:Method",
        "Compare-With",
        "SVM:Method"
      ],
      [
        "POS:Task",
        "Synonym-Of",
        "part - of - speech tagging:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "part - of - speech tagging:Task"
      ],
      [
        "NER:Task",
        "Synonym-Of",
        "named entity recognition:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "named entity recognition:Task"
      ],
      [
        "SRL:Task",
        "Synonym-Of",
        "semantic role labeling:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "semantic role labeling:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "sentiment classification:Task"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "IMDb:Dataset",
        "Benchmark-For",
        "sentiment classification:Task"
      ],
      [
        "RCV 1:Dataset",
        "Benchmark-For",
        "topic categorization:Task"
      ],
      [
        "max - pooling:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word 2 vec skip - gram method:Method",
        "Part-Of",
        "word embeddings:Method"
      ],
      [
        "word embeddings:Method",
        "Trained-With",
        "Wikipedia dumps:Dataset"
      ],
      [
        "hierarchical classification:Task",
        "Compare-With",
        "flat classification:Task"
      ],
      [
        "SVM:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "Bayesian models:Method",
        "Used-For",
        "HTC:Task"
      ]
    ]
  },
  {
    "doc_id": "52169846",
    "chunk_id": 3,
    "content": [
      "Although Collobert et al. [ 4 ] show that the use of CNN provides competitive results in more than one NLP classification task , the concepts that have been preached by them and others influenced many following researchers who later on started to reconsider NN models and find promising results [ 4 1 , 1 6 ] .",
      "Their most important contribution to the present investigation is the indication that adequate word embeddings combined with appropriate classification NN 's provide promising results .",
      "Nevertheless , this assumption lacks of empirical evidence when it comes to the hierarchical text context , as no report has been found specifically about it and it is doubtful that simple TC problems are adequate to evaluate deep neural networks representations , which in theory have power expected to provide much better final classification results [ 1 5 ] .",
      "We have designed and implemented experiments to analyze the effectiveness of combining word embeddings as the text representation layer with modern classification algorithms applied to the HTC problem .",
      "Since no dataset is widely used in the HTC research , choosing an appropriate dataset to perform HTC experiments becomes a somewhat hard task .",
      "Therefore , we have mainly considered corpora provided by Reuters ( Reuters - 2 2 1 7 3 and RCV 1 ) and PubMed ( from BioASQ ) .",
      "We consider this as a downside for two reasons : ( 1 ) the results obtained within such a specific corpus might not generalize to other HTC tasks and ( 2 ) GloVe and word 2 vec pretrained word vectors are general , which makes them inadequate for such a specific classification task 3 0 .",
      "This approach is based on the assumption that 2 9 Those two papers combined had more than 3, 5 0 0 citations as counted by Google Scholar by Jan 2 0 1 7 3 0 BioASQ has recently provided word embeddings pre - trained using word 2 vec , which could be potentially useful for this analysis .",
      "Nevertheless , as we intend to compare GloVe and word 2 vec results , having word embeddings trained with a single method only is not enough for our purposes . 3 1 http://trec.nist.gov/data/reuters/reuters.html < ? xml version=\" 1 . 0 \" encoding=\"iso - 8 8 5 9 - 1 \" ? > ... < headline > Tylan stock jumps ; weighs sale of company.</headline > ... < text><p > The stock of Tylan General Inc. jumped Tuesday after the maker of process - management equipment said it is exploring the sale of the company and added that it has already received some inquiries from potential buyers.</p>( ... )</text > ...",
      "< metadata > ... < codes class=\"bip : topics: 1 . 0 \" > < code code=\"C 1 5 \" > < /code > < code code=\"C 1 5 2 \" > < /code > < code code=\"C 1 8 \" > < /code > < code code=\"C 1 8 1 \" > < /code > < code code=\"CCAT \" > < /code > < /codes > ... < /metadata > Figure 1 : An excerpt from a random XML file of the RCV 1 dataset . [ 2 4 ] the least common label is the one that more specifically identifies the document .",
      "In order to compare a flat classification against hierarchical LCPN approach , we have created two data groups : ( 1 ) a train/test split by applying a holdout method [ 1 1 ] to randomly reserve 1 0 % of all RCV 1 tuples for test purposes and ( 2 ) a so - called \" hierarchical split \" by recursively stratifying subsets of the train subset based on the parent nodes .",
      "Initial experiments with these datasets indicated the already expected incorrect classifications that occur with NMLNP in deeper hierarchy levels due to the models ' inability to stop the classification before reaching a leaf node or recovering from it [ 3 8 ] .",
      "Besides using RCV 1 and its hierarchy as the main elements for experimentation , we also employed general - purpose pre - trained word embeddings .",
      "The group responsible for word 2 vec published a dataset with around 3 million word vectors with 3 0 0 elements in length that were trained on about 1 0 0 billion words read from Google News dataset 3 2 .",
      "The authors of GloVe also published pre - trained versions of word vectors ; for these experiments , we will use a   Out of the many possible classification models mentioned in section 5 , we concentrated our efforts on three of them , namely fastText , XGBoost , and CNN .",
      "These classifiers were trained using the two aforementioned pre - trained word embeddings as well as word embeddings obtained from the fastText supervised algorithm - more details in the upcoming paragraphs - and the following hierarchical strategies : • Flat : A single model was trained as in a general multi - class task with 1 0 3 classes while ignoring the hierarchical information - see subsection 2. 1 for more details . • LCPN + VC : An individual classification model was created for each one of the 2 2 datasets generated by the \" hierarchical split \" described in subsection 6. 1 .",
      "The following list describes the learning algorithms we used with details about parameters , specific data preprocessing , and variations : • fastText : We used this algorithm in two ways - as a classification learner and as a word embedding generator .",
      "We explored word embeddings with 5 different vector sizes to investigate how expanding the numerical distribution affects the final classification effectiveness in a flat strategy .",
      "During that step , we learned that fastText was able to improve its classification effectives as we increased the vector size up to a certain point only .",
      "Other training parameters : softmax loss function , bigrams , learning rate 0. 1 , and 5 epochs . • XGBoost : In order to accommodate the distributed text representation in a format suitable to this algorithm , we decided to combine word embeddings to compose a document representation from the corresponding word embeddings average .",
      "Besides these architectures that use word embeddings , we implemented one with TF - IDF representation , created from an all - lowercase , stemmed , puntuation - and stopword - free version of the RVC 1 dataset .",
      "For all experiments , we set the XGBoost algorithm to use a softmax objective and run it for 3 0 rounds , which seemed to be enough to converge to minimum loss .",
      "We then used the Keras API 3 5 to built a neural network with the following architecture : a frozen embedding layer that uses the fastText vectors , two convolution layers with rectified linear units ( ReLU with kernel size 5 and 1 2 8 output filters ) and max pooling ( pool size 5 ) each [ 8 ] , a densely - connected layer with ReLU activation and finally another densely - connected layer with softmax function .",
      "Other training parameters : 1 0 epochs , batch size 1 2 8 [ 2 1 ] . • SVM : We used the same document representation resulting from the word embeddings combination created for XGBoost as input attributes for an SVM classifier [ 1 3 ] .",
      "Moreover , despite a somewhat high Pearson correlation of 0. 7 5 6 between flat F 1 and lcaF 1 , the former is potentially misleading to assess the model effectiveness - for example , it indicates XGBoost with word 2 vec 3 4 Our pre - processed RCV 1 training subset had an average document length of 2 6 1 . 5 7 words with 9 0 as the mode .",
      "We suspect this comes from the fact that , when used in supervised mode , fastText uses the class label as learning objective , which results in word embeddings that specifically reflect the concepts behind the classes distribution .",
      "This hypothesis is supported by the fact that fastText word embeddings created in supervised mode with a relatively small amount of data yielded effectiveness on par with pre - trained word embeddings generated from much more data in an unsupervised manner .",
      "Considering the XGBoost algorithm , flat models using any pre - trained word embeddings are surpassed by SVM counterpart .",
      "Moreover , these XGBoost flat models had worse results than the traditional TF - IDF representation .",
      "Nevertheless , XGBoost achieved reasonable classification results and exceeded the baseline classifier in all contexts where it could take advantage of LCPN+VC strategy .",
      "Training this CNN - which is a rather simple implementation considering the complexity that top - notch CNN can reach - took around 6 hours in our computer ( Intel R Core TM i 5 - 4 3 0 0 U CPU at 1. 9 GHz with 8GB RAM ) , while typical training time for fastText and XGBoost ranged from 3 to 8 minutes for the former and 0. 2 to 2. 2 hours for the latter .",
      "Our results finally suggest that word embedding systems depend on the word embeddings quality to some extent , as we noticed that word 2 vec embeddings have a slight advantage over GloVe 's .",
      "At the same time , both SVM and XGBoost achieved fair results when using supervised fastText word embeddings generated from a relatively small amount of data .",
      "This contributes to the understanding word embeddings specifically generated during the classification task , even when short , are well appropriate representations for this problem .",
      "Throughout this work , we have analyzed the application of distributed text representations combined with modern classification algorithms implementations to the HTC task .",
      "After an observant literature research and careful examination of related works , we identified three noticeable word embeddings generation methods - GloVe , word 2 vec , and fastText - and three prominent classification models - fastText , XGBoost , and CNN - that recently improved the results for the typical text classification and could potentially provide similar advancements for the hierarchical specialization .",
      "We also noticed we could exploit the hierarchical structure to build classification models using LCPN strategy and virtual categories .",
      "In order to assess the feasibility and effectiveness of these representations , models , and strategies to the HTC task , we performed experiments using the RCV 1 dataset .",
      "The results indicate that classification models created using a hierarchical strategy ( LCPN with \" virtual category \" ) surpasses the flat approach in all experimented equivalent comparisons .",
      "FastText was the outstanding method as a classifier and provided very good results as a word embedding generator , despite the relatively small amount of data provided for this second task .",
      "These findings support the increasing understanding that combining task - specific word embeddings provides the best results for text classification [ 1 6 , 4 1 ] , to which now we include its hierarchical specialization .",
      "We plan to apply these methods to the PubMed data to check how such an approach extents to the medical text context - in particular the usage of fastText .",
      "As BioASQ provides pre - trained word embeddings generated with word 2 vec using a considerable amount of medical texts , comparing them with those that fastText creates in supervised mode should provides us with evidence for a more general understanding on how their quality affects the final classification results .",
      "We would also like to study the behavior of fastText when applied to task with more classes , such as the BioASQ , to check whether word embeddings with more than 3 0 or 4 0 elements would still allow for classification effectiveness improvement .",
      "Additionally , the exploration of other text representation extensions of word 2 vec , like paragraph 2 vec and doc 2 vec , could complement this investigation .",
      "However , both XGBoost and CNN ( through the Keras API ) allow for the loss function customization .",
      "We believe that finding a differentiable function that approximates either hF 1 or lcaF 1 and using it as the loss function rather then the softmax could finally bring together state - of - the - art algorithms with hierarchical information to create a method that implements a global HTC approach ."
    ],
    "relations": [
      [
        "NN:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "boosting methods:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "Rocchio:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "k NN:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "classification:Task",
        "Used-For",
        "TC:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "LSHTC:Dataset"
      ],
      [
        "LSHTC:Dataset",
        "Benchmark-For",
        "sentiment analysis:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "sentiment analysis:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "k -NN classifier:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "NLP classification:Task"
      ],
      [
        "NN models:Method",
        "Used-For",
        "NLP classification:Task"
      ],
      [
        "deep neural networks:Method",
        "Used-For",
        "TC:Task"
      ],
      [
        "deep neural networks:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "modern classification algorithms:Method",
        "Used-For",
        "HTC:Task"
      ],
      [
        "GloVe:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word 2 vec:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word embeddings:Method",
        "Trained-With",
        "BioASQ:Dataset"
      ],
      [
        "word 2 vec:Method",
        "Part-Of",
        "word embeddings:Method"
      ],
      [
        "GloVe:Method",
        "Compare-With",
        "word 2 vec:Method"
      ],
      [
        "flat classification:Method",
        "Compare-With",
        "hierarchical LCPN approach:Method"
      ],
      [
        "NMLNP:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word 2 vec:Method",
        "Trained-With",
        "Google News dataset:Dataset"
      ],
      [
        "fastText:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "XGBoost:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Part-Of",
        "word embeddings:Method"
      ],
      [
        "LCPN + VC:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "classification learner:Method"
      ],
      [
        "fastText:Method",
        "Used-For",
        "word embedding generator:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word embeddings:Method",
        "Part-Of",
        "TF - IDF representation:Method"
      ],
      [
        "TF - IDF representation:Method",
        "Trained-With",
        "RVC 1:Dataset"
      ],
      [
        "softmax:Method",
        "Part-Of",
        "XGBoost:Method"
      ],
      [
        "fastText:Method",
        "Part-Of",
        "frozen embedding layer:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "frozen embedding layer:Method"
      ],
      [
        "max pooling:Method",
        "Part-Of",
        "frozen embedding layer:Method"
      ],
      [
        "densely - connected layer:Method",
        "Part-Of",
        "frozen embedding layer:Method"
      ],
      [
        "densely - connected layer:Method",
        "Part-Of",
        "frozen embedding layer:Method"
      ],
      [
        "rectified linear units:Method",
        "Part-Of",
        "convolution:Method"
      ],
      [
        "ReLU:Method",
        "Synonym-Of",
        "rectified linear units:Method"
      ],
      [
        "ReLU activation:Method",
        "Part-Of",
        "densely - connected layer:Method"
      ],
      [
        "softmax:Method",
        "Part-Of",
        "densely - connected layer:Method"
      ],
      [
        "word embeddings:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "XGBoost:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "word 2 vec:Method",
        "Part-Of",
        "XGBoost:Method"
      ],
      [
        "fastText word embeddings:Method",
        "Compare-With",
        "word embeddings:Method"
      ],
      [
        "word embeddings:Method",
        "Part-Of",
        "XGBoost:Method"
      ],
      [
        "XGBoost:Method",
        "Compare-With",
        "SVM:Method"
      ],
      [
        "XGBoost:Method",
        "Compare-With",
        "TF - IDF:Method"
      ],
      [
        "XGBoost:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "word 2 vec:Method",
        "Compare-With",
        "GloVe:Method"
      ],
      [
        "fastText word embeddings:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "fastText word embeddings:Method",
        "Part-Of",
        "XGBoost:Method"
      ],
      [
        "SVM:Method",
        "Compare-With",
        "XGBoost:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "GloVe:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "word 2 vec:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "fastText:Method",
        "SubClass-Of",
        "word embeddings:Method"
      ],
      [
        "CNN:Method",
        "SubClass-Of",
        "classification models:Method"
      ],
      [
        "XGBoost:Method",
        "SubClass-Of",
        "classification models:Method"
      ],
      [
        "fastText:Method",
        "SubClass-Of",
        "classification models:Method"
      ],
      [
        "classification models:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "GloVe:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "word 2 vec:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "XGBoost:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "LCPN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "RCV 1:Dataset",
        "Benchmark-For",
        "HTC:Task"
      ],
      [
        "LCPN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "FastText:Method",
        "SubClass-Of",
        "word embedding generator:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "text classification:Task"
      ],
      [
        "fastText:Method",
        "Evaluated-With",
        "PubMed data:Dataset"
      ],
      [
        "word embeddings:Method",
        "Trained-With",
        "BioASQ:Dataset"
      ],
      [
        "word 2 vec:Method",
        "Part-Of",
        "word embeddings:Method"
      ],
      [
        "word embeddings:Method",
        "Compare-With",
        "fastText:Method"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fastText:Method",
        "Evaluated-With",
        "BioASQ:Dataset"
      ],
      [
        "word embeddings:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "paragraph 2 vec:Method",
        "SubClass-Of",
        "word 2 vec:Method"
      ],
      [
        "doc 2 vec:Method",
        "SubClass-Of",
        "word 2 vec:Method"
      ]
    ]
  },
  {
    "doc_id": "210157153",
    "chunk_id": 1,
    "content": [
      "In this paper , we propose Matrix - LSTM , a grid of Long Short - Term Memory ( LSTM ) cells that efficiently process events and learn end - to - end task - dependent event - surfaces .",
      "Compared to existing reconstruction approaches , our learned event - surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state - of - the - art of event - based object classification on the N - Cars dataset .",
      "Event - cameras only provide a timed sequence of changes that is not directly compatible with computer vision LSTM Neural Network End - to - End Training Figure 1 .",
      "The Matrix - LSTM end - to - end differentiable surface applied on an N - MNIST [ 3 1 ] sample .",
      "In this paper , we focus on this recent trend in event - based processing , and propose to apply a Long Short - Term Memory ( LSTM ) network [ 1 6 ] as a convolutional filter over the 2D stream of events to accumulate pixel information through time and extract pixel values for building 2D event representations .",
      "The framework is end - to - end differentiable , it can be used as input of any existing frame - based state - of - the - art architecture and jointly trained to extract the best representation from the events . • Replacing input representations with a Matrix - LSTM layer in existing architectures , we show that it improves the state - of - the - art on event - based object classification on N - CARS [ 4 0 ] by 3. 3 % and performs better than hand - crafted features on N - Caltech 1 0 1 [ 3 1 ] .",
      "Finally , it improves optical flow estimation on the MVSEC benchmark [ 4 9 ] up to 3 0 . 7 6 % over handcrafted features [ 4 9 ] and up to 2 3 . 0 7 % over end - to - end differentiable ones [ 1 3 ] . • We developed custom CUDA kernels to efficiently aggregate events by position and perform a convolutionlike operation on the stream of events using an LSTM as a convolutional filter .",
      "To this end , convolutional neural networks ( CNNs ) are by far the most widespread method in frame - based architectures for image classification [ 1 5 , 1 9 , 4 2 ] , object detection [ 1 4 , 2 4 , 3 5 ] , semantic segmentation [ 7 , 2 5 , 4 6 ] , and many others .",
      "Initially , neural systems designed to perform spike - based computation , such as Spiking Neural Networks ( SNNs ) [ 2 6 ] , have been applied to event - based processing in several tasks , e.g. , edge detection [ 4 4 , 2 8 ] , object classification [ 2 1 , 1 0 ] and hand - gestures recognition [ 4 ] .",
      "Recent works [ 3 6 ] tried to overcome these limitations by first training a frame - based conventional neural network , and then convert its weights into SNNs parameters , managing to deal with complex structures such as GoogLeNet Inception - V 3 [ 4 2 ] .",
      "Crucially , the accumulation procedure employed in HATS is hand - crafted , while our work is end - toend trainable thanks to a grid of LSTM cells [ 1 6 ] that learn the accumulation step .",
      "In [ 4 8 ] , the authors propose the EV - FlowNet network for optical flow estimation together with a new time - surface variant .",
      "A multi - layer perceptron ( MLP ) is used to implement a trilinear filter that produces a voxelgrid of temporal features .",
      "Among these , [ 3 0 ] uses a variant of the LSTM network , called Phas - edLSTM , to learn the precise timings of events .",
      "In contrast , in this paper we use the LSTM as a convolutional filter , thus , we obtain a translation - invariant module that integrates local temporal features independently while retaining spatial structures .",
      "Although LSTMs should be able to retain memory over very long periods , we found that discretizing time into intervals helps the Matrix - LSTM layer in maintaining event information , especially in tasks requiring rich and precise time information such as optical flow estimation ( see Section 4. 2 ) .",
      "Inspired by the convolution operation defined on images , we designed the Matrix - LSTM layer to enjoy translation invariance by performing a local mapping of events into features .",
      "By sharing the parameters across the LSTM cells , we can consider Matrix - LSTM as a convolution operator over the 2D - grid streams of events applied with a 1 × 1 receptive field .",
      "To showcase the flexibility of the proposed mechanism , we tested it on two different tasks : a high - level task , i.e. , object classification ( see Section A ) and a low - level one , i.e. , optical flow estimation ( see Section 4. 2 ) where the network is required to extract effective temporal features .",
      "We evaluated the model on the classification task using two publicly available event - based collections , namely the N - Cars [ 4 0 ] and the N - Caltech 1 0 1 [ 3 1 ] datasets .",
      "The N - Caltech 1 0 1 collection is an event - based conversion of the popular Caltech - 1 0 1 [ 2 2 ] dataset obtained by moving an event - based camera in front of a still monitor showing one of the original RGB images ."
    ],
    "relations": [
      [
        "LSTM:Method",
        "Synonym-Of",
        "Long Short - Term Memory:Method"
      ],
      [
        "Matrix - LSTM:Method",
        "SubClass-Of",
        "Long Short - Term Memory:Method"
      ],
      [
        "MVSEC:Dataset",
        "Benchmark-For",
        "optical flow estimation:Task"
      ],
      [
        "N - Cars:Dataset",
        "Benchmark-For",
        "object classification:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Evaluated-With",
        "N - MNIST:Dataset"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "Long Short - Term Memory:Method"
      ],
      [
        "Matrix - LSTM:Method",
        "Used-For",
        "object classification:Task"
      ],
      [
        "N - CARS:Dataset",
        "Benchmark-For",
        "object classification:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Evaluated-With",
        "N - CARS:Dataset"
      ],
      [
        "Matrix - LSTM:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "MVSEC:Dataset",
        "Benchmark-For",
        "optical flow estimation:Task"
      ],
      [
        "LSTM:Method",
        "SubClass-Of",
        "convolutional filter:Method"
      ],
      [
        "CNNs:Method",
        "Synonym-Of",
        "convolutional neural networks:Method"
      ],
      [
        "convolutional neural networks:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "convolutional neural networks:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "convolutional neural networks:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "SNNs:Method",
        "Synonym-Of",
        "Spiking Neural Networks:Method"
      ],
      [
        "Spiking Neural Networks:Method",
        "Used-For",
        "edge detection:Task"
      ],
      [
        "Spiking Neural Networks:Method",
        "Used-For",
        "object classification:Task"
      ],
      [
        "Spiking Neural Networks:Method",
        "Used-For",
        "hand - gestures recognition:Task"
      ],
      [
        "EV - FlowNet:Method",
        "Used-For",
        "optical flow estimation:Task"
      ],
      [
        "MLP:Method",
        "Synonym-Of",
        "multi - layer perceptron:Method"
      ],
      [
        "Phas - edLSTM:Method",
        "SubClass-Of",
        "LSTM:Method"
      ],
      [
        "LSTM:Method",
        "SubClass-Of",
        "convolutional filter:Method"
      ],
      [
        "Matrix - LSTM:Method",
        "Used-For",
        "optical flow estimation:Task"
      ],
      [
        "1 × 1 receptive field:Method",
        "Part-Of",
        "Matrix - LSTM:Method"
      ],
      [
        "Matrix - LSTM:Method",
        "SubClass-Of",
        "convolution operator:Method"
      ]
    ]
  },
  {
    "doc_id": "210157153",
    "chunk_id": 2,
    "content": [
      "We used two network configurations to test Matrix - LSTM on both datasets , namely the classifier used in Events - to - Video [ 3 4 ] , and the one used to evaluate the EST [ 1 3 ] reconstruction .",
      "Both are based on ResNet [ 1 5 ] backbones and pre - trained on ImageNet [ 9 ] .",
      "Events - to - Video [ 3 4 ] uses a ResNet 1 8 configuration maintaining the first 3 channels convolution ( since reconstructed images are RGB ) while adding an extra fully - connected layer to account for the different number of classes in both N - Calthec 1 0 1 and N - Cars ( we refer to this configuration as ResNet - Ev 2 Vid ) .",
      "EST [ 1 3 ] instead uses a ResNet 3 4 backbone and replaces both the first and last layers respectively , with a convolution matching the input features , and a fullyconnected layer with the proper number of neurons ( we refer to this configuration as ResNet - EST ) .",
      "We perform early stopping on a validation set in all experiments , using 2 0 % of the training on N - Cars and using the splits provided by the EST official code repository [ 1 2 ] for N - Caltech 1 0 1 .",
      "On N - Caltech 1 0 1 , instead , we use a batch size of 1 6 while decaying the learning rate by a factor of 0. 8 after each epoch when testing on ResNet - Ev 2 Vid , and a batch size of 1 0 0 with no decay with the ResNet - EST setup .",
      "The empirical evaluation is organized as it follow for both ResNet - Ev 2 Vid and ResNet - EST configurations .",
      "We al - ways perform hyper - parameters search using a ResNet 1 8 backbone on N - Cars , since it is faster to train and thus it allows us to explore a larger combination of parameters .",
      "We then select the best configuration to train the remaining architectures , namely , ResNet 3 4 on N - Cars and both variants on N - Caltech 1 0 1 .",
      "Matrix - LSTM + ResNet - Ev 2 Vid .",
      "We start out with the ResNet - Ev 2 Vid baseline ( setting up the Matrix - LSTM to output 3 channels ) by identifying the optimal time feature to provide as input to the LSTM , as reported in Table 1 .",
      "In Table 1 we also show how the network performance changes when the frame normalization used while training the ResNet backbone on ImageNet is applied to the Matrix - LSTM output .",
      "Matrix - LSTM + ResNet - EST .",
      "We continue the experiments on N - Cars by considering the ResNet - EST configuration as the baseline , where we explore the effects of using bins , i.e. , intervals , on the quality of the Matrix - LSTM reconstruction .",
      "Having found the bin setup performing the best ( namely B = 2 , 4 ) , and since our Matrix - LSTM module can produce a variable number of output features , we perform the last set of experiments to select the Matrix - LSTM hidden size ( a parameter that also controls the number of output channels ) .",
      "Results of the top performing configurations for both ResNet - Ev 2 Vid and ResNet - EST variants on both N - Cars and N - Caltech 1 0 1 are reported in Table 5 .",
      "We use relative delay with ResNet - Ev 2 Vid and global ts + local ts with ResNet - EST .",
      "Indeed , using the ResNet 3 4 - Ev 2 Vid setup , our solution sets a new state - of - the - art on N - Cars , even surpassing the Events - to - Video model that was trained to extract realistic reconstructions , and that could , therefore , take full advantage of the ResNet pre - training .",
      "On the ResNet - EST configuration , the model performs consistently better on N - Cars , while slightly worse on N - Caltech 1 0 1 on most configurations .",
      "However , we remark that search for the best configuration was indeed performed on N - Cars , while a hyper - parameter search directly performed on N - Caltech 1 0 1 would have probably lead to better results .",
      "Fusing event - data with lidar , IMU , motion capture and GPS sources , MVSEC is the first eventbased dataset to provide a solid benchmark for event camera in real urban conditions ."
    ],
    "relations": [
      [
        "N - Cars:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "N - Caltech 1 0 1:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Part-Of",
        "Events - to - Video:Method"
      ],
      [
        "EST:Method",
        "Used-For",
        "reconstruction:Task"
      ],
      [
        "ResNet:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "ResNet 1 8:Method",
        "Part-Of",
        "Events - to - Video:Method"
      ],
      [
        "fully - connected layer:Method",
        "Part-Of",
        "Events - to - Video:Method"
      ],
      [
        "3 channels convolution:Method",
        "Part-Of",
        "ResNet 1 8:Method"
      ],
      [
        "Events - to - Video:Method",
        "Evaluated-With",
        "N - Calthec 1 0 1:Dataset"
      ],
      [
        "ResNet - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Calthec 1 0 1:Dataset"
      ],
      [
        "Events - to - Video:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet 1 8:Method",
        "Part-Of",
        "ResNet - Ev 2 Vid:Method"
      ],
      [
        "3 channels convolution:Method",
        "Part-Of",
        "ResNet - Ev 2 Vid:Method"
      ],
      [
        "fully - connected layer:Method",
        "Part-Of",
        "ResNet - Ev 2 Vid:Method"
      ],
      [
        "fullyconnected layer:Method",
        "Part-Of",
        "ResNet - EST:Method"
      ],
      [
        "EST:Method",
        "Part-Of",
        "ResNet - EST:Method"
      ],
      [
        "ResNet - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "ResNet - EST:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "ResNet 1 8:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet 3 4:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet 3 4:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "ResNet:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "ResNet:Method",
        "Part-Of",
        "Matrix - LSTM:Method"
      ],
      [
        "ResNet - EST:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "Matrix - LSTM:Method",
        "Used-For",
        "reconstruction:Task"
      ],
      [
        "ResNet - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ]
    ]
  },
  {
    "doc_id": "210157153",
    "chunk_id": 3,
    "content": [
      "We replaced this input representation with the Matrix - LSTM reconstruction mechanism , setting the output channels to 4 as well .",
      "To perform a fair comparison with Ev - FlowNet , that uses a 4 - channels eventsurface , we fix the hidden state of our LSTM to 4 , and use the same training hyper - parameters as well .",
      "We noticed that EV - FlowNet is quite unstable at higher learning rates , while Matrix - LSTM could benefit from larger rates , so we multiply its learning rate , i.e. , the Matrix - LSTM gradients , by a factor of 1 0 during training .",
      "We compared the time performance of Matrix - LSTM with other event representations following EST [ 1 3 ] and HATS [ 4 0 ] evaluation procedure .",
      "The average forward pass time of Matrix - LSTM has been computed on both ResNet - Ev 2 Vid and ResNet - EST configurations using a GeForce GTX 1 0 8 0 Ti GPU .",
      "Our surface achieves similar time performance than both HATS and EST , performing only ≈ 2 ms slower than EST on the same configuration ( 9 bins and 2 channels ) .",
      "However , while EST can exploit parallel batch computation of events within the same sample , since each event feature is processed independently , Matrix - LSTM relies on sequential computation to reconstruct the surface .",
      "In Figure 4 we analyze the accuracy - vs - latency tradeoff on the N - Cars dataset , as proposed in [ 4 0 ] , using the ResNet 1 8 - Ev 2 Vid configuration .",
      "Gabor - SNN [ 4 0 ] HOTS [ 2 0 ] Matrix - LSTM delay Matrix - LSTM delay + aug .",
      "By modeling the reconstruction with a spatially shared LSTM we obtained a fully differentiable procedure that can be trained end - to - end to extract the event representation that best fits the task at hand .",
      "We proposed an efficient implementation of the method that exploits parallel batchwise computation and demonstrates the effectiveness of the Matrix - LSTM layer on multiple tasks , improving the stateof - the - art of object classification on N - Cars by 3. 3 % and the performance on optical flow prediction on MVSEC by up to 2 3 . 0 7 % over previous differentiable techniques [ 1 3 ] .",
      "As a future line of research , we plan to explore the use of Matrix - LSTM for more complex tasks such as gray - scale frame reconstruction [ 3 4 ] , ego - motion and depth estimation [ 5 0 , 4 5 ] .",
      "In Table 8 we show how different choices of receptive field size impact classification accuracy on the N - Cars [ 4 0 ] datasets using the ResNet 1 8 - Ev 2 Vid configuration with relative delay encoding .",
      "Event surfaces produced by the Matrix - LSTM layer are indeed more blurry and this may prevent the subsequent ResNet backbone from extracting effective features .",
      "In both configurations , finally , increasing the batch size reduces the mean processing time .   We performed optical flow experiments starting from the publicly available Ev - FlowNet codebase [ 4 7 ] and replacing the original hand - crafted features with the proposed Matrix - LSTM layer .",
      "Despite this discrepancy , which prevents the Matrix - LSTM performance on dt= 4 settings to be directly compared with the results reported on the Ev - FlowNet paper , we can still evaluate the benefits of our surface on larger flow magnitudes .",
      "Using our Ev - FlowNet results as baseline , we show that Matrix - LSTM is able to improve the optical flow quality even on the dt= 4 setting , highlighting the capability of the layer to adapt to different sequence lengths and movement conditions .",
      "We propose to visualize the Matrix - LSTM surface as an RGB image by using the ResNet 1 8 - Ev 2 Vid configuration and interpreting the 3 output channels as RGB color .",
      "A video of such visualization showing the incremental frame reconstruction on N - Caltech 1 0 1 samples is provided at this url : https://drive.google.com/open ? id= 1 KzhKKwJGXvMnhbg 1 l 6 gEArgYote 7 WW 8 V .",
      "The same visualization technique is also used to display N - Cars Matrix - LSTM surfaces in Table 8 ."
    ],
    "relations": [
      [
        "ResNet - EST:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "ResNet - EST:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "ResNet 3 4 - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet 3 4 - Ev 2 Vid:Method",
        "Compare-With",
        "Events - to - Video:Method"
      ],
      [
        "ResNet:Method",
        "Part-Of",
        "Events - to - Video:Method"
      ],
      [
        "ResNet - EST:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "ResNet - EST:Method",
        "Evaluated-With",
        "N - Caltech 1 0 1:Dataset"
      ],
      [
        "Matrix - LSTM:Method",
        "Used-For",
        "reconstruction:Task"
      ],
      [
        "EV - FlowNet:Method",
        "Compare-With",
        "Matrix - LSTM:Method"
      ],
      [
        "EST:Method",
        "Compare-With",
        "Matrix - LSTM:Method"
      ],
      [
        "ResNet 1 8 - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "reconstruction:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Used-For",
        "object classification:Task"
      ],
      [
        "N - Cars:Dataset",
        "Benchmark-For",
        "object classification:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "Matrix - LSTM:Method",
        "Used-For",
        "optical flow prediction:Task"
      ],
      [
        "MVSEC:Dataset",
        "Evaluated-With",
        "optical flow prediction:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Evaluated-With",
        "MVSEC:Dataset"
      ],
      [
        "ResNet 1 8 - Ev 2 Vid:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "N - Cars:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "ResNet 1 8 - Ev 2 Vid:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ],
      [
        "Matrix - LSTM:Method",
        "Part-Of",
        "Ev - FlowNet:Method"
      ],
      [
        "Matrix - LSTM:Method",
        "Compare-With",
        "Ev - FlowNet:Method"
      ],
      [
        "Ev - FlowNet:Method",
        "Compare-With",
        "Matrix - LSTM:Method"
      ],
      [
        "N - Caltech 1 0 1:Dataset",
        "Benchmark-For",
        "reconstruction:Task"
      ],
      [
        "Matrix - LSTM:Method",
        "Evaluated-With",
        "N - Cars:Dataset"
      ]
    ]
  },
  {
    "doc_id": "3627225",
    "chunk_id": 1,
    "content": [
      "By learning to learn in the concept space rather than in the complicated instance space , deep meta - learning can substantially improve vanilla meta - learning , which is demonstrated on various few - shot image recognition problems .",
      "For example , on 5 - way - 1 - shot image recognition on CIFAR - 1 0 0 and CUB - 2 0 0 , it improves Matching Nets from 5 0 . 5 3 % and 5 6 . 5 3 % to 5 8 . 1 8 % and 6 3 . 4 7 % , improves MAML from 4 9 . 2 8 % and 5 0 . 4 5 % to 5 6 . 6 5 % and 6 4 . 6 3 % , and improves Meta - SGD from 5 3 . 8 3 % and 5 3 . 3 4 % to 6 1 . 6 2 % and 6 6 . 9 5 % , respectively .",
      "Recently , meta - learning , pioneered by ( Schmidhuber , 1 9 8 7 ) , draws renewed interest which learns on the level of tasks instead of instances , and learns task - agnostic learning algorithms ( e.g. SGD ) instead of task - specific models ( e.g. CNN ) .",
      "Meta - learning has been shown to significantly outperform conventional learning on various few - shot learning problems , ranging from classification ( Santoro et al. , 2 0 1 6 ; Vinyals et al. , 2 0 1 6 ; Ravi & Larochelle , 2 0 1 7 ; Finn et al. , 2 0 1 7 ; Li et al. , 2 0 1 7 ; Snell et al. , 2 0 1 7 ) , reinforcement learning ( Wang et al. , 2 0 1 6 ; Duan et al. , 2 0 1 6 ; Nikhil Mishra , 2 0 1 8 ; Kevin Frans , 2 0 1 8) , regression ( Santoro et al. , 2 0 1 6 ; Finn et al. , 2 0 1 7 ) , machine translation ( Kaiser et al. , 2 0 1 7 ) , to object tracking ( Park & Berg , 2 0 1 8) .",
      "In our deep meta - learning framework , the key idea is to train a concept generator together with meta - learning tasks and large - scale image recognition tasks , which will finally improve the performance of vanilla meta learning methods .",
      "Our main contributions can be summarized as follows : • We propose deep meta - learning to integrate the power of deep learning into meta - learning , and show it improves vanilla meta - learning significantly on the problem of few - shot image recognition ( see Figure 1 ) .",
      "Since the concept generator will continue to evolve with coming labeled data , this framework could literally be implemented as a life - long learning system . • We instantiate the deep meta - learning framework on top of three state - of - the - art meta - learners including Matching Nets ( Vinyals et al. , 2 0 1 6 ) , MAML ( Finn et al. , 2 0 1 7 ) , and Meta - SGD ( Li et al. , 2 0 1 7 ) , and conduct extensive experiments to show that deep metalearning utilizes data more efficiently than all existing methods , and provides significantly better results on few - shot image recognition .",
      "In ( Ravi & Larochelle , 2 0 1 7 ) , an LSTM is learned to train a learner such as CNN as it rolls out . ( Finn et al. , 2 0 1 7 ) learn how to initialize SGD while ( Li et al. , 2 0 1 7 ) learn a full - stack SGD , including initialization , update direction , and learning rate .",
      "Memory - augmented neural networks show high capacity for meta - learning . ( Santoro et al. , 2 0 1 6 ) train an LSTM as a controller for accessing ( read and write ) an additional memory module , which is an extension of the internal memory in LSTM .",
      "For example , Fast R - CNN ( Girshick , 2 0 1 5 ) trains image classifiers and bounding - box regressors simultaneously to perform object detection .",
      "Quite a few methods have been proposed for few - shot image recognition . ( Li et al. , 2 0 0 6 ) present a Bayesian model for learning categories from a few examples per category . ( Salakhutdinov et al. , 2 0 1 2 ) organize seen categories into super - categories to derive hierarchically structured priors for new categories using a hierarchical Bayesian model . ( Lake et al. , 2 0 1 1 ) develop a generative model that composes pen strokes into characters for handwritten character recognition . ( Wong & Yuille , 2 0 1 5 ) extend this idea to natural images without relying on domain knowledge .",
      "In this section , we propose a new meta - learning framework , called deep meta - learning ( DEML ) , which integrates the representation power of deep learning into meta - learning , and enables learning to learn in the concept space .",
      "Meta - learning tasks T follow a distribution p(T ) in a task space , and ( x , y ) represents a labeled instance sampled from an external dataset D. The objective is to minimize the expectation of the joint , denoted by J , of two losses : the loss L T ( θ M , θ G ) on the meta - learning tasks and the loss L ( x , y ) ( θ D , θ G ) on the concept discrimination tasks .",
      "The concept generator G , parameterized by θ G , is a deep neural network that could be any popular convolutional neural network such as AlexNet ( Krizhevsky et al. , 2 0 1 2 ) , Inception ( Szegedy et al. , 2 0 1 5 ) , VGG ( Simonyan & Zisserman , 2 0 1 4 ) , or ResNet ( He et al. , 2 0 1 6 ) .",
      "The concept discriminator D , parameterized by θ D , is designed to predict labels for concepts generated by G. It could be implemented with any supervised learning method , such as support vector machines , nearest neighbor classifiers , or neural networks .",
      "The goal is to minimize the expected loss on the concept discrimination tasks : where could be any loss function suitable for concept discrimination .",
      "For example , if we choose Matching Nets ( Vinyals et al. , 2 0 1 6 ) as our meta - learner , then f T • G(x ) would be formalized as follows : is the softmax over the cosine distance c and the embedding function g. If MAML ( Finn et al. , 2 0 1 7 ) is chosen as the meta - learner , then where and α is a fixed learning rate ."
    ],
    "relations": [
      [
        "deep meta - learning:Method",
        "Compare-With",
        "vanilla meta - learning:Method"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "few - shot image recognition:Task"
      ],
      [
        "CIFAR - 1 0 0:Dataset",
        "Benchmark-For",
        "5 - way - 1 - shot image recognition:Task"
      ],
      [
        "CUB - 2 0 0:Dataset",
        "Benchmark-For",
        "5 - way - 1 - shot image recognition:Task"
      ],
      [
        "Matching Nets:Method",
        "Used-For",
        "5 - way - 1 - shot image recognition:Task"
      ],
      [
        "MAML:Method",
        "Used-For",
        "5 - way - 1 - shot image recognition:Task"
      ],
      [
        "Meta - SGD:Method",
        "Used-For",
        "5 - way - 1 - shot image recognition:Task"
      ],
      [
        "Matching Nets:Method",
        "Evaluated-With",
        "CIFAR - 1 0 0:Dataset"
      ],
      [
        "MAML:Method",
        "Evaluated-With",
        "CIFAR - 1 0 0:Dataset"
      ],
      [
        "Meta - SGD:Method",
        "Evaluated-With",
        "CIFAR - 1 0 0:Dataset"
      ],
      [
        "Matching Nets:Method",
        "Evaluated-With",
        "CUB - 2 0 0:Dataset"
      ],
      [
        "MAML:Method",
        "Evaluated-With",
        "CUB - 2 0 0:Dataset"
      ],
      [
        "Meta - SGD:Method",
        "Evaluated-With",
        "CUB - 2 0 0:Dataset"
      ],
      [
        "SGD:Method",
        "SubClass-Of",
        "task - agnostic learning algorithms:Method"
      ],
      [
        "CNN:Method",
        "SubClass-Of",
        "task - specific models:Method"
      ],
      [
        "Meta - learning:Method",
        "Used-For",
        "few - shot learning problems:Task"
      ],
      [
        "classification:Task",
        "SubTask-Of",
        "few - shot learning problems:Task"
      ],
      [
        "reinforcement learning:Task",
        "SubTask-Of",
        "few - shot learning problems:Task"
      ],
      [
        "regression:Task",
        "SubTask-Of",
        "few - shot learning problems:Task"
      ],
      [
        "machine translation:Task",
        "SubTask-Of",
        "few - shot learning problems:Task"
      ],
      [
        "object tracking:Task",
        "SubTask-Of",
        "few - shot learning problems:Task"
      ],
      [
        "Meta - learning:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Meta - learning:Method",
        "Used-For",
        "reinforcement learning:Task"
      ],
      [
        "Meta - learning:Method",
        "Used-For",
        "regression:Task"
      ],
      [
        "Meta - learning:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "Meta - learning:Method",
        "Used-For",
        "object tracking:Task"
      ],
      [
        "concept generator:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "meta - learning tasks:Task",
        "Used-For",
        "concept generator:Method"
      ],
      [
        "image recognition:Task",
        "Used-For",
        "concept generator:Method"
      ],
      [
        "deep meta - learning:Method",
        "Compare-With",
        "vanilla meta learning:Method"
      ],
      [
        "meta - learning:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "deep learning:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "deep meta - learning:Method",
        "Compare-With",
        "vanilla meta - learning:Method"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "few - shot image recognition:Task"
      ],
      [
        "vanilla meta - learning:Method",
        "Used-For",
        "few - shot image recognition:Task"
      ],
      [
        "Matching Nets:Method",
        "SubClass-Of",
        "meta - learners:Method"
      ],
      [
        "MAML:Method",
        "SubClass-Of",
        "meta - learners:Method"
      ],
      [
        "Meta - SGD:Method",
        "SubClass-Of",
        "meta - learners:Method"
      ],
      [
        "deep metalearning:Method",
        "Used-For",
        "few - shot image recognition:Task"
      ],
      [
        "Memory - augmented neural networks:Method",
        "Used-For",
        "meta - learning:Method"
      ]
    ]
  },
  {
    "doc_id": "3627225",
    "chunk_id": 2,
    "content": [
      "After introducing the framework , modules , and criterion of deep meta - learning , we are ready to describe a complete algorithm of DEML .",
      "In this case , our deep meta - learning can be formulated as the following optimization problem : and λ is a hyperparameter balancing meta - learning and concept discrimination .",
      "The stochastic gradient descent ( SGD ) algorithm can be applied to optimize the above objective .",
      "In our implementation , we use the Adam ( Kingma & Ba , 2 0 1 4 ) method , a variant of SGD .",
      "In this section , we evaluate the proposed deep meta - learning ( DEML ) on a number of few - shot image recognition problems , but note that it is applicable to classification , reinforcement learning , and regression in general .",
      "For concept discrimination tasks , we perform experiments on a subset of ImageNet ( Deng et al. , 2 0 0 9 ) .",
      "For meta - learning tasks , we perform experiments on MiniImagenet ( Vinyals et al. , 2 0 1 6 ) , Caltech - 2 5 6 ( Griffin et al. , 2 0 0 7 ) , CIFAR - 1 0 0 ( Krizhevsky , 2 0 0 9 ) , and CUB - 2 0 0 ( Wah et al. , 2 0 1 1 ) .",
      "The whole dataset is too large , and so in our experiments we use a subset ImageNet - 2 0 0 with 2 0 0 classes sampled from 9 0 0 classes ( excluding the 1 0 0 classes used in MiniImagenet ( Vinyals et al. , 2 0 1 6 ) ) .",
      "Particularly , MiniImagenet and ImageNet - 2 0 0 are mutually exclusive at class level .",
      "The Caltech - 2 5 6 dataset ( Griffin et al. , 2 0 0 7 ) is a successor to the well - known dataset Caltech - 1 0 1 .",
      "Sample n tasks T i ∼ p(T ) and m instances ( x j , y j ) ∼ D 6 : 8 : end for 1 1 : 1 2 : end while To compare DEML with existing meta - learning methods , we evaluate existing meta - learning methods ( Matching Nets ( Vinyals et al. , 2 0 1 6 ) , MAML ( Finn et al. , 2 0 1 7 ) , and Meta - SGD ( Li et al. , 2 0 1 7 ) ) on meta - learning datasets as our baselines .",
      "To show that the improvements of DEML are not solely attributed to the deeper neural network and rescaled images , we also evaluate the previous approaches with exactly the same architecture ( excluding the concept discriminator ) and inputs as DEML .",
      "Accordingly , their deep version implementations are denoted by Deep Matching Nets , Deep MAML , and Deep Meta - SGD , respectively .",
      "Since DEML is a meta - learneragnostic framework for meta - learning , we re - implement Matching Nets , MAML and Meta - SGD on DEML with the following implementation configurations .",
      "When choosing Matching Nets as the meta - learner , the learner is a neural network with an input layer of size 2 0 4 8 , followed by one hidden layer of size 1 0 2 4 with ReLU nonlinearities , and then an output layer of size 5 1 2 .",
      "When choosing MAML or Meta - SGD as the meta - learner , the learner is a neural network with the same input layer , followed by two hidden layers of size 1 0 2 4 and 5 1 2 with ReLU nonlinearities , and then an output layer of size 5 .",
      "ImageNet - 2 0 0 is sampled for the image recognition pipeline D • G. The prediction loss is measured by the mean of crossentropy over all the examples in this batch ."
    ],
    "relations": [
      [
        "Fast R - CNN:Method",
        "Used-For",
        "image classifiers:Task"
      ],
      [
        "Fast R - CNN:Method",
        "Used-For",
        "bounding - box regressors:Task"
      ],
      [
        "Fast R - CNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "generative model:Method",
        "Used-For",
        "handwritten character recognition:Task"
      ],
      [
        "deep meta - learning:Method",
        "SubClass-Of",
        "meta - learning framework:Method"
      ],
      [
        "DEML:Method",
        "Synonym-Of",
        "deep meta - learning:Method"
      ],
      [
        "deep learning:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "meta - learning:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "convolutional neural network:Method",
        "SubClass-Of",
        "deep neural network:Method"
      ],
      [
        "concept generator:Method",
        "SubClass-Of",
        "deep neural network:Method"
      ],
      [
        "AlexNet:Method",
        "SubClass-Of",
        "convolutional neural network:Method"
      ],
      [
        "Inception:Method",
        "SubClass-Of",
        "convolutional neural network:Method"
      ],
      [
        "VGG:Method",
        "SubClass-Of",
        "convolutional neural network:Method"
      ],
      [
        "ResNet:Method",
        "SubClass-Of",
        "convolutional neural network:Method"
      ],
      [
        "concept discriminator:Method",
        "Used-For",
        "predict labels for concepts generated by G.:Task"
      ],
      [
        "support vector machines:Method",
        "SubClass-Of",
        "supervised learning method:Method"
      ],
      [
        "nearest neighbor classifiers:Method",
        "SubClass-Of",
        "supervised learning method:Method"
      ],
      [
        "neural networks:Method",
        "SubClass-Of",
        "supervised learning method:Method"
      ],
      [
        "Matching Nets:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "MAML:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "meta - learning:Task"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "concept discrimination:Task"
      ],
      [
        "SGD:Method",
        "Synonym-Of",
        "stochastic gradient descent:Method"
      ],
      [
        "Adam:Method",
        "SubClass-Of",
        "SGD:Method"
      ],
      [
        "DEML:Method",
        "Synonym-Of",
        "deep meta - learning:Method"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "few - shot image recognition:Task"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "reinforcement learning:Task"
      ],
      [
        "deep meta - learning:Method",
        "Used-For",
        "regression:Task"
      ],
      [
        "ImageNet:Dataset",
        "Benchmark-For",
        "concept discrimination:Task"
      ],
      [
        "MiniImagenet:Dataset",
        "Benchmark-For",
        "meta - learning tasks:Task"
      ],
      [
        "Caltech - 2 5 6:Dataset",
        "Benchmark-For",
        "meta - learning tasks:Task"
      ],
      [
        "CIFAR - 1 0 0:Dataset",
        "Benchmark-For",
        "meta - learning tasks:Task"
      ],
      [
        "CUB - 2 0 0:Dataset",
        "Benchmark-For",
        "meta - learning tasks:Task"
      ],
      [
        "DEML:Method",
        "Compare-With",
        "meta - learning methods:Method"
      ],
      [
        "Matching Nets:Method",
        "SubClass-Of",
        "meta - learning methods:Method"
      ],
      [
        "MAML:Method",
        "SubClass-Of",
        "meta - learning methods:Method"
      ],
      [
        "Meta - SGD:Method",
        "Used-For",
        "meta - learning:Task"
      ],
      [
        "Matching Nets:Method",
        "Used-For",
        "meta - learning:Task"
      ],
      [
        "MAML:Method",
        "Used-For",
        "meta - learning:Task"
      ]
    ]
  },
  {
    "doc_id": "3627225",
    "chunk_id": 3,
    "content": [
      "The number of iterations is 6 0 , 0 0 0 in the experiments on MiniImagenet , Caltech - 2 5 6 , Meta - testing .",
      "For both MAML and Meta - SGD , the meta - learner uses onestep adaptation during meta - training and meta - testing for fair , and the learning rate α for MAML is set to 0.0 1 in all experiments .",
      "The comparison results between DEML versions and vanilla versions of Matching Nets , MAML , and Meta - SGD are summarized in Table   1 .",
      "To validate that the improvements of DEML are not merely because of the deeper neural network and rescaled images , we also evaluate the deep versions of the previous approaches on MiniImagenet as mentioned in Section 4. 2 .",
      "We enlarge the metatraining dataset by merging together the original 6 4 classes of MiniImagenet and the 2 0 0 classes of ImageNet - 2 0 0 .",
      "To compare deep metalearning with transfer learning , we also evaluate some vari - Another simplified version of DEML is Decaf+Meta - SGD , where one pretrained generator G is attached to the metalearner to execute meta - training process .",
      "It is interesting to note that the baseline Decaf+kNN achieves the best performance on MiniImagenet and Caltech - 2 5 6 .",
      "Since the concept generator G is trained on ImageNet - 2 0 0 , which is quite similar to MiniImagenet and Caltech - 2 5 6 ( Tommasi et al. , 2 0 1 7 ) , representations provided by it are so effective that the naive nearest - neighbor baseline achieves the best performance .",
      "On the contrary , the performance of Decaf+kNN drops a lot on CIFAR - 1 0 0 and CUB - 2 0 0 , since these two datasets are quite different from ImageNet - 2 0 0 .",
      "To emphasize the necessity of our joint learning process , we propose the third version Decaf+Fine - Tune+Meta - SGD which is the same as Decaf+Meta - SGD except that the concept generator and the meta - learner are trained together during meta - training process .",
      "The models are trained for 6 0 , 0 0 0 and 2 0 , 0 0 0 iterations on CIFAR - 1 0 0 and CUB - 2 0 0 , respectively , and the results are shown in Figure 3 , together with the results of Decaf+Meta - SGD and DEML+Meta - SGD .",
      "DEML+Meta - SGD performs consistently better than Decaf+Meta - SGD and Decaf+Fine - Tune+Meta - SGD on all cases by a wide margin .",
      "We verify it on CIFAR - 1 0 0 with the 5 - way - 5 - shot case with DEML+Meta - SGD ( Figure 4 ) .",
      "A balance between the external knowledge and the internal meta - level knowledge is useful in DEML .   In this paper , we propose deep meta - learning that integrates the representation power of deep learning into meta - learning , and enables learning to learn in the concept space .",
      "Extensive experiments on few - shot image recognition show that this new framework improves the vanilla meta - learning greatly ."
    ],
    "relations": [
      [
        "meta - learning methods:Method",
        "Used-For",
        "meta - learning:Task"
      ],
      [
        "DEML:Method",
        "Used-For",
        "meta - learning:Task"
      ],
      [
        "deeper neural network:Method",
        "Part-Of",
        "DEML:Method"
      ],
      [
        "DEML:Method",
        "SubClass-Of",
        "meta - learneragnostic framework:Method"
      ],
      [
        "DEML:Method",
        "Used-For",
        "meta - learning:Task"
      ],
      [
        "Meta - SGD:Method",
        "Part-Of",
        "DEML:Method"
      ],
      [
        "MAML:Method",
        "Part-Of",
        "DEML:Method"
      ],
      [
        "Matching Nets:Method",
        "Part-Of",
        "DEML:Method"
      ],
      [
        "Matching Nets:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "neural network:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "ReLU:Method",
        "Part-Of",
        "neural network:Method"
      ],
      [
        "Meta - SGD:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "MAML:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "neural network:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "ReLU:Method",
        "Part-Of",
        "neural network:Method"
      ],
      [
        "ImageNet - 2 0 0:Dataset",
        "Benchmark-For",
        "image recognition:Task"
      ],
      [
        "Meta - SGD:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "MAML:Method",
        "Part-Of",
        "meta - learner:Method"
      ],
      [
        "DEML:Method",
        "Compare-With",
        "Matching Nets:Method"
      ],
      [
        "DEML:Method",
        "Compare-With",
        "MAML:Method"
      ],
      [
        "DEML:Method",
        "Compare-With",
        "Meta - SGD:Method"
      ],
      [
        "deeper neural network:Method",
        "Part-Of",
        "DEML:Method"
      ],
      [
        "DEML:Method",
        "Evaluated-With",
        "MiniImagenet:Dataset"
      ],
      [
        "deep metalearning:Method",
        "Compare-With",
        "transfer learning:Method"
      ],
      [
        "Decaf+Meta - SGD:Method",
        "SubClass-Of",
        "DEML:Method"
      ],
      [
        "Decaf+kNN:Method",
        "Evaluated-With",
        "MiniImagenet:Dataset"
      ],
      [
        "Decaf+kNN:Method",
        "Evaluated-With",
        "Caltech - 2 5 6:Dataset"
      ],
      [
        "concept generator:Method",
        "Trained-With",
        "ImageNet - 2 0 0:Dataset"
      ],
      [
        "ImageNet - 2 0 0:Dataset",
        "Compare-With",
        "MiniImagenet:Dataset"
      ],
      [
        "ImageNet - 2 0 0:Dataset",
        "Compare-With",
        "Caltech - 2 5 6:Dataset"
      ],
      [
        "Decaf+kNN:Method",
        "Evaluated-With",
        "CIFAR - 1 0 0:Dataset"
      ],
      [
        "Decaf+kNN:Method",
        "Evaluated-With",
        "CUB - 2 0 0:Dataset"
      ],
      [
        "CIFAR - 1 0 0:Dataset",
        "Compare-With",
        "ImageNet - 2 0 0:Dataset"
      ],
      [
        "CUB - 2 0 0:Dataset",
        "Compare-With",
        "ImageNet - 2 0 0:Dataset"
      ],
      [
        "DEML+Meta - SGD:Method",
        "Compare-With",
        "Decaf+Meta - SGD:Method"
      ],
      [
        "DEML+Meta - SGD:Method",
        "Compare-With",
        "Decaf+Fine - Tune+Meta - SGD:Method"
      ],
      [
        "DEML+Meta - SGD:Method",
        "Evaluated-With",
        "CIFAR - 1 0 0:Dataset"
      ],
      [
        "deep learning:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "meta - learning:Method",
        "Part-Of",
        "deep meta - learning:Method"
      ],
      [
        "vanilla meta - learning:Method",
        "Used-For",
        "few - shot image recognition:Task"
      ]
    ]
  },
  {
    "doc_id": "202719492",
    "chunk_id": 1,
    "content": [
      "Most Named Entity Recognition ( NER ) systems use additional features like part - of - speech ( POS ) tags , shallow parsing , gazetteers , etc .",
      "We propose CNN based models that incorporate this semantic information and use them for NER .",
      "Our models show an improvement over the baseline BERT - BiLSTM - CRF model .",
      "We present a state - of - the - art F 1 score on Weibo dataset of 7 1 . 8 1 and show a competitive improvement of + 0. 7 2 over baseline on ResumeNER dataset .",
      "Augmenting named entity recognition ( NER ) systems with additional features like gazetteers , bag of words or character level information has been commonplace like Sang and Meulder ( 2 0 0 3 ) , Collobert et al. ( 2 0 1 1 ) and Chiu and Nichols ( 2 0 1 6 ) .",
      "Several authors Shi et al. ( 2 0 1 5 ) , Li et al. ( 2 0 1 5 ) , Sun et al. ( 2 0 1 4 ) and Meng et al. ( 2 0 1 9 ) managed to use the radical representations successfully in a wide range of natural language understanding ( NLU ) tasks .",
      "Only recently , Meng et al. ( 2 0 1 9 ) presented a complex Glyph reinforced model that concatenates glyph embeddings with BERT ( Devlin et al. 2 0 1 8 ) embeddings .",
      "We show that our models have a significant improvement over our baseline on two datasets , Chinese OntoNotes v 5 . 0 ( Pradhan and Ramshaw 2 0 1 7 ) and Weibo ( Peng and Dredze 2 0 1 5 ) .",
      "We approach the problem of incorporating Chinese glyphs as an image classification problem and present two CNNs which we call \" strided \" and \" GLYNN \" inspired from computer vision .",
      "We treat this encoding problem purely in terms of computer vision , i.e. to extract \" meaningful \" features from the image , instead of a specialized CNN that encapsulates the subtle radicals .",
      "Both CNNs are used to encode the glyphs and these encoded images are then used as an added feature for our NER system .",
      "We also present an autoencoder architecture to pretrain GLYNN and compare the results .",
      "Since the OntoNotes v 5 . 0 and Weibo datasets have a significant number of non - Chinese characters in them , we also show our model is robust by conducting a robustness test of our systems by throwing in the pictures of the non - Chinese characters as well pictures of English alphabets and show that it still beats the baseline model .",
      "The main strength of our model is as follows : • Easy to implement and train • Robust to non - Chinese languages in the dataset • Requires less amount of glyph data to train 2 Architecture of our GLYPH models BERT is the state of the art language model introduced by Devlin et al. ( 2 0 1 8) .",
      "BERT is a transformer based model which is trained on masked word prediction and next sentence prediction tasks and is trained on Wikipedia data and a large book corpus .",
      "We then combine BERT with a popular used architecture in NER , BiLSTM - CRF as in Huang , Xu and Yu ( 2 0 1 5 ) , Ma and Hovy ( 2 0 1 6 ) , Chiu and Nichols ( 2 0 1 6 ) and Zhang and Yang ( 2 0 1 8) .",
      "Our model ( figure 1 ) consists of the following parts : pretrained BERT embeddings and the ( pretrained ) CNN embeddings .",
      "We concatenate the last four layers of BERT and the CNN vectors which are our new \" character \" embeddings .",
      "We then feed these character embeddings to the BiLSTM layer which are finally decoded via a CRF layer .",
      "The CNN - LSTM - CRF is then being trained end - to - end while we keep BERT frozen .",
      "Thus we try to take advantage of BERT 's large pre - scale training and the information from Chinese glyphs encoded by our CNN 's .",
      "The selection of the CNN 's are primarily motivated by problems in computer vision .",
      "To vindicate our choice of these CNN 's we applied them to \" Fashion MNIST \" dataset and we got around 9 3 % accuracy .",
      "This consists of 4 2D convolution layers with strides 2 , filter size of 6 4 , kernel size of 3 and activation leaky ReLU .",
      "Furthermore we normalize the final output by using layer normalization as introduced by Ba , Kiros , and Hinton ( 2 0 1 6 ) .   In this subsection we describe another CNN which we call \" Glynn \" to encode the glyphs ."
    ],
    "relations": [
      [
        "NER:Task",
        "Synonym-Of",
        "Named Entity Recognition:Task"
      ],
      [
        "part - of - speech:Method",
        "Used-For",
        "Named Entity Recognition:Task"
      ],
      [
        "shallow parsing:Method",
        "Used-For",
        "Named Entity Recognition:Task"
      ],
      [
        "gazetteers:Method",
        "Used-For",
        "Named Entity Recognition:Task"
      ],
      [
        "POS:Method",
        "Synonym-Of",
        "part - of - speech:Method"
      ],
      [
        "CNN:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "NER:Task",
        "Synonym-Of",
        "named entity recognition:Task"
      ],
      [
        "character level information:Method",
        "Used-For",
        "named entity recognition:Task"
      ],
      [
        "bag of words:Method",
        "Used-For",
        "named entity recognition:Task"
      ],
      [
        "gazetteers:Method",
        "Used-For",
        "named entity recognition:Task"
      ],
      [
        "NLU:Task",
        "Synonym-Of",
        "natural language understanding:Task"
      ],
      [
        "glyph embeddings:Method",
        "Part-Of",
        "Glyph reinforced model:Method"
      ],
      [
        "BERT:Method",
        "Part-Of",
        "Glyph reinforced model:Method"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "strided:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "GLYNN:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "strided:Method",
        "SubClass-Of",
        "CNNs:Method"
      ],
      [
        "GLYNN:Method",
        "SubClass-Of",
        "CNNs:Method"
      ],
      [
        "strided:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "GLYNN:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "autoencoder:Method",
        "Used-For",
        "GLYNN:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "language model:Method"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "transformer based model:Method"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "masked word prediction:Task"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "next sentence prediction:Task"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "Wikipedia:Dataset"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "large book corpus:Dataset"
      ],
      [
        "BERT:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "CNN:Method",
        "Part-Of",
        "\" character \" embeddings:Method"
      ],
      [
        "BERT:Method",
        "Part-Of",
        "\" character \" embeddings:Method"
      ],
      [
        "CNN:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "CNN:Method",
        "Evaluated-With",
        "Fashion MNIST:Dataset"
      ],
      [
        "Glynn:Method",
        "SubClass-Of",
        "CNN:Method"
      ]
    ]
  },
  {
    "doc_id": "202719492",
    "chunk_id": 2,
    "content": [
      "The idea for this CNN comes purely from image classification tasks .",
      "Batch normalization ( Ioffe and Szegedy 2 0 1 5 ) is used to speed up training and the dropout layers are used to prevent overfitting and the maxpooling layers are used to reduce the computational complexity of the network .",
      "We use filter size of 3 2 , kernel size of 3 and padding='same ' in both the convolution layers , while we use sigmoid activation and strides=( 2 , 2 ) for our first convolution layer and ReLU activation and strides=( 1 , 1 ) for the second convolution layer .",
      "We pretrain the CNN using an autoencoder shown in figure 4 .",
      "We also employ autoencoder ( Zhou et al. 2 0 1 5 ) to pretrain the CNN .",
      "The main purpose of the autoencoder is dimensionality reduction , i.e. the autoencoder encourages the CNN to extract high level features without employing multiple convolution layers like the strided CNN .",
      "We train the autoencoder for 2 0 0 epochs and we use RM - Sprop as our optimizer .",
      "Mainstream neural approach predates to 2 0 0 3 when Hammerton ( 2 0 0 3 ) used Long Short - Term Memory for NER , achieving just above average for English F 1 scores and improvement for German NER .",
      "Hochreiter and Schmidhuber ( 1 9 9 7 ) presented Long Short - Term Memory ( LSTM ) , and it was expanded by Gers , Schmidhuber and Cummings ( 2 0 0 0 ) , and reached its current form by Graves and Schmidhuber ( 2 0 0 5 ) .",
      "LSTM is increasing in its use with NER problems over the past 2 decades .",
      "Recent works in NER follows this approach , mainly using BiLSTM - CRF architecture .",
      "Bi - LSTM - CRF architecture was first proposed by Huang , Xu and Yu ( 2 0 1 5 ) , and has been widely studied and augmented .",
      "Chiu and Nichols ( 2 0 1 6 ) and Ma and Hovy ( 2 0 1 6 ) augmented LSTM - CRF architecture with character - level convolutional neural network to add an additional features to the architecture .",
      "Instead of applying convolutional neural network to the text , we apply it to the glyphs to augment our Bi - LSTM - CRF .",
      "Recently , transfer learning architectures has shown significant improvement in various natural language processing tasks such as question answering , natural language understanding , machine translation and natural language inference .",
      "Devlin et al. ( 2 0 1 8) uses stacked bi - directional transformer layers called BERT that is trained on masked word prediction and next sentence prediction tasks .",
      "By employing a task - specific final output layer , BERT can be tuned to many different natural language processing tasks .",
      "In this work , we apply BERT to NER and use BiLSTM - CRF as the output layer of BERT - CNN .",
      "Even though there are over 2 0 , 0 0 0 CJK characters , we only have about a hundred of out - of - vocabulary characters in OntoNotes v 5 . 0 and Weibo .",
      "Another major difference between our approach and all the aforementioned authors in the introduction is that our CNN 's are agnostic to the subtleties of Chinese characters and we treat this encoding problem with computer vision ideas and extract \" meaningful \" features from the image , instead of having a specialized CNN that encapsulates the subtle radicals .",
      "Both Su and Lee ( 2 0 1 7 ) and Meng et al ( 2 0 1 9 ) use autoencoders to pretrain their CNN .",
      "Su and Lee ( 2 0 1 7 ) pretrain the CNN by freezing some layers while Meng et al ( 2 0 1 9 ) pretrain the CNN with the objective of recovering an \" image i d \" while we follow the approach of jointly training all layers of the CNN ( Zhou et al. 2 0 1 5 ) with the global objective of reconstructing the image .",
      "Another difference between our architecture and the GLYCE model ( Meng et al. 2 0 1 9 ) is that we use a BiLSTM instead of a transformer to encode the BERT + Glyph Embeddings .",
      "We ran 1 0 trials for 3 0 epochs for each of Glynn CNN ( default and higher dropout ) and strided CNN on Chinese OntoNotes v 5 . 0 .",
      "We report our scores in Table 3 Using the F 1 scores obtained by the Glynn CNN ( dropout . 5 ) and the strided CNN , we perform the 2sample t - test and obtain a p - value of < . 0 0 1 ."
    ],
    "relations": [
      [
        "CNN:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "sigmoid activation:Method",
        "Part-Of",
        "convolution layer:Method"
      ],
      [
        "ReLU:Method",
        "Part-Of",
        "convolution layer:Method"
      ],
      [
        "autoencoder:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "autoencoder:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "autoencoder:Method",
        "Used-For",
        "dimensionality reduction:Task"
      ],
      [
        "autoencoder:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "convolution layers:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "RM - Sprop:Method",
        "Part-Of",
        "autoencoder:Method"
      ],
      [
        "Long Short - Term Memory:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "Long Short - Term Memory:Method",
        "Used-For",
        "German NER:Task"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "Long Short - Term Memory:Method"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "BiLSTM - CRF:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "character - level convolutional neural network:Method",
        "Part-Of",
        "LSTM - CRF:Method"
      ],
      [
        "convolutional neural network:Method",
        "Used-For",
        "Bi - LSTM - CRF:Method"
      ],
      [
        "transfer learning architectures:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "question answering:Task",
        "SubTask-Of",
        "natural language processing:Task"
      ],
      [
        "natural language understanding:Task",
        "SubTask-Of",
        "natural language processing:Task"
      ],
      [
        "machine translation:Task",
        "SubTask-Of",
        "natural language processing:Task"
      ],
      [
        "natural language inference:Task",
        "SubTask-Of",
        "natural language processing:Task"
      ],
      [
        "transfer learning architectures:Method",
        "Used-For",
        "question answering:Task"
      ],
      [
        "transfer learning architectures:Method",
        "Used-For",
        "natural language understanding:Task"
      ],
      [
        "transfer learning architectures:Method",
        "Used-For",
        "machine translation:Task"
      ],
      [
        "transfer learning architectures:Method",
        "Used-For",
        "natural language inference:Task"
      ],
      [
        "BERT:Method",
        "SubClass-Of",
        "stacked bi - directional transformer layers:Method"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "masked word prediction:Task"
      ],
      [
        "BERT:Method",
        "Trained-With",
        "next sentence prediction:Task"
      ],
      [
        "task - specific final output layer:Method",
        "Part-Of",
        "BERT:Method"
      ],
      [
        "BERT:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "BERT:Method",
        "Used-For",
        "NER:Task"
      ],
      [
        "BiLSTM - CRF:Method",
        "Part-Of",
        "BERT - CNN:Method"
      ],
      [
        "autoencoders:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "transformer:Method",
        "Part-Of",
        "GLYCE:Method"
      ]
    ]
  },
  {
    "doc_id": "202719492",
    "chunk_id": 3,
    "content": [
      "Thus we see that strided CNN is a significant improvement over both the BERT baseline and the GLYNN .",
      "Since the Weibo dataset is smaller and is noisier than OntoNotes v 5 . 0 , we ran vanilla BERT - BiLSTM - CRF 2 0 times and 4 0 times respectively on Weibo NAM and the Weibo dataset to establish a baseline .",
      "We ran 2 0 trials for 3 0 epochs for each of Glynn CNN ( default and higher dropout ) and strided CNN on Weibo .",
      "We also calculate the p - value between our CNN 's and the baseline BERT to see if our results are statistically significant .",
      "Table 4 shows that Weibo experiment results from both CNNs are statistically significant than the vanilla BERT .",
      "However pvalue between strided CNN and GLYNN is . 7 1 , which shows that their performance on average is statistically the same .",
      "Even though strided CNN did not show any improvement , we made significant gains with the GLYNN CNN over the baseline and set a new SOTA F 1 score .",
      "Avg However the difference in performance between GLYNN ( with . 5 dropout ) and strided is statistically insignificant as the p - value is . 7 6 8 .",
      "We used Adam optimizer on OntoNotes v 5 . 0 for 5 trials with the learning rates . 0 0 1 , . 0 0 0 5 and . 0 0 0 1 with early stopping if the loss did not decrease over 5 epochs .",
      "But in general , we found Adam performs poorly compared to Adafactor .",
      "Early stopping with all the above learning rates with both Adam and Adafactor on Weibo produced erratic results with extremely high standard deviations .",
      "We also used dropouts of . 5 on each the dropout layers of the Glynn CNN and ran multiple trials .",
      "Table 3 summarizes our results on OntoNotes v 5 . 0 after 1 0 trials and we also compute the p - value to test the difference in the average performance of GLYNN .",
      "We also ran 2 0 trials on Weibo with this new dropout .",
      "We did 2 sample t - test between GLYNN with . 5 dropouts and the strided CNN ( resp .",
      "GLYNN and GLYNN with . 5 dropouts ) and we found the p - value to be . 8 4 ( resp . 8 3 ) .",
      "So the two CNN ( with or without higher dropout ) behave pretty much the same .",
      "Losses tend to increase after 3 0 epochs and so the models start doing worse at 4 0 epochs . 3 0 is also an optimum choice for the full Weibo dataset and OntoNotes .",
      "Figure 6 shows the relation between training epochs of GLYNN and the test and dev F 1 scores on Weibo NAM and OntoNotes .",
      "How important is the autoencoder : We ran multiple tests ( 2 0 for Weibo NAM , 1 0 for OntoNotes v 5 . 0 ) with varying learning rates and training epochs .",
      "For example , GLYNN with default hyperparameters without the autoencoder got an average test score of 7 0 . 9 8 (± 1 . 4 7 ) on Weibo NAM .",
      "Using two very different CNNs with and without an autoencoder , we have shown gains over the baseline system on the three most commonly used datasets and achieve state of the art F 1 score on the Weibo dataset .",
      "We are excited by the future of glyphs in NLP and we would like use glyphs for other NLP tasks ."
    ],
    "relations": [
      [
        "BERT + Glyph Embeddings:Method",
        "Part-Of",
        "BiLSTM:Method"
      ],
      [
        "BERT + Glyph Embeddings:Method",
        "Part-Of",
        "transformer:Method"
      ],
      [
        "dropout:Method",
        "Part-Of",
        "Glynn CNN:Method"
      ],
      [
        "Glynn CNN:Method",
        "Trained-With",
        "Chinese OntoNotes v 5 . 0:Dataset"
      ],
      [
        "strided CNN:Method",
        "Trained-With",
        "Chinese OntoNotes v 5 . 0:Dataset"
      ],
      [
        "dropout:Method",
        "Part-Of",
        "Glynn CNN:Method"
      ],
      [
        "strided CNN:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "strided CNN:Method",
        "Compare-With",
        "GLYNN:Method"
      ],
      [
        "Weibo:Dataset",
        "Compare-With",
        "OntoNotes v 5 . 0:Dataset"
      ],
      [
        "BERT - BiLSTM - CRF:Method",
        "Trained-With",
        "Weibo NAM:Dataset"
      ],
      [
        "BERT - BiLSTM - CRF:Method",
        "Trained-With",
        "Weibo:Dataset"
      ],
      [
        "dropout:Method",
        "Part-Of",
        "Glynn CNN:Method"
      ],
      [
        "Glynn CNN:Method",
        "Trained-With",
        "Weibo:Dataset"
      ],
      [
        "strided CNN:Method",
        "Trained-With",
        "Weibo:Dataset"
      ],
      [
        "CNN:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "CNNs:Method",
        "Evaluated-With",
        "Weibo:Dataset"
      ],
      [
        "BERT:Method",
        "Evaluated-With",
        "Weibo:Dataset"
      ],
      [
        "CNNs:Method",
        "Compare-With",
        "BERT:Method"
      ],
      [
        "strided CNN:Method",
        "Compare-With",
        "GLYNN:Method"
      ],
      [
        "strided CNN:Method",
        "Compare-With",
        "GLYNN CNN:Method"
      ],
      [
        "dropout:Method",
        "Part-Of",
        "GLYNN:Method"
      ],
      [
        "Adam:Method",
        "Compare-With",
        "Adafactor:Method"
      ],
      [
        "dropout:Method",
        "Part-Of",
        "Glynn CNN:Method"
      ],
      [
        "GLYNN:Method",
        "Evaluated-With",
        "OntoNotes v 5 . 0:Dataset"
      ],
      [
        "dropouts:Method",
        "Part-Of",
        "GLYNN:Method"
      ],
      [
        "dropouts:Method",
        "Part-Of",
        "GLYNN:Method"
      ],
      [
        "dropout:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "GLYNN:Method",
        "Evaluated-With",
        "Weibo NAM:Dataset"
      ],
      [
        "GLYNN:Method",
        "Evaluated-With",
        "OntoNotes:Dataset"
      ],
      [
        "autoencoder:Method",
        "Evaluated-With",
        "Weibo NAM:Dataset"
      ],
      [
        "autoencoder:Method",
        "Evaluated-With",
        "OntoNotes v 5 . 0:Dataset"
      ],
      [
        "GLYNN:Method",
        "Evaluated-With",
        "Weibo NAM:Dataset"
      ],
      [
        "CNNs:Method",
        "Evaluated-With",
        "Weibo:Dataset"
      ]
    ]
  },
  {
    "doc_id": "53109398",
    "chunk_id": 1,
    "content": [
      "Face presentation attack detection ( PAD ) has become a thorny problem for biometric systems and numerous countermeasures have been proposed to address it .",
      "Inspired by the generator of generative adversarial network ( GAN ) , the proposed network consists of a space generator and a feature extractor .",
      "Extensive experiments on two standard face PAD databases , i.e. , Relay - Attack and OULU - NPU , indicate that our proposed color - liked space analysis based countermeasure significantly outperforms the state - of - the - art methods and show excellent generalization capability . access that person 's social network data including good quality images that can be used for face PAD .",
      "In the last decade , many face PAD methods have been proposed to detect fake faces [ 3 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .",
      "In the last decade , many face PAD methods have been proposed to detect fake faces [ 3 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .",
      "Inspired by generative adversarial network ( GAN ) [ 2 1 ] , a color - liked space generator is constructed to map existing color spaces .",
      "Finally , the extracted features are fed into a Support Vector Machine ( SVM ) [ 2 2 ] classifier to detect face presentation attack .",
      "We train and test our proposed method on two public available databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "Therefore , multi - scale local binary pattern ( LBP ) fe tures [ 1 2 ] were extracted to describe edge texture difference In another work , Chingovska et al. [ 1 0 ] used different kin of LBP features to improve detection accuracy .",
      "To capture t color differences in luminance and chrominance , Boulkenaf et al. [ 1 3 ] , [ 2 9 ] proposed a method by computing LBP featur from different color spaces and concatenating all LBP in a single feature vector .",
      "With the recent advanc of deep learning in computer vision [ 3 3 ] , [ 3 4 ] , deep textur have also been applied in face PAD .",
      "Yang et al. [ 3 5 ] propos an end - to - end convolutional neural network ( CNN ) model f face PAD .",
      "Instead of using fully - connected layers , Li at a [ 3 6 ] , [ 3 7 ] extracted hand - crafted features from convolution Fig. 2 .",
      "Inspired by generative adversarial network ( GAN ) [ 2 1 ] , a color - liked space generator is constructed to map existing color spaces .",
      "Finally , the extracted features are fed into a Support Vector Machine ( SVM ) [ 2 2 ] classifier to detect face presentation attack .",
      "We train and test our proposed method on two public available databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "Unlike traditional mechanism , the proposed pointsto - set mechanism can guarantee a stable decline in triplet loss . 3 ) Extensive experimental analysis is conducted on the two latest and challenging face PAD databases using their pre - defined publicly well - defined experimental evaluation protocols ensuring the reproducibility of the results and a fair comparison with the state - of - the - art methods .",
      "The remainder of the paper is organized as follows : Section II reviews the existing state - of - the - art methods of face PAD and briefly provides the development of triplet network .",
      "A. State - of - the - art of Face PAD In the past few years , face PAD has received great attention and many detection approaches have been developed [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .",
      "Based on different clues , these countermeasures can be further categorized into texture analysis [ 1 0 ] , [ 1 2 ] , [ 1 3 ] , motion analysis [ 1 4 ] , [ 1 5 ] , [ 2 5 ] , image quality analysis [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] , and hardware based methods [ 1 1 ] , [ 1 9 ] , [ 2 0 ] , [ 2 6 ] , [ 2 7 ] , [ 2 8 ] . 1 ) Texture analysis based methods : Due to the limitations of printers and display devices , there are substantial differences in color distribution and edge texture between real and fake faces .",
      "Therefore , multi - scale local binary pattern ( LBP ) features [ 1 2 ] were extracted to describe edge texture differences .",
      "To capture the color differences in luminance and chrominance , Boulkenafet et al. [ 1 3 ] , [ 2 9 ] proposed a method by computing LBP features from different color spaces and concatenating all LBP into a single feature vector .",
      "Instead of using LBP features , Akshay et al. [ 3 1 ] extracted Haralick [ 3 2 ] texture features for face PAD .",
      "With the recent advances of deep learning in computer vision [ 3 3 ] , [ 3 4 ] , deep textures have also been applied in face PAD .",
      "Yang et al. [ 3 5 ] proposed an end - to - end convolutional neural network ( CNN ) model for face PAD .",
      "For capturing texture variations , Xu et al. [ 3 8 ] proposed a long short memory network ( LSTM ) and Li et al. [ 3 9 ] proposed a 3D CNN to detect face presentation attacks , respectively .",
      "More recently , a LBP network [ 4 0 ] has been designed for face PAD which simulates the idea of basic LBP .",
      "However , with the popularity of high - definition screens , their detection performances tend to decrease drastically . 2 ) Motion analysis based methods : Apart from texture analysis , motion also plays an important role in face PAD .",
      "For instance , based on the fact that involuntary eyes blinking often occurs in the interval of 2 to 4 seconds [ 4 1 ] , an undirected conditional random field framework [ 1 4 ] was proposed to detect printed photo attacks .",
      "In [ 4 2 ] and [ 4 3 ] , LBP - TOP [ 4 4 ] and LDP - TOP [ 4 5 ] features were extracted to describe these variations , respectively .",
      "In another work , Santosh et al. [ 4 6 ] used dynamic mode decomposition ( DMD ) to capture the dynamics of movements .",
      "Tan et al. [ 1 5 ] used Difference - of - Gaussians ( DoG ) to extract the differences in motion deformation patterns between real and fake faces .",
      "In the past few years , many computer version and multimedia analysis tasks such as face verification and person re - identification ( Re - ID ) have explored the effectiveness of triplet network ."
    ],
    "relations": [
      [
        "PAD:Task",
        "Synonym-Of",
        "Face presentation attack detection:Task"
      ],
      [
        "GAN:Method",
        "Synonym-Of",
        "generative adversarial network:Method"
      ],
      [
        "Relay - Attack:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "OULU - NPU:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "face PAD methods:Method",
        "Used-For",
        "detect fake faces:Task"
      ],
      [
        "face PAD methods:Method",
        "Used-For",
        "detect fake faces:Task"
      ],
      [
        "GAN:Method",
        "Synonym-Of",
        "generative adversarial network:Method"
      ],
      [
        "SVM:Method",
        "Synonym-Of",
        "Support Vector Machine:Method"
      ],
      [
        "Support Vector Machine:Method",
        "Used-For",
        "detect face presentation attack:Task"
      ],
      [
        "LBP:Method",
        "Synonym-Of",
        "multi - scale local binary pattern:Method"
      ],
      [
        "LBP:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "CNN:Method",
        "Synonym-Of",
        "convolutional neural network:Method"
      ],
      [
        "convolutional neural network:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "GAN:Method",
        "Synonym-Of",
        "generative adversarial network:Method"
      ],
      [
        "SVM:Method",
        "Synonym-Of",
        "Support Vector Machine:Method"
      ],
      [
        "Support Vector Machine:Method",
        "Used-For",
        "detect face presentation attack:Task"
      ],
      [
        "pointsto - set mechanism:Method",
        "Part-Of",
        "triplet loss:Method"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "LBP:Method",
        "Synonym-Of",
        "multi - scale local binary pattern:Method"
      ],
      [
        "LBP:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "CNN:Method",
        "Synonym-Of",
        "convolutional neural network:Method"
      ],
      [
        "convolutional neural network:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long short memory network:Method"
      ],
      [
        "3D CNN:Method",
        "Used-For",
        "detect face presentation attacks:Task"
      ],
      [
        "LBP:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "Motion analysis based methods:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "undirected conditional random field:Method",
        "Used-For",
        "detect printed photo attacks:Task"
      ],
      [
        "DMD:Method",
        "Synonym-Of",
        "dynamic mode decomposition:Method"
      ],
      [
        "DoG:Method",
        "Synonym-Of",
        "Difference - of - Gaussians:Method"
      ],
      [
        "person re - identification:Task",
        "SubTask-Of",
        "computer version:Task"
      ],
      [
        "face verification:Task",
        "SubTask-Of",
        "computer version:Task"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "computer version:Task"
      ],
      [
        "face verification:Task",
        "SubTask-Of",
        "multimedia analysis tasks:Task"
      ],
      [
        "person re - identification:Task",
        "SubTask-Of",
        "multimedia analysis tasks:Task"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "multimedia analysis tasks:Task"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "face verification:Task"
      ],
      [
        "Re - ID:Task",
        "Synonym-Of",
        "person re - identification:Task"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "person re - identification:Task"
      ],
      [
        "triplet loss:Method",
        "Part-Of",
        "deep neural network:Method"
      ],
      [
        "deep neural network:Method",
        "Used-For",
        "person Re - ID:Task"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "face verification:Task"
      ],
      [
        "leaky rectified linear unit:Method",
        "Part-Of",
        "convolutional layer:Method"
      ],
      [
        "lReLU:Method",
        "Synonym-Of",
        "leaky rectified linear unit:Method"
      ],
      [
        "BN:Method",
        "Synonym-Of",
        "batch normalization:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Layer Conv:Method"
      ],
      [
        "Residual:Method",
        "Part-Of",
        "Layer Conv:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ]
    ]
  },
  {
    "doc_id": "53109398",
    "chunk_id": 2,
    "content": [
      "For instance , Ding et al. [ 5 7 ] used the triplet loss to learn a deep neural network for person Re - ID .",
      "In [ 5 9 ] , Swami et al. proposed a triplet network for face verification and got promising results .",
      "More specifically , given a color video frame f ( x , y ) , a convolutional layer with filter size of 3 × 3 × 3 × 6 4 and a leaky rectified linear unit ( lReLU ) layer are firstly used to process f ( x , y ) .",
      "Moreover , the batch normalization ( BN ) layer [ 6 3 ] is introduced after convolutional layer .",
      "The hierarchic link of color - liked space generator is summarized in Table I .    Layer Conv , lReLU w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum Conv space into a feature space by a differentiable function ∅. After feature extraction , the generated f ′ ( x , y ) can be written , where W 2 represents the parameters of feature extractor .",
      "More specifically , given a color video frame f ( x , y ) , a convolutional layer with filter size of 3 × 3 × 3 × 6 4 and a leaky rectified linear unit ( lReLU ) layer are firstly used to process f ( x , y ) .",
      "Moreover , the batch normalization ( BN ) layer [ 6 3 ] is introduced after convolutional layer .",
      "Rather than computing triplet loss in generated color - liked space ,   Layer Residual : Conv , BN , lReLU , Conv , BN , Sum w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum w × h × 6 4 Residual : Conv , BN , lReLU , Conv , BN , Sum Conv , lReLU w × h × 3 Conv our proposed method first maps the generated color - liked space into a feature space by a differentiable function ∅. After feature extraction , the generated f ( x , y ) can be written as ∅(f ( x , y ) , W 2 ) , where W 2 represents the parameters of feature extractor .",
      "For the feature mapping ∅ , we exploit the pre - trained implementation of the popular VGG - 1 9 [ 2 4 ] network , which consists of stacked convolutional layers coupled with pooling operations to gradually decrease the spatial dimension of the image and to extract higher - level features in higher layers .",
      "In training stage , the stochastic gradient descent ( SGD ) algorithm [ 6 6 ] is used to learn the network parameters .",
      "In training stage , the stochastic gradient descent ( SGD ) algorithm [ 6 6 ] is used to learn the network parameters .",
      "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( · ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .",
      "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .",
      "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0   Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .",
      "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( · ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .",
      "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .",
      "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0 1 and liblinear with the version 1. 9 6 2 [ 6 8 ] , respectively .",
      "Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .",
      "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( · ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .",
      "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .",
      "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0 1 and liblinear with the version 1. 9 6 2 [ 6 8 ] , respectively .",
      "Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "But for the parameters in color - liked space generator , they are initialized based on [ 6 7 ] as illustrated in Eq. 7 , which can ensure that all convolutional layers in the network initially have the approximately same output distribution and empirically improve the rate of convergence . where rand ( · ) samples from a zero mean , unit standard derivation gaussian function , and n l is the channel number of inputs in convolutional layer .",
      "In training stage , the momentum and weight decay of SGD are set to 0. 9 and 0.0 0 0 5 , respectively .",
      "In our paper , we realize the proposed color - liked space generator and SVM based on the toolbox of MatConvNet with the version 1. 0 - beta 2 0 1 and liblinear with the version 1. 9 6 2 [ 6 8 ] , respectively .   Our proposed face PAD algorithm is validated on two publicly available face PAD databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .",
      "Fig. 6 shows some examples of real and fake faces . 2 ) OULU - NPU : The OULU - NPU Database 4 [ 2 3 ] consists of 4 9 5 0 real access and attack videos and attempts 5 5 clients .",
      "More specifically , for ReplayAttack database , γ should be set to 0. 1 with the averaged ACER= 0 . 5 % ; but for OULU - NPU database , γ should be set to 1 with the averaged ACER= 5 . 4 % .",
      "While considering the averaged ACER of both Replay - Attack and OULU - NPU databases , we set the γ to 0. 5 in our proposed method .",
      "More specifically , for ReplayAttack database , γ should be set to 0. 1 with the averaged ACER= 0 . 5 % ; but for OULU - NPU database , γ should be set to 1 with the averaged ACER= 5 . 4 % .",
      "While considering the averaged ACER of both Replay - Attack and OULU - NPU databases , we set the γ to 0. 5 in our proposed method ."
    ],
    "relations": [
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "leaky rectified linear unit:Method",
        "Part-Of",
        "convolutional layer:Method"
      ],
      [
        "lReLU:Method",
        "Synonym-Of",
        "leaky rectified linear unit:Method"
      ],
      [
        "BN:Method",
        "Synonym-Of",
        "batch normalization:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "BN:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "lReLU:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "Conv:Method",
        "Part-Of",
        "Residual:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "VGG - 1 9:Method"
      ],
      [
        "SGD:Method",
        "Synonym-Of",
        "stochastic gradient descent:Method"
      ],
      [
        "SGD:Method",
        "Synonym-Of",
        "stochastic gradient descent:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "momentum:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "weight decay:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "face PAD algorithm:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "Replay - Attack:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "OULU - NPU:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "Replay - Attack:Dataset"
      ]
    ]
  },
  {
    "doc_id": "53109398",
    "chunk_id": 3,
    "content": [
      "The reason may lie in that the distribution of real and fake faces in Replay - Attack database is simpler than that of OULU - NPU database , which limits the superiority of P 2 C .",
      "For a clearer analysis of the generated color - liked space , the sample distributions of Replay - Attack and OULU - NPU databases are described in Fig. 1 0 and Fig. 1 1 , respectively .",
      "By comparing Fig. 1 0 and Fig. 1 1 , we can find that the ReplayAttack database is easier to be addressed than the OULU - NPU database , which is consistent with the results shown in Table   II .",
      "For instance , in RGB color space , the averaged APCER , BPCER and ACER of Replay - Attack and OULU - NPU are 6. 1 % , 3. 1 % , 4. 6 % and 1 7 . 7 % , 1 1 . 6 % 1 4 . 7 % , respectively .",
      "For hand - crafted feature , the LBP extracted from our learned color - liked space takes on better superiority compared with [ 2 9 ] that extracts LBP features from existing color spaces .",
      "More specifically , the colorliked space generator is trained and tuned on one of the + means the actual value is greater than the value . † was retested on OULU - NPU database . ‡ was retested on Replay - Attack and OULU - NPU databases . databases and then tested on another database .",
      "When the generator is trained on OULU - NPU and tested on Replay - Attack , we notice that the averaged ACER of VGG features is 4 0 . 1 % .",
      "When the generator is trained on ReplayAttack and tested on OULU - NPU , the averaged metric is 4 5 . 1 % .",
      "From these results , we conclude that the generator trained on Replay - Attack is not able to be generalized as good as trained on OULU - NPU .",
      "It is caused that the OULU - NPU database contains more variations in the collecting environment ( e.g. , light and camera quality ) compared to ReplayAttack .",
      "Based on triplet training and perceptual similarity measure mechanisms , a new ( a ) Replay - Attack database in the generated color - liked space . ( b ) OULU - NPU database in the generated color - liked space . the detection results obtained based on these two different combination mechanisms .",
      "The reason may lie in that the distribution of real and fake faces in Replay - Attack database is simpler than that of OULU - NPU database , which limits the superiority of P 2 C .",
      "For a clearer analysis of the generated color - liked space , the sample distributions of Replay - Attack and OULU - NPU databases are described in Fig. 1 0 and Fig. 1 1 , respectively .",
      "By comparing Fig. 1 0 and Fig. 1 1 , we can find that the ReplayAttack database is easier to be addressed than the OULU - NPU database , which is consistent with the results shown in Table   II .",
      "For instance , in RGB color space , the averaged APCER , BPCER and ACER of Replay - Attack and OULU - NPU are 6. 1 % , 3. 1 % , 4. 6 % and 1 7 . 7 % , 1 1 . 6 % 1 4 . 7 % , respectively .",
      "For hand - crafted feature , the LBP extracted from our learned color - liked space takes on better superiority compared with [ 2 9 ] that extracts LBP features from existing color spaces .",
      "Extensive experiments on two latest and challenging presentation attack databases ( the ReplayAttack and OULU - NPU ) showed excellent results .",
      "On OULU - NPU database , the proposed color - liked space based method outperformed the baseline , while very competitive results were achieved on Replay - Attack database .",
      "Overall , from the results of Replay - Attack and OULU - NPU databases , we find that external - environment factors ( e.g. light and camera quality ) limit the effectiveness of our proposed detection method .",
      "Inspired by generative adversarial network ( GAN ) [ 2 1 ] , a color - liked space generator is constructed to map existing color spaces .",
      "Finally , the extracted features are fed into a Support Vector Machine ( SVM ) [ 2 2 ] classifier to detect face presentation attack .",
      "We train and test our proposed method on two public available databases : Replay - Attack [ 1 0 ] and OULU - NPU [ 2 3 ] .",
      "Unlike traditional mechanism , the proposed pointsto - set mechanism can guarantee a stable decline in triplet loss . 3 ) Extensive experimental analysis is conducted on the two latest and challenging face PAD databases using their pre - defined publicly well - defined experimental evaluation protocols ensuring the reproducibility of the results and a fair comparison with the state - of - the - art methods .",
      "The remainder of the paper is organized as follows : Section II reviews the existing state - of - the - art methods of face PAD and briefly provides the development of triplet network .",
      "A. State - of - the - art of Face PAD In the past few years , face PAD has received great attention and many detection approaches have been developed [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] .",
      "Based on different clues , these countermeasures can be further categorized into texture analysis [ 1 0 ] , [ 1 2 ] , [ 1 3 ] , motion analysis [ 1 4 ] , [ 1 5 ] , [ 2 5 ] , image quality analysis [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] , and hardware based methods [ 1 1 ] , [ 1 9 ] , [ 2 0 ] , [ 2 6 ] , [ 2 7 ] , [ 2 8 ] . 1 ) Texture analysis based methods : Due to the limitations of printers and display devices , there are substantial differences in color distribution and edge texture between real and fake faces .",
      "Therefore , multi - scale local binary pattern ( LBP ) features [ 1 2 ] were extracted to describe edge texture differences .",
      "To capture the color differences in luminance and chrominance , Boulkenafet et al. [ 1 3 ] , [ 2 9 ] proposed a method by computing LBP features from different color spaces and concatenating all LBP into a single feature vector .",
      "Instead of using LBP features , Akshay et al. [ 3 1 ] extracted Haralick [ 3 2 ] texture features for face PAD .",
      "With the recent advances of deep learning in computer vision [ 3 3 ] , [ 3 4 ] , deep textures have also been applied in face PAD .",
      "Yang et al. [ 3 5 ] proposed an end - to - end convolutional neural network ( CNN ) model for face PAD .",
      "For capturing texture variations , Xu et al. [ 3 8 ] proposed a long short memory network ( LSTM ) and Li et al. [ 3 9 ] proposed a 3D CNN to detect face presentation attacks , respectively ."
    ],
    "relations": [
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "OULU - NPU:Dataset"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "weight decay:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "momentum:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "face PAD algorithm:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "Replay - Attack:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "OULU - NPU:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "Replay - Attack:Dataset"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "OULU - NPU:Dataset"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "momentum:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "weight decay:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "face PAD algorithm:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "OULU - NPU:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "Replay - Attack:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "Replay - Attack:Dataset"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "OULU - NPU:Dataset"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "weight decay:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "momentum:Method",
        "Part-Of",
        "SGD:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "color - liked space generator:Method"
      ],
      [
        "MatConvNet:Method",
        "Part-Of",
        "SVM:Method"
      ],
      [
        "face PAD algorithm:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "Replay - Attack:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "OULU - NPU:Dataset",
        "Benchmark-For",
        "face PAD:Task"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "Replay - Attack:Dataset"
      ],
      [
        "face PAD algorithm:Method",
        "Evaluated-With",
        "OULU - NPU:Dataset"
      ],
      [
        "generator:Method",
        "Trained-With",
        "OULU - NPU:Dataset"
      ],
      [
        "generator:Method",
        "Evaluated-With",
        "Replay - Attack:Dataset"
      ],
      [
        "generator:Method",
        "Trained-With",
        "ReplayAttack:Dataset"
      ],
      [
        "generator:Method",
        "Evaluated-With",
        "OULU - NPU:Dataset"
      ],
      [
        "generator:Method",
        "Trained-With",
        "Replay - Attack:Dataset"
      ],
      [
        "generator:Method",
        "Trained-With",
        "OULU - NPU:Dataset"
      ],
      [
        "OULU - NPU:Dataset",
        "Compare-With",
        "ReplayAttack:Dataset"
      ],
      [
        "Replay - Attack:Dataset",
        "Compare-With",
        "OULU - NPU:Dataset"
      ],
      [
        "ReplayAttack:Dataset",
        "Compare-With",
        "OULU - NPU:Dataset"
      ],
      [
        "GAN:Method",
        "Synonym-Of",
        "generative adversarial network:Method"
      ],
      [
        "SVM:Method",
        "Synonym-Of",
        "Support Vector Machine:Method"
      ],
      [
        "pointsto - set mechanism:Method",
        "Part-Of",
        "triplet loss:Method"
      ],
      [
        "triplet network:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "LBP:Method",
        "Synonym-Of",
        "multi - scale local binary pattern:Method"
      ],
      [
        "LBP:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "CNN:Method",
        "Synonym-Of",
        "convolutional neural network:Method"
      ],
      [
        "convolutional neural network:Method",
        "Used-For",
        "face PAD:Task"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long short memory network:Method"
      ],
      [
        "3D CNN:Method",
        "Used-For",
        "detect face presentation attacks:Task"
      ],
      [
        "long short memory network:Method",
        "Used-For",
        "detect face presentation attacks:Task"
      ]
    ]
  },
  {
    "doc_id": "210702798",
    "chunk_id": 1,
    "content": [
      "Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding , medical image analysis , robotic perception , video surveillance , augmented reality , and image compression , among many others .",
      "In this survey , we provide a comprehensive review of the literature at the time of this writing , covering a broad spectrum of pioneering works for semantic and instance - level segmentation , including fully convolutional pixel - labeling networks , encoder - decoder architectures , multi - scale and pyramid based approaches , recurrent networks , visual attention models , and generative models in adversarial settings .",
      "Segmentation plays a central role in a broad range of applications [ 2 ] , including medical image analysis ( e.g. , tumor boundary extraction and measurement of tissue volumes ) , autonomous vehicles ( e.g. , navigable surface and pedestrian detection ) , video surveillance , and augmented reality to count a few .",
      "Numerous image segmentation algorithms have been developed in the literature , from the earliest methods , such as thresholding [ 3 ] , histogram - based bundling , regiongrowing [ 4 ] , k - means clustering [ 5 ] , watersheds [ 6 ] , to more advanced algorithms such as active contours [ 7 ] , graph cuts [ 8 ] , conditional and Markov random fields [ 9 ] , and sparsitybased [ 1 0 ] - [ 1 1 ] methods .",
      "For example , Figure 1 presents sample image segmentation outputs of a prominent deep learning model , DeepLabv 3 [ 1 2 ] .",
      "Image segmentation can be formulated as a classification problem of pixels with semantic labels ( semantic segmentation ) or partitioning of individual objects ( instance segmentation ) .",
      "Semantic segmentation performs pixel - level labeling with a set of object categories ( e.g. , human , car , tree , sky ) for all image pixels , thus it is generally a harder undertaking than image classification , which predicts a • S. Minaee is with Expedia Inc , and New York University . single label for the entire image .",
      "Instance segmentation extends semantic segmentation scope further by detecting and delineating each object of interest in the image ( e.g. , partitioning of individual persons ) .",
      "Our survey covers the most recent literature in image segmentation and discusses more than a hundred deep learning - based segmentation methods proposed until 2 0 1 9 .",
      "CV ] 1 8 Jan 2 0 2 0 1 ) Fully convolutional networks 2 ) Convolutional models with graphical models 3 ) Encoder - decoder based models 4 ) Multi - scale and pyramid network based models 5 ) R - CNN based models ( for instance segmentation ) 6 ) Dilated convolutional models and DeepLab family 7 ) Recurrent neural network based models 8) Attention - based models 9 ) Generative models and adversarial training 1 0 ) Convolutional models with active contour models 1 1 ) Other models Some the key contributions of this survey paper can be summarized as follows : • This survey covers the contemporary literature with respect to segmentation problem , and overviews more than 1 0 0 segmentation algorithms proposed till 2 0 1 9 , grouped into 1 0 categories .",
      "• We provide a comprehensive review and an insightful analysis of different aspects of segmentation algorithms using deep learning , including the training data , the choice of network architectures , loss functions , training strategies , and their key contributions . • We provide an overview of around 2 0 popular image segmentation datasets , grouped into 2D , 2. 5 D ( RGB - D ) , and 3D images . • We provide a comparative summary of the properties and performance of the reviewed methods for segmentation purposes , on popular benchmarks . • We provide several challenges and potential future directions for deep learning - based image segmentation .",
      "This section provides an overview of some of the most prominent deep learning architectures used by the computer vision community , including convolutional neural networks ( CNNs ) [ 1 3 ] , recurrent neural networks ( RNNs ) and long short term memory ( LSTM ) [ 1 4 ] , encoder - decoders [ 1 5 ] , and generative adversarial networks ( GANs ) [ 1 6 ] .",
      "With the popularity of deep learning in recent years , several other deep neural architectures have been proposed , such as transformers , capsule networks , gated recurrent units , spatial transformer networks , etc . , which will not be covered here .",
      "CNNs are among the most successful and widely used architectures in the deep learning community , especially for computer vision tasks .",
      "Subsequently , Waibel et al. [ 1 8 ] introduced CNNs with weights shared among temporal receptive fields and backpropagation training for phoneme recognition , and LeCun et al. [ 1 3 ] developed a CNN architecture for document recognition ( Figure 2 ) .",
      "Some of the most well - known CNN architectures include : AlexNet [ 1 9 ] , VGGNet [ 2 0 ] , ResNet [ 2 1 ] , GoogLeNet [ 2 2 ] , MobileNet [ 2 3 ] , and DenseNet [ 2 4 ] .",
      "However , a type of RNNs called Long Short Term Memory ( LSTM ) [ 1 4 ] is designed to avoid these issues .",
      "The output here could be an enhanced version of the image ( such as in image de - blurring , or super - resolution ) , or a segmentation map .",
      "One of the most popular is the stacked denoising auto - encoder ( SDAE ) [ 2 6 ] , which stacks several auto - encoders and uses them for image denoising purposes .",
      "Another popular variant is the variational auto - encoder ( VAE ) [ 2 7 ] , which imposes a prior distribution on the latent representation .",
      "The GAN loss function may be written as We can regard the GAN as a minimax game between G and D , where D is trying to minimize its classification error in distinguishing fake samples from real ones , hence maximizing the loss function , and G is trying to maximize the discriminator network 's error , hence minimizing the loss function .",
      "Since the invention of GANs , researchers have endeavored to improve/modify GANs several ways .",
      "For example , Radford et al. [ 2 8 ] proposed a convolutional GAN model , which works better than fully - connected networks when used for image generation .",
      "For example , one can imagine adapting an image classification model trained on ImageNet to a different task , such as texture classification , or face recognition .",
      "In image segmentation case , many people use a model trained on ImageNet ( a larger dataset than most of image segmentation datasets ) , as the encoder part of the network , and re - train their model from those initial weights .",
      "Long et al. [ 3 2 ] proposed one of the first deep learning works for semantic image segmentation , using a fully convolutional network ( FCN ) .",
      "An FCN ( Figure 7 ) includes only convolutional layers , which enables it to take an image of arbitrary size and produce a segmentation map of the same size .",
      "The authors modified existing CNN architectures , such as VGG 1 6 and GoogLeNet , to manage non - fixed sized input and output , by replacing all fully - connected layers with the fully - convolutional layers .",
      "As a result , the model outputs a spatial segmentation map instead of classification scores .",
      "The model was tested on PASCAL VOC , NYUDv 2 , and SIFT Flow , and achieved state - of - the - art segmentation performance .",
      "This work is considered a milestone in image segmentation , demonstrating that deep networks can be trained for semantic segmentation in an end - to - end manner on variablesized images .",
      "For instance , Liu et al. [ 3 3 ] proposed a model called ParseNet , to address an issue with FCN - ignoring global context information .",
      "ParseNet adds global context to FCNs by using the average feature for a layer to augment the features at each location .",
      "In a nutshell , ParseNet is an FCN with the described module replacing the convolutional layers ( Figure 9 ) .",
      "FCNs have been applied to a variety of segmentation problems , such as brain tumor segmentation [ 3 4 ] , instanceaware semantic segmentation [ 3 5 ] , skin lesion segmentation [ 3 6 ] , and iris segmentation [ 3 7 ] .",
      "To integrate more context , several ap - proaches incorporate probabilistic graphical models , such as Conditional Random Fields ( CRFs ) and Markov Random Field ( MRFs ) , into DL architectures .",
      "Chen et al. [ 3 8 ] proposed a semantic segmentation algorithm based on the combination of CNNs and fully connected CRFs ( Figure 1 0 ) .",
      "They showed that responses from the final layer of deep CNNs are not sufficiently localized for accurate object segmentation ( due to the invariance properties that make CNNs good for high level tasks such as classification ) .",
      "To overcome the poor localization property of deep CNNs , they combined the responses at the final CNN layer with a fully - connected CRF .",
      "The coarse score map of a CNN is upsampled via interpolated interpolation , and fed to a fully - connected CRF to refine the segmentation result .",
      "They presented a method that jointly trains CNNs and fullyconnected CRFs for semantic image segmentation , and achieved encouraging results on the challenging PASCAL VOC 2 0 1 2 dataset .",
      "In [ 4 0 ] , Zheng et al. proposed a similar semantic segmentation approach integrating CRF with CNN .",
      "In another relevant work , Lin et al. [ 4 1 ] proposed an efficient algorithm for semantic segmentation based on contextual deep CRFs .",
      "Liu et al. [ 4 2 ] proposed a semantic segmentation algorithm that incorporates rich information into MRFs , including highorder relations and mixture of label contexts .",
      "Unlike previous works that optimized MRFs using iterative algorithms , they proposed a CNN model , namely a Parsing Network , which enables deterministic end - to - end computation in a single forward pass .",
      "Another popular family of deep models for image segmentation is based on the convolutional encoder - decoder architecture .",
      "Most of the DL - based segmentation works use some kind of encoder - decoder models .",
      "We group these works into two categories , encoder - decoder models for general segmentation , and for medical image segmentation ( to better distinguish between applications ) .",
      "Noh et al. [ 4 3 ] published an early paper on semantic segmentation based on deconvolution ( a.k.a . transposed convolution ) .",
      "Following a convolution network based on the VGG 1 6 - layer net , is a multi - layer deconvolution network to generate the accurate segmentation map .",
      "In another promising work known as SegNet , Badrinarayanan et al. [ 4 4 ] proposed a convolutional encoderdecoder architecture for image segmentation ( Figure 1 2 ) .",
      "Similar to the deconvolution network , the core trainable segmentation engine of SegNet consists of an encoder network , which is topologically identical to the 1 3 convolutional layers in the VGG 1 6 network , and a corresponding decoder network followed by a pixel - wise classification layer .",
      "The main novelty of SegNet is in the way the decoder upsamples its lower resolution input feature map(s ) ; specifically , it uses pooling indices computed in the max - pooling step of the corresponding encoder to perform non - linear upsampling .",
      "A Bayesian version of SegNet was also proposed by the same authors to model the uncertainty inherent to the convolutional encoder - decoder network for scene segmentation [ 4 5 ] .",
      "Several other works adopt transposed convolutions , or encoder - decoders for image segmentation , such as Stacked Deconvolutional Network ( SDN ) [ 4 6 ] , Linknet [ 4 7 ] , W - Net [ 4 8 ] , and locality - sensitive deconvolution networks for RGB - D segmentation [ 4 9 ] .   There are several models initially developed for medical/biomedical image segmentation , which are inspired by FCNs and encoder - decoder models .",
      "U - Net [ 5 0 ] , and V - Net [ 5 1 ] , are two well - known such architectures , which are now also being used outside the medical domain .",
      "Finally , a 1 × 1 convolution processes the feature maps to generate a segmentation map that categorizes each pixel of the input image .",
      "U - Net was trained on 3 0 transmitted light microscopy images , and it won the ISBI cell tracking challenge 2 0 1 5 by a large margin .",
      "For example , Zhang et al. [ 5 4 ] developed a road segmentation/extraction algorithm based on U - Net .",
      "V - Net ( Figure 1 4 ) is another well - known , FCN - based model , which was proposed by Milletari et al. [ 5 1 ] for 3D medical image segmentation ."
    ],
    "relations": [
      [
        "scene understanding:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "medical image analysis:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "robotic perception:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "video surveillance:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "augmented reality:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "image compression:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "fully convolutional pixel - labeling networks:Method",
        "Used-For",
        "semantic and instance - level segmentation:Task"
      ],
      [
        "encoder - decoder architectures:Method",
        "Used-For",
        "semantic and instance - level segmentation:Task"
      ],
      [
        "multi - scale and pyramid based approaches:Method",
        "Used-For",
        "semantic and instance - level segmentation:Task"
      ],
      [
        "recurrent networks:Method",
        "Used-For",
        "semantic and instance - level segmentation:Task"
      ],
      [
        "visual attention:Method",
        "Used-For",
        "semantic and instance - level segmentation:Task"
      ],
      [
        "generative models:Method",
        "Used-For",
        "semantic and instance - level segmentation:Task"
      ],
      [
        "Segmentation:Task",
        "Used-For",
        "medical image analysis:Task"
      ],
      [
        "tumor boundary extraction:Task",
        "SubTask-Of",
        "medical image analysis:Task"
      ],
      [
        "measurement of tissue volumes:Task",
        "SubTask-Of",
        "medical image analysis:Task"
      ],
      [
        "Segmentation:Task",
        "Used-For",
        "autonomous vehicles:Task"
      ],
      [
        "navigable surface:Task",
        "SubTask-Of",
        "autonomous vehicles:Task"
      ],
      [
        "pedestrian detection:Task",
        "SubTask-Of",
        "autonomous vehicles:Task"
      ],
      [
        "Segmentation:Task",
        "Used-For",
        "video surveillance:Task"
      ],
      [
        "Segmentation:Task",
        "Used-For",
        "augmented reality:Task"
      ],
      [
        "thresholding:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "histogram - based bundling:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "regiongrowing:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "k - means clustering:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "watersheds:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "active contours:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "graph cuts:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "conditional and Markov random fields:Method",
        "SubClass-Of",
        "image segmentation algorithms:Method"
      ],
      [
        "DeepLabv 3:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "semantic segmentation:Task",
        "SubTask-Of",
        "Image segmentation:Task"
      ],
      [
        "instance segmentation:Task",
        "SubTask-Of",
        "Image segmentation:Task"
      ],
      [
        "Semantic segmentation:Task",
        "Used-For",
        "pixel - level labeling:Task"
      ],
      [
        "Semantic segmentation:Task",
        "Compare-With",
        "image classification:Task"
      ],
      [
        "Instance segmentation:Task",
        "SubTask-Of",
        "semantic segmentation:Task"
      ],
      [
        "deep learning - based segmentation methods:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "R - CNN based models:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "convolutional neural networks:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "recurrent neural networks:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "long short term memory:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "encoder - decoders:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "generative adversarial networks:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "CNNs:Method",
        "Synonym-Of",
        "convolutional neural networks:Method"
      ],
      [
        "RNNs:Method",
        "Synonym-Of",
        "recurrent neural networks:Method"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long short term memory:Method"
      ],
      [
        "GANs:Method",
        "Synonym-Of",
        "generative adversarial networks:Method"
      ],
      [
        "transformers:Method",
        "SubClass-Of",
        "deep neural architectures:Method"
      ],
      [
        "capsule networks:Method",
        "SubClass-Of",
        "deep neural architectures:Method"
      ],
      [
        "gated recurrent units:Method",
        "SubClass-Of",
        "deep neural architectures:Method"
      ],
      [
        "spatial transformer networks:Method",
        "SubClass-Of",
        "deep neural architectures:Method"
      ],
      [
        "CNNs:Method",
        "SubClass-Of",
        "deep learning:Method"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "phoneme recognition:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "document recognition:Task"
      ],
      [
        "AlexNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "VGGNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "ResNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "GoogLeNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "MobileNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "DenseNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "Long Short Term Memory:Method",
        "SubClass-Of",
        "RNNs:Method"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "Long Short Term Memory:Method"
      ],
      [
        "SDAE:Method",
        "Synonym-Of",
        "stacked denoising auto - encoder:Method"
      ],
      [
        "auto - encoders:Method",
        "Part-Of",
        "stacked denoising auto - encoder:Method"
      ],
      [
        "stacked denoising auto - encoder:Method",
        "Used-For",
        "image denoising:Task"
      ],
      [
        "VAE:Method",
        "Synonym-Of",
        "variational auto - encoder:Method"
      ],
      [
        "convolutional GAN:Method",
        "Compare-With",
        "fully - connected networks:Method"
      ],
      [
        "convolutional GAN:Method",
        "Used-For",
        "image generation:Task"
      ],
      [
        "fully - connected networks:Method",
        "Used-For",
        "image generation:Task"
      ],
      [
        "image classification model:Method",
        "Trained-With",
        "ImageNet:Dataset"
      ],
      [
        "image classification model:Method",
        "Used-For",
        "texture classification:Task"
      ],
      [
        "image classification model:Method",
        "Used-For",
        "face recognition:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "semantic image segmentation:Task"
      ],
      [
        "fully convolutional network:Method",
        "Used-For",
        "semantic image segmentation:Task"
      ],
      [
        "FCN:Method",
        "Synonym-Of",
        "fully convolutional network:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "FCN:Method"
      ],
      [
        "FCN:Method",
        "Used-For",
        "segmentation map:Task"
      ],
      [
        "VGG 1 6:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "GoogLeNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "PASCAL VOC:Dataset",
        "Benchmark-For",
        "segmentation:Task"
      ],
      [
        "NYUDv 2:Dataset",
        "Benchmark-For",
        "segmentation:Task"
      ],
      [
        "SIFT Flow:Dataset",
        "Benchmark-For",
        "segmentation:Task"
      ],
      [
        "FCNs:Method",
        "SubClass-Of",
        "ParseNet:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "ParseNet:Method"
      ],
      [
        "ParseNet:Method",
        "SubClass-Of",
        "FCN:Method"
      ],
      [
        "FCNs:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "brain tumor segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "instanceaware semantic segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "skin lesion segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "iris segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "FCNs:Method",
        "Used-For",
        "brain tumor segmentation:Task"
      ],
      [
        "FCNs:Method",
        "Used-For",
        "instanceaware semantic segmentation:Task"
      ],
      [
        "FCNs:Method",
        "Used-For",
        "skin lesion segmentation:Task"
      ],
      [
        "FCNs:Method",
        "Used-For",
        "iris segmentation:Task"
      ],
      [
        "Conditional Random Fields:Method",
        "SubClass-Of",
        "probabilistic graphical models:Method"
      ],
      [
        "Markov Random Field:Method",
        "SubClass-Of",
        "probabilistic graphical models:Method"
      ],
      [
        "CRFs:Method",
        "Synonym-Of",
        "Conditional Random Fields:Method"
      ],
      [
        "MRFs:Method",
        "Synonym-Of",
        "Markov Random Field:Method"
      ],
      [
        "Conditional Random Fields:Method",
        "Part-Of",
        "DL architectures:Method"
      ],
      [
        "Markov Random Field:Method",
        "Part-Of",
        "DL architectures:Method"
      ],
      [
        "probabilistic graphical models:Method",
        "Part-Of",
        "DL architectures:Method"
      ],
      [
        "CNNs:Method",
        "Part-Of",
        "semantic segmentation algorithm:Method"
      ],
      [
        "fully connected CRFs:Method",
        "Part-Of",
        "semantic segmentation algorithm:Method"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "fully - connected CRF:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "interpolated interpolation:Method",
        "Used-For",
        "CNN:Method"
      ],
      [
        "fully - connected CRF:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "fullyconnected CRFs:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "PASCAL VOC 2 0 1 2:Dataset",
        "Benchmark-For",
        "image segmentation:Task"
      ],
      [
        "fullyconnected CRFs:Method",
        "Evaluated-With",
        "PASCAL VOC 2 0 1 2:Dataset"
      ],
      [
        "CNNs:Method",
        "Evaluated-With",
        "PASCAL VOC 2 0 1 2:Dataset"
      ],
      [
        "CRF:Method",
        "Part-Of",
        "semantic segmentation approach:Method"
      ],
      [
        "CNN:Method",
        "Part-Of",
        "semantic segmentation approach:Method"
      ],
      [
        "contextual deep CRFs:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "MRFs:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Parsing Network:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "MRFs:Method",
        "Compare-With",
        "Parsing Network:Method"
      ],
      [
        "encoder - decoder:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "DL:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "encoder - decoder:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "encoder - decoder:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "encoder - decoder:Method",
        "Used-For",
        "medical image segmentation:Task"
      ],
      [
        "deconvolution:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "transposed convolution:Method",
        "Synonym-Of",
        "deconvolution:Method"
      ],
      [
        "VGG 1 6 - layer net:Method",
        "Part-Of",
        "convolution network:Method"
      ],
      [
        "convolution network:Method",
        "SubClass-Of",
        "multi - layer deconvolution network:Method"
      ],
      [
        "convolution network:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "convolutional encoderdecoder architecture:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "SegNet:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "encoder network:Method",
        "Part-Of",
        "SegNet:Method"
      ],
      [
        "pixel - wise classification layer:Method",
        "Part-Of",
        "encoder network:Method"
      ]
    ]
  },
  {
    "doc_id": "210702798",
    "chunk_id": 2,
    "content": [
      "One of the most prominent models of this sort is the Feature Pyramid Network ( FPN ) proposed by Lin et al. [ 5 6 ] , which was developed mainly for object detection but was then also applied to segmentation .",
      "For image segmentation , the authors use two multi - layer perceptrons ( MLPs ) to generate the masks .",
      "Zhao et al. [ 5 7 ] developed the Pyramid Scene Parsing Network ( PSPN ) , a multi - scale network to better learn the global context representation of a scene ( Figure 1 6 ) .",
      "Different patterns are extracted from the input image using a residual network ( ResNet ) as a feature extractor , with a dilated network .",
      "There are other models using multi - scale analysis for segmentation , such as DM - Net ( Dynamic Multi - scale Filters Network ) [ 5 9 ] , Context contrasted network and gated multiscale aggregation ( CCN ) [ 6 0 ] , Adaptive Pyramid Context Network ( APC - Net ) [ 6 1 ] , Multi - scale context intertwining ( MSCI ) [ 6 2 ] , and salient object segmentation [ 6 3 ] .",
      "The regional convolutional network ( R - CNN ) and its extensions ( Fast R - CNN , Faster R - CNN , Maksed - RCNN ) have proven successful in object detection applications .",
      "Some of the extensions of R - CNN have been heavily used to address the instance segmentation problem ; i.e. , the task of simultaneously performing object detection and semantic segmentation .",
      "In particular , the Faster R - CNN [ 6 4 ] architecture ( Figure 1 7 ) developed for object detection uses a region proposal network ( RPN ) to propose bounding box candidates .",
      "The RPN extracts a Region of Interest ( RoI ) , and a RoIPool layer computes features from these proposals in order to infer the bounding box coordinates and the class of the object .",
      "In one extension of this model , He et al. [ 6 5 ] proposed a Mask R - CNN for object instance segmentation , which beat all previous benchmarks on many COCO challenges .",
      "Mask R - CNN is essentially a Faster R - CNN with 3 output branches ( Figure 1 8 ) -the first computes the bounding box coordinates , the second computes the associated classes , and the third computes the binary mask to Fig. 1 7 .",
      "Each image is processed by convolutional layers and its features are extracted , a sliding window is used in RPN for each location over the feature map , for each location , k ( k = 9 ) anchor boxes are used ( 3 scales of 1 2 8 , 2 5 6 and 5 1 2 , and 3 aspect ratios of 1: 1 , 1: 2 , 2: 1 ) to generate a region proposal ; A cls layer outputs 2k scores whether there or not there is an object for k boxes ; A reg layer outputs 4k for the coordinates ( box center coordinates , width and height ) of k boxes .",
      "The Mask R - CNN loss function combines the losses of the bounding box coordinates , the predicted class , and the segmentation mask , and trains all of them jointly .",
      "As in the Mask R - CNN , the output of the adaptive feature pooling layer feeds three branches .",
      "Chen et al. [ 6 9 ] developed an instance segmentation model , MaskLab ( Figure 2 1 ) , by refining object detection with semantic and direction features based on Faster R - CNN .",
      "This model produces three outputs , box detection , semantic segmentation , and direction prediction .",
      "Within each region of interest , MaskLab performs foreground/background segmentation by combining semantic and direction prediction .",
      "Another interesting model is Tensormask , proposed by Chen et al. [ 7 0 ] , which is based on dense sliding window instance segmentation .",
      "Many other instance segmentation models have been developed based on R - CNN , such as those developed for mask proposals , including R - FCN [ 7 1 ] , DeepMask [ 7 2 ] , SharpMask [ 7 3 ] , PolarMask [ 7 4 ] , and boundary - aware instance segmentation [ 7 5 ] .",
      "It is worth noting that there is another promising research direction that attempts to solve the instance segmentation problem by learning grouping cues for bottom - up segmentation , such as Deep Watershed Transform [ 7 6 ] , and Semantic Instance Segmentation via Deep Metric Learning [ 7 7 ] .   Dilated convolution ( a.k.a . \" atrous \" convolution ) introduces another parameter to convolutional layers , the dilation rate .",
      "Dilated convolutions have been popular in the field of real - time segmentation , and many recent publications report the use of this technique .",
      "Some of most important include the DeepLab family [ 7 8 ] , multiscale context aggregation [ 7 9 ] , dense upsampling convolution and hybrid dilatedconvolution ( DUC - HDC ) [ 8 0 ] , densely connected Atrous Spatial Pyramid Pooling ( DenseASPP ) [ 8 1 ] , and the efficient neural network ( ENet ) [ 8 2 ] .",
      "DeepLabv 1 [ 3 8 ] and DeepLabv 2 [ 7 8 ] are among some of the most popular image segmentation approaches , developed by Chen et al .. The latter has three key features .",
      "First is the use of dilated convolution to address the decreasing resolution in the network ( caused by max - pooling and striding ) .",
      "Second is Atrous Spatial Pyramid Pooling ( ASPP ) , which probes an incoming convolutional feature layer with filters at multiple sampling rates , thus capturing objects as well as image context at multiple scales to robustly segment objects at multiple scales .",
      "The best DeepLab ( using a ResNet - 1 0 1 as backbone ) has reached a 7 9 . 7 % mIoU score on the 2 0 1 2 PASCAL VOC challenge , a 4 5 . 7 % mIoU score on the PASCAL - Context challenge and a 7 0 . 4 % mIoU score on the Cityscapes challenge .",
      "Figure 2 4 illustrates the Deeplab model , which is similar to [ 3 8 ] , the main difference being the use of dilated convolution and ASPP .",
      "Subsequently , Chen et al. [ 1 2 ] proposed DeepLabv 3 , which combines cascaded and parallel modules of dilated convolutions .",
      "The parallel convolution modules are grouped in the ASPP .",
      "A 1 × 1 convolution and batch normalisation A bilinear interpolation stage enlarges the feature maps to the original image resolution .",
      "Finally , a fully connected CRF refines the segmentation result to better capture the object boundaries .",
      "In 2 0 1 8 , Chen et al. [ 8 3 ] released Deeplabv 3 + , which uses an encoder - decoder architecture ( Figure 2 5 ) , including atrous separable convolution , composed of a depthwise convolution ( spatial convolution for each channel of the input ) and pointwise convolution ( 1 × 1 convolution with the depthwise convolution as input ) .",
      "The most relevant model has a modified Xception backbone with more layers , dilated depthwise separable convolutions instead of max pooling and batch normalization .",
      "The best DeepLabv 3 + pretrained on the COCO and the JFT datasets has obtained a 8 9 . 0 % mIoU score on the 2 0 1 2 PASCAL VOC challenge .   While CNNs are a natural fit for computer vision problems , they are not the only possibility .",
      "RNNs are useful in modeling the short/long term dependencies among pixels to ( potentially ) improve the estimation of the segmentation map .",
      "Using RNNs , pixels may be linked together and processed sequentially to model global contexts and improve semantic segmentation .",
      "Visin et al. [ 8 4 ] proposed an RNN - based model for semantic segmentation called ReSeg .",
      "This model is mainly based on another work , ReNet [ 8 5 ] , which was developed for image classification .",
      "Each ReNet layer ( Figure 2 6 ) is composed of four RNNs that sweep the image horizontally and vertically in both directions , encoding patches/activations , and providing relevant global information .",
      "To perform image segmentation with the ReSeg model ( Figure 2 7 ) , ReNet layers are stacked on top of pre - trained VGG - 1 6 convolutional layers that extract generic local features .",
      "Gated Recurrent Units ( GRUs ) are used because they provide a good balance between memory usage and computational power .",
      "In another work , Byeon et al. [ 8 6 ] developed a pixellevel segmentation and classification of scene images using long - short - term - memory ( LSTM ) network .",
      "In this work , classification , segmentation , and context integration are all carried out by 2D LSTM networks , allowing texture and spatial model parameters to be learned within a single model .",
      "The block - diagram of the proposed 2D LSTM network for image segmentation in [ 8 6 ] is shown in Figure 2 8 .",
      "Liang et al. [ 8 7 ] proposed a semantic segmentation model based on the Graph Long Short - Term Memory ( Graph LSTM ) network , a generalization of LSTM from sequential data or multidimensional data to general graph - structured data .",
      "Instead of evenly dividing an image to pixels or patches in existing multi - dimensional LSTM structures ( e.g. , row , grid and diagonal LSTMs ) , they take each arbitrary - shaped superpixel as a semantically consistent node , and adaptively construct an undirected graph for the image , where the spatial relations of the superpixels are naturally used as edges .",
      "Figure 2 9 presents a visual comparison of the traditional pixel - wise RNN model and graph - LSTM model .",
      "To adapt the Graph LSTM model to semantic segmentation ( Figure 3 0 ) , LSTM layers built on a super - pixel map are appended on the convolutional layers to enhance visual features with global structure context .",
      "The node updating sequence for the subsequent Graph LSTM layers is determined by the confidence - drive scheme based on the initial confidence maps , and then the Graph LSTM layers can sequentially update the hidden states of all superpixel nodes .",
      "Xiang and Fox [ 8 8 ] proposed Data Associated Recurrent Neural Networks ( DA - RNNs ) , for joint 3D scene mapping and semantic labeling .",
      "DA - RNNs use a new recurrent neural network architecture ( Figure 3 1 ) for semantic labeling on RGB - D videos .",
      "Hu et al. [ 8 9 ] developed a semantic segmentation algorithm based on natural language expression , using a combination of CNN to encode the image and LSTM to encode its natural language description .",
      "In the considered model , a recurrent LSTM network is used to encode the referential expression into a vector representation , and an FCN is used to extract a spatial feature map from the image and output a spatial response map for the target object .",
      "An example segmentation result of this model ( for the query \" people in blue coat \" ) is shown in Figure 3 3 .   Attention mechanisms have been persistently explored in computer vision over the years , and it is therefore not surprising to find publications that apply such mechanisms to semantic segmentation .",
      "Their Reverse Attention Network ( RAN ) architecture ( Figure 3 5 ) trains the model to capture the opposite concept ( i.e. , features that are not associated with a target class ) as well .",
      "Li et al. [ 9 2 ] developed a Pyramid Attention Network for semantic segmentation .",
      "More recently , Fu et al. [ 9 3 ] proposed a dual attention network for scene segmentation , which can capture rich contextual dependencies based on the self - attention mechanism .",
      "Various other works explore attention mechanisms for semantic segmentation , such as OCNet [ 9 4 ] which proposed an object context pooling inspired by self - attention mechanism , Expectation - Maximization Attention ( EMANet ) [ 9 5 ] , Criss - Cross Attention Network ( CCNet ) [ 9 6 ] , end - to - end instance segmentation with recurrent attention [ 9 7 ] , a pointwise spatial attention network for scene parsing [ 9 8 ] , and a discriminative feature network ( DFN ) [ 9 9 ] , which comprises two sub - networks : a Smooth Network ( that contains a Channel Attention Block and global average pooling to select the more discriminative features ) and a Border Network ( to make the bilateral features of the boundary distinguishable ) .   Since their introduction , GANs have been applied to a wide range tasks in computer vision , and have been adopted for image segmentation too .",
      "They trained a convolutional semantic segmentation network ( Figure 3 7 ) , along with an adversarial network that discriminates ground - truth segmentation maps from those generated by the segmentation network .",
      "They showed that the adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2 0 1 2 datasets ."
    ],
    "relations": [
      [
        "decoder network:Method",
        "Part-Of",
        "encoder network:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "max - pooling:Method",
        "Part-Of",
        "SegNet:Method"
      ],
      [
        "max - pooling:Method",
        "Used-For",
        "non - linear upsampling:Task"
      ],
      [
        "convolutional encoder - decoder network:Method",
        "Used-For",
        "scene segmentation:Task"
      ],
      [
        "encoder - decoders:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "transposed convolutions:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "SDN:Method",
        "Synonym-Of",
        "Stacked Deconvolutional Network:Method"
      ],
      [
        "Stacked Deconvolutional Network:Method",
        "Used-For",
        "RGB - D segmentation:Task"
      ],
      [
        "Linknet:Method",
        "Used-For",
        "RGB - D segmentation:Task"
      ],
      [
        "W - Net:Method",
        "Used-For",
        "RGB - D segmentation:Task"
      ],
      [
        "1 × 1 convolution:Method",
        "Used-For",
        "generate a segmentation map:Task"
      ],
      [
        "U - Net:Method",
        "Evaluated-With",
        "ISBI cell tracking challenge 2 0 1 5:Dataset"
      ],
      [
        "U - Net:Method",
        "Used-For",
        "road segmentation/extraction:Task"
      ],
      [
        "V - Net:Method",
        "SubClass-Of",
        "FCN:Method"
      ],
      [
        "V - Net:Method",
        "Used-For",
        "3D medical image segmentation:Task"
      ],
      [
        "FPN:Method",
        "Synonym-Of",
        "Feature Pyramid Network:Method"
      ],
      [
        "Feature Pyramid Network:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "Feature Pyramid Network:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "multi - layer perceptrons:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "MLPs:Method",
        "Synonym-Of",
        "multi - layer perceptrons:Method"
      ],
      [
        "PSPN:Method",
        "Synonym-Of",
        "Pyramid Scene Parsing Network:Method"
      ],
      [
        "Pyramid Scene Parsing Network:Method",
        "SubClass-Of",
        "multi - scale network:Method"
      ],
      [
        "ResNet:Method",
        "Synonym-Of",
        "residual network:Method"
      ],
      [
        "dilated network:Method",
        "Part-Of",
        "residual network:Method"
      ],
      [
        "DM - Net:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "Context contrasted network:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "gated multiscale aggregation:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "Adaptive Pyramid Context Network:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "Multi - scale context intertwining:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "salient object segmentation:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "DM - Net:Method",
        "Synonym-Of",
        "Dynamic Multi - scale Filters Network:Method"
      ],
      [
        "CCN:Method",
        "Synonym-Of",
        "gated multiscale aggregation:Method"
      ],
      [
        "APC - Net:Method",
        "Synonym-Of",
        "Adaptive Pyramid Context Network:Method"
      ],
      [
        "MSCI:Method",
        "Synonym-Of",
        "Multi - scale context intertwining:Method"
      ],
      [
        "R - CNN:Method",
        "Synonym-Of",
        "regional convolutional network:Method"
      ],
      [
        "Fast R - CNN:Method",
        "SubClass-Of",
        "regional convolutional network:Method"
      ],
      [
        "Faster R - CNN:Method",
        "SubClass-Of",
        "regional convolutional network:Method"
      ],
      [
        "Maksed - RCNN:Method",
        "SubClass-Of",
        "regional convolutional network:Method"
      ],
      [
        "regional convolutional network:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "Fast R - CNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "Faster R - CNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "Maksed - RCNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "object detection:Task",
        "SubTask-Of",
        "instance segmentation:Task"
      ],
      [
        "semantic segmentation:Task",
        "SubTask-Of",
        "instance segmentation:Task"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "region proposal network:Method",
        "Part-Of",
        "Faster R - CNN:Method"
      ],
      [
        "Faster R - CNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "RPN:Method",
        "Synonym-Of",
        "region proposal network:Method"
      ],
      [
        "RoIPool layer:Method",
        "Part-Of",
        "RPN:Method"
      ],
      [
        "RoI:Task",
        "Synonym-Of",
        "Region of Interest:Task"
      ],
      [
        "RPN:Method",
        "Used-For",
        "Region of Interest:Task"
      ],
      [
        "Mask R - CNN:Method",
        "Used-For",
        "object instance segmentation:Task"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "object instance segmentation:Task"
      ],
      [
        "Mask R - CNN:Method",
        "Evaluated-With",
        "COCO:Dataset"
      ],
      [
        "Mask R - CNN:Method",
        "SubClass-Of",
        "Faster R - CNN:Method"
      ],
      [
        "RPN:Method",
        "Used-For",
        "region proposal:Task"
      ],
      [
        "adaptive feature pooling:Method",
        "Part-Of",
        "Mask R - CNN:Method"
      ],
      [
        "MaskLab:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "MaskLab:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "MaskLab:Method",
        "Used-For",
        "foreground/background segmentation:Task"
      ],
      [
        "MaskLab:Method",
        "Used-For",
        "semantic and direction prediction:Task"
      ],
      [
        "dense sliding window instance segmentation:Method",
        "Part-Of",
        "Tensormask:Method"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "R - FCN:Method",
        "SubClass-Of",
        "R - CNN:Method"
      ],
      [
        "DeepMask:Method",
        "SubClass-Of",
        "R - CNN:Method"
      ],
      [
        "SharpMask:Method",
        "SubClass-Of",
        "R - CNN:Method"
      ],
      [
        "PolarMask:Method",
        "SubClass-Of",
        "R - CNN:Method"
      ],
      [
        "boundary - aware instance segmentation:Method",
        "SubClass-Of",
        "R - CNN:Method"
      ],
      [
        "Semantic Instance Segmentation:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "Deep Watershed Transform:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "Deep Watershed Transform:Method",
        "Used-For",
        "bottom - up segmentation:Task"
      ],
      [
        "Semantic Instance Segmentation:Method",
        "Used-For",
        "bottom - up segmentation:Task"
      ],
      [
        "Deep Metric Learning:Method",
        "Used-For",
        "Semantic Instance Segmentation:Method"
      ],
      [
        "\" atrous \" convolution:Method",
        "Synonym-Of",
        "Dilated convolution:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "Dilated convolution:Method"
      ],
      [
        "Dilated convolutions:Method",
        "Used-For",
        "real - time segmentation:Task"
      ],
      [
        "multiscale context aggregation:Method",
        "SubClass-Of",
        "DeepLab:Method"
      ],
      [
        "dense upsampling convolution:Method",
        "SubClass-Of",
        "DeepLab:Method"
      ],
      [
        "hybrid dilatedconvolution:Method",
        "SubClass-Of",
        "DeepLab:Method"
      ],
      [
        "densely connected Atrous Spatial Pyramid Pooling:Method",
        "SubClass-Of",
        "DeepLab:Method"
      ],
      [
        "efficient neural network:Method",
        "SubClass-Of",
        "DeepLab:Method"
      ],
      [
        "DUC - HDC:Method",
        "Synonym-Of",
        "hybrid dilatedconvolution:Method"
      ],
      [
        "DenseASPP:Method",
        "Synonym-Of",
        "densely connected Atrous Spatial Pyramid Pooling:Method"
      ],
      [
        "ENet:Method",
        "Synonym-Of",
        "efficient neural network:Method"
      ],
      [
        "DeepLabv 1:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "DeepLabv 2:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "ASPP:Method",
        "Synonym-Of",
        "Atrous Spatial Pyramid Pooling:Method"
      ],
      [
        "ResNet - 1 0 1:Method",
        "Part-Of",
        "DeepLab:Method"
      ],
      [
        "DeepLab:Method",
        "Evaluated-With",
        "2 0 1 2 PASCAL VOC:Dataset"
      ],
      [
        "DeepLab:Method",
        "Evaluated-With",
        "PASCAL - Context:Dataset"
      ],
      [
        "DeepLab:Method",
        "Evaluated-With",
        "Cityscapes:Dataset"
      ],
      [
        "ASPP:Method",
        "Part-Of",
        "Deeplab:Method"
      ],
      [
        "dilated convolution:Method",
        "Part-Of",
        "Deeplab:Method"
      ],
      [
        "dilated convolutions:Method",
        "Part-Of",
        "DeepLabv 3:Method"
      ],
      [
        "parallel convolution modules:Method",
        "Part-Of",
        "ASPP:Method"
      ],
      [
        "fully connected CRF:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "encoder - decoder:Method",
        "Part-Of",
        "Deeplabv 3 +:Method"
      ],
      [
        "1 × 1 convolution:Method",
        "Part-Of",
        "pointwise convolution:Method"
      ],
      [
        "depthwise convolution:Method",
        "Part-Of",
        "pointwise convolution:Method"
      ],
      [
        "dilated depthwise separable convolutions:Method",
        "Part-Of",
        "Xception:Method"
      ],
      [
        "DeepLabv 3 +:Method",
        "Trained-With",
        "COCO:Dataset"
      ],
      [
        "DeepLabv 3 +:Method",
        "Trained-With",
        "JFT:Dataset"
      ],
      [
        "DeepLabv 3 +:Method",
        "Evaluated-With",
        "2 0 1 2 PASCAL VOC:Dataset"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "RNNs:Method",
        "Used-For",
        "segmentation map:Task"
      ],
      [
        "RNNs:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "ReSeg:Method",
        "SubClass-Of",
        "RNN:Method"
      ],
      [
        "RNN:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "ReSeg:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "ReNet:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "RNNs:Method",
        "Part-Of",
        "ReNet:Method"
      ],
      [
        "ReSeg:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "VGG - 1 6 convolutional layers:Method",
        "Part-Of",
        "ReNet:Method"
      ],
      [
        "GRUs:Method",
        "Synonym-Of",
        "Gated Recurrent Units:Method"
      ],
      [
        "long - short - term - memory:Method",
        "Used-For",
        "pixellevel segmentation:Task"
      ],
      [
        "long - short - term - memory:Method",
        "Used-For",
        "classification of scene images:Task"
      ],
      [
        "LSTM:Method",
        "Synonym-Of",
        "long - short - term - memory:Method"
      ],
      [
        "2D LSTM:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "2D LSTM:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "2D LSTM:Method",
        "Used-For",
        "context integration:Task"
      ],
      [
        "2D LSTM:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "Graph Long Short - Term Memory:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Graph LSTM:Method",
        "Synonym-Of",
        "Graph Long Short - Term Memory:Method"
      ],
      [
        "Graph Long Short - Term Memory:Method",
        "SubClass-Of",
        "LSTM:Method"
      ],
      [
        "pixel - wise RNN model:Method",
        "Compare-With",
        "graph - LSTM:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "Graph LSTM:Method"
      ],
      [
        "LSTM layers:Method",
        "Part-Of",
        "Graph LSTM:Method"
      ],
      [
        "Graph LSTM:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "DA - RNNs:Method",
        "Synonym-Of",
        "Data Associated Recurrent Neural Networks:Method"
      ],
      [
        "Data Associated Recurrent Neural Networks:Method",
        "Used-For",
        "3D scene mapping:Task"
      ]
    ]
  },
  {
    "doc_id": "210702798",
    "chunk_id": 3,
    "content": [
      "Souly et al. [ 1 0 1 ] proposed semi - weakly supervised semantic segmentation using GANs .",
      "They designed an FCN discriminator to differentiate the predicted probability maps from the ground truth segmentation distribution , considering the spatial resolution .",
      "They used an FCN as the segmentor to generate segmentation label maps , and proposed a novel adversarial critic network with a multi - scale L 1 loss function to force the critic and segmentor to learn both global and local features that capture long and short range spatial relationships between pixels .",
      "Various other publications report on segmentation models based on adversarial training , such as Cell Image Segmentation Using GANs [ 1 0 4 ] , and segmentation and generation of the invisible parts of objects [ 1 0 5 ] .",
      "The exploration of synergies between FCNs and Active Contour Models ( ACMs ) [ 7 ] has recently attracted research interest .",
      "For example , inspired by the global energy formulation of [ 1 0 6 ] , Chen et al. [ 1 0 7 ] proposed a supervised loss layer that incorporated area and size information of the predicted masks during training of an FCN and tackled the problem of ventricle segmentation in cardiac MRI .",
      "Similarly , Gur et al. [ 1 0 8 ] presented an unsupervised loss function based on morphological active contours without edges [ 1 0 9 ] for microvascular image segmentation .",
      "A different approach initially sought to utilize the ACM merely as a post - processor of the output of an FCN and several efforts attempted modest co - learning by pre - training the FCN .",
      "One example of an ACM post - processor for the task of semantic segmentation of natural images is the work by Le et al. [ 1 1 0 ] in which level - set ACMs are implemented as RNNs .",
      "For medical image segmentation , Hatamizadeh et al. [ 1 1 2 ] proposed an integrated Deep Active Lesion Segmentation ( DALS ) model that trains the FCN backbone to predict the parameter functions of a novel , locallyparameterized level - set energy functional .",
      "In another relevant effort , Marcos et al. [ 1 1 3 ] proposed Deep Structured Active Contours ( DSAC ) , which combines ACMs and pre - trained FCNs in a structured prediction framework for building instance segmentation ( albeit with manual initialization ) in aerial images .",
      "For the same application , Cheng et al. [ 1 1 4 ] proposed the Deep Active Ray Network ( DarNet ) , which is similar to DSAC , but with a different explicit ACM formulation based on polar coordinates to prevent contour self - intersection .",
      "A truly end - to - end backpropagation trainable , fully - integrated FCN - ACM combination was recently introduced by Hatamizadeh et al. [ 1 1 5 ] , dubbed Deep Convolutional Active Contours ( DCAC ) .",
      "In addition to the above models , there are several other popular DL architectures for segmentation , such as the following : Context Encoding Network ( EncNet ) that uses a basic feature extractor and feeds the feature maps into a Context Encoding Module [ 1 1 6 ] .",
      "RefineNet [ 1 1 7 ] , which is a multi - path refinement network that explicitly exploits all the information available along the down - sampling process to enable highresolution prediction using long - range residual connections . \" Object - Contextual Representations \" ( OCR ) [ 1 1 9 ] , which learns object regions under the supervision of the groundtruth , and computes the object region representation , and the relation between each pixel and each object region , and augment the representation pixels with the object - contextual representation .",
      "Seednet [ 1 1 8 ] , which introduced an automatic seed generation technique with deep reinforcement learning that learns to solve the interactive segmentation problem , Feedforward - Net [ 1 2 4 ] which maps image super - pixels to rich feature representations extracted from a sequence of nested regions of increasing extent and exploits statistical structures in the image and in the label space without setting up explicit structured prediction mechanisms .",
      "Yet additional models include BoxSup [ 1 2 0 ] , Graph convolutional networks [ 1 2 1 ] , Wide ResNet [ 1 2 2 ] , Exfuse ( enhancing low - level and high - level features fusion ) [ 1 2 3 ] , dual image segmentation ( DIS ) [ 1 2 5 ] , FoveaNet ( Perspectiveaware scene parsing ) [ 1 2 6 ] , Ladder DenseNet [ 1 2 7 ] , Bilateral segmentation network ( BiSeNet ) [ 1 2 8 ] , Semantic Prediction Guidance for Scene Parsing ( SPGNet ) [ 1 2 9 ] , Gated shape CNNs [ 1 3 0 ] , Adaptive context network ( AC - Net ) [ 1 3 1 ] , Dynamic - structured semantic propagation network ( DSSPN ) [ 1 3 2 ] , symbolic graph reasoning ( SGR ) [ 1 3 3 ] , CascadeNet [ 1 3 4 ] , Scale - adaptive convolutions ( SAC ) [ 1 3 5 ] , Unified perceptual parsing ( UperNet ) [ 1 3 6 ] .",
      "Panoptic segmentation [ 1 3 7 ] is also another interesting ( and newer ) segmentation problem with rising popularity , and there are already several interesting works on this direction , including Panoptic Feature Pyramid Network [ 1 3 8 ] , attention - guided network for Panoptic segmentation [ 1 3 9 ] , and Seamless Scene Segmentation [ 1 4 0 ] .",
      "Figure 4 1 illustrates the timeline of popular DL - based works for semantic segmentation , as well as instance segmentation since 2 0 1 4 .",
      "Data augmentation serves to increase the number of training samples by applying a set of transformation ( either in the data space , or feature space , or sometimes both ) to the images ( i.e. , both the input image and the segmentation map ) .",
      "Data augmentation has proven to improve the performance of the models , especially when learning from limited datasets , such as those in medical image analysis .",
      "The following are some of the most popular : PASCAL Visual Object Classes ( VOC ) [ 1 4 1 ] is one of most popular datasets in computer vision , with annotated images available for 5 tasks - classification , segmentation , detection , action recognition , and person layout .",
      "From [ 1 4 1 ] . [ 1 4 2 ] is an extension of the PASCAL VOC 2 0 1 0 detection challenge , and it contains pixel - wise labels for all training images .",
      "It contains more than 4 0 0 classes ( including the original 2 0 classes plus backgrounds from PASCAL VOC segmentation ) , divided into three categories ( objects , stuff , and hybrids ) .",
      "Figure 4 3 shows the segmentation map of three sample images of this dataset . [ 1 4 3 ] is another large - scale object detection , segmentation , and captioning dataset .",
      "A sample image and its segmentation map in COCO , and its comparison with previous datasets .",
      "From [ 1 4 3 ] .   Cityscapes [ 1 4 4 ] is a large - scale database with a focus on semantic understanding of urban street scenes .",
      "ADE 2 0 K / MIT Scene Parsing ( SceneParse 1 5 0 ) offers a standard training and evaluation platform for scene parsing algorithms .",
      "Stanford background [ 1 4 6 ] contains outdoor images of scenes from existing datasets , such as LabelMe , MSRC , and PASCAL VOC .",
      "Berkeley Segmentation Dataset ( BSD ) [ 1 4 7 ] contains 1 2 , 0 0 0 hand - labeled segmentations of 1, 0 0 0 Corel dataset images from 3 0 human subjects .",
      "It aims to provide an empirical basis for research on image segmentation and boundary detection .",
      "Youtube - Objects [ 1 4 8 ] contains videos collected from YouTube , which include objects from ten PASCAL VOC classes ( aeroplane , bird , boat , car , cat , cow , dog , horse , motorbike , and train ) .",
      "KITTI [ 1 5 0 ] is one of the most popular datasets for mobile robotics and autonomous driving .",
      "Other Datasets are available for image segmentation purposes too , such as Semantic Boundaries Dataset ( SBD ) [ 1 5 2 ] , PASCAL Part [ 1 5 3 ] , SYNTHIA [ 1 5 4 ] , and Adobes Portrait Segmentation [ 1 5 5 ] .",
      "SUN RGB - D [ 1 5 8 ] provides an RGB - D benchmark for the goal of advancing the state - of - the - art in all major scene understanding tasks .",
      "The objects are organized into 5 1 categories , arranged using WordNet hypernym - hyponym relationships ( similar to ImageNet ) .",
      "ScanNet [ 1 6 0 ] is an RGB - D video dataset containing 2. 5 million views in more than 1, 5 0 0 scans , annotated with 3D camera poses , surface reconstructions , and instancelevel semantic segmentations .",
      "Using this data helped achieve state - of - the - art performance on several 3D scene understanding tasks , including 3D object classification , semantic voxel labeling , and CAD model retrieval . 3D image datasets are popular in robotic , medical image analysis , 3D scene analysis , and construction applications .",
      "ShapeNet Core : ShapeNetCore is a subset of the full ShapeNet dataset [ 1 6 2 ] with single clean 3D models and manually verified category and alignment annotations [ 1 6 3 ] .",
      "In this section , we first provide a summary of some of the popular metrics used in evaluating the performance of segmentation models , and then we provide the quantitative performance of the promising DL - based segmentation models on popular datasets .",
      "It is defined as the area of intersection between the predicted segmentation map and the ground truth , divided by the area of union between the predicted segmentation map and the ground truth : where A and B denote the ground truth and the predicted segmentation maps , respectively .",
      "Furthermore , only a small percentage of publications provide additional information , such as execution time and memory footprint , in a reproducible way , which is important to industrial applications of segmentation models ( such as drones , selfdriving cars , robotics , etc . ) that may run on embedded consumer devices with limited computational power and storage , making fast , light - weight models crucial .",
      "ResNet - 5 0 3 7 . 9 DSSPN [ 1 3 2 ] ResNet - 1 0 1 3 7 . 3 EMA - Net [ 9 5 ] ResNet - 5 0 3 7 . 5 SGR [ 1 3 3 ] ResNet - 1 0 1 3 9 . 1 OCR [ 1 1 9 ] ResNet - 1 0 1 3 9 . 5 DANet [ 9 3 ] ResNet - 1 0 1 3 9 . 7 EMA - Net [ 9 5 ] ResNet - 5 0 3 9 . 9 AC - Net [ 1 3 1 ] ResNet - 1 0 1 4 0 . 1 OCR [ 1 1 9 ] HRNetV 2 - W 4 8 4 0 . 5 ResNet - 1 0 1 4 3 . 6 8 DM - Net [ 5 9 ] ResNet - 1 0 1 4 5 . 5 OCR [ 1 1 9 ] HRNetV 2 - W 4 8 4 5 . 6 AC - Net [ 1 3 1 ] ResNet - 1 0 1 4 5 . 9 The following tables summarize the performances of several of the prominent DL - based segmentation models on different datasets .",
      "Clearly , there has been much improvement in the accuracy of the models since the introduction of the FCN , the first DL - based image segmentation model . 1 Table 2 focuses on the Cityscape test dataset .",
      "This dataset is more challenging than PASCAL VOC , and Cityescapes , as the highest mIoU is approximately 4 0 % .",
      "This dataset is also more challenging than the PASCAL VOC and Cityescapes datasets .",
      "Finally , Table 5 summarizes the performance of several prominent models for RGB - D segmentation on the NYUD - v 2 and SUN - RGBD datasets .",
      "Note that some works report two versions of their models : one which is only trained on PASCAL VOC and another that is pre - trained on a different dataset ( such as MS - COCO , ImageNet , or JFT - 3 0 0 M ) and then fine - tuned on VOC .",
      "Performance of segmentation models on the NYUD - v 2 , and SUN - RGBD datasets , in terms of mIoU , and mean Accuracy ( mAcc ) .    There is not doubt that image segmentation has benefited greatly from deep learning , but several challenges lie ahead .",
      "Several large - scale image datasets have been created for semantic segmentation and instance segmentation .",
      "Weakly - supervised ( a.k.a . few shot learning ) and unsupervised learning are becoming very active research areas .",
      "These techniques promise to be specially valuable for image segmentation , as collecting labeled samples for segmentation problem is problematic in many application domains , particularly so in medical image analysis .",
      "The transfer learning approach is to train a generic image segmentation model on a large set of labeled samples ( perhaps from a public benchmark ) , and then fine - tune that model on a few samples from some specific target application .",
      "There are many details in images that that can be captured to train a segmentation models with far fewer training samples , with the help of self - supervised learning .",
      "This is useful for computer vision systems that are , for example , deployed in autonomous vehicles .",
      "Models based on dilated convolution help to increase the speed of segmentation models to some extent , but there is still plenty of room for improvement .",
      "This can be done either by using simpler models , or by using model compression techniques , or even training a complex model and then using knowledge distillation techniques to compress it into a smaller , memory efficient network that mimics the complex model .",
      "However , there has been an increasing interest in pointcloud segmentation , which has a wide range of applications , in 3D modeling , self - driving cars , robotics , building modeling , etc .",
      "We have surveyed more than 1 0 0 recent image segmentation algorithms based on deep learning models , which have achieved impressive performance in various image segmentation tasks and benchmarks , grouped into ten categories such as : CNN and FCN , RNN , R - CNN , dilated CNN , attentionbased models , generative and adversarial models , among others .",
      "We summarized quantitative performance analyses of these models on some popular benchmarks , such as the PASCAL VOC , MS COCO , Cityscapes , and ADE 2 0 k datasets ."
    ],
    "relations": [
      [
        "Data Associated Recurrent Neural Networks:Method",
        "Used-For",
        "semantic labeling:Task"
      ],
      [
        "DA - RNNs:Method",
        "SubClass-Of",
        "recurrent neural network:Method"
      ],
      [
        "DA - RNNs:Method",
        "Used-For",
        "semantic labeling:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "LSTM:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "Attention mechanisms:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "Attention mechanisms:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "RAN:Method",
        "Synonym-Of",
        "Reverse Attention Network:Method"
      ],
      [
        "Pyramid Attention Network:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "self - attention mechanism:Method",
        "Part-Of",
        "dual attention network:Method"
      ],
      [
        "dual attention network:Method",
        "Used-For",
        "scene segmentation:Task"
      ],
      [
        "attention mechanisms:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "OCNet:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "EMANet:Method",
        "Synonym-Of",
        "Expectation - Maximization Attention:Method"
      ],
      [
        "CCNet:Method",
        "Synonym-Of",
        "Criss - Cross Attention Network:Method"
      ],
      [
        "pointwise spatial attention network:Method",
        "Used-For",
        "scene parsing:Task"
      ],
      [
        "DFN:Method",
        "Synonym-Of",
        "discriminative feature network:Method"
      ],
      [
        "Smooth Network:Method",
        "Part-Of",
        "discriminative feature network:Method"
      ],
      [
        "Border Network:Method",
        "Part-Of",
        "discriminative feature network:Method"
      ],
      [
        "global average pooling:Method",
        "Part-Of",
        "Smooth Network:Method"
      ],
      [
        "Channel Attention Block:Method",
        "Part-Of",
        "Smooth Network:Method"
      ],
      [
        "GANs:Method",
        "Used-For",
        "computer vision:Task"
      ],
      [
        "image segmentation:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "GANs:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "adversarial network:Method",
        "Part-Of",
        "convolutional semantic segmentation network:Method"
      ],
      [
        "adversarial network:Method",
        "Used-For",
        "segmentation maps:Task"
      ],
      [
        "GANs:Method",
        "Used-For",
        "semi - weakly supervised semantic segmentation:Task"
      ],
      [
        "FCN discriminator:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "FCN:Method",
        "Used-For",
        "generate segmentation label maps:Task"
      ],
      [
        "Cell Image Segmentation:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "GANs:Method",
        "Part-Of",
        "Cell Image Segmentation:Method"
      ],
      [
        "ACMs:Method",
        "Synonym-Of",
        "Active Contour Models:Method"
      ],
      [
        "FCN:Method",
        "Used-For",
        "ventricle segmentation in cardiac MRI:Task"
      ],
      [
        "morphological active contours:Method",
        "Used-For",
        "microvascular image segmentation:Task"
      ],
      [
        "ACM:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "RNNs:Method",
        "Used-For",
        "ACMs:Method"
      ],
      [
        "Deep Active Lesion Segmentation:Method",
        "Used-For",
        "medical image segmentation:Task"
      ],
      [
        "DALS:Method",
        "Synonym-Of",
        "Deep Active Lesion Segmentation:Method"
      ],
      [
        "FCN:Method",
        "Part-Of",
        "Deep Active Lesion Segmentation:Method"
      ],
      [
        "DSAC:Method",
        "Synonym-Of",
        "Deep Structured Active Contours:Method"
      ],
      [
        "ACMs:Method",
        "Part-Of",
        "Deep Structured Active Contours:Method"
      ],
      [
        "structured prediction framework:Method",
        "Part-Of",
        "Deep Structured Active Contours:Method"
      ],
      [
        "FCNs:Method",
        "Part-Of",
        "Deep Structured Active Contours:Method"
      ],
      [
        "FCNs:Method",
        "Part-Of",
        "structured prediction framework:Method"
      ],
      [
        "ACMs:Method",
        "Part-Of",
        "structured prediction framework:Method"
      ],
      [
        "structured prediction framework:Method",
        "Used-For",
        "building instance segmentation:Task"
      ],
      [
        "DarNet:Method",
        "Synonym-Of",
        "Deep Active Ray Network:Method"
      ],
      [
        "ACM:Method",
        "Part-Of",
        "Deep Active Ray Network:Method"
      ],
      [
        "DCAC:Method",
        "Synonym-Of",
        "Deep Convolutional Active Contours:Method"
      ],
      [
        "DL:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "Context Encoding Network:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "EncNet:Method",
        "Synonym-Of",
        "Context Encoding Network:Method"
      ],
      [
        "Context Encoding Module:Method",
        "Part-Of",
        "Context Encoding Network:Method"
      ],
      [
        "RefineNet:Method",
        "SubClass-Of",
        "multi - path refinement network:Method"
      ],
      [
        "OCR:Method",
        "Synonym-Of",
        "Object - Contextual Representations \":Method"
      ],
      [
        "deep reinforcement learning:Method",
        "Used-For",
        "Seednet:Method"
      ],
      [
        "Seednet:Method",
        "Used-For",
        "interactive segmentation:Task"
      ],
      [
        "deep reinforcement learning:Method",
        "Used-For",
        "interactive segmentation:Task"
      ],
      [
        "enhancing low - level and high - level features fusion:Method",
        "Synonym-Of",
        "Exfuse:Method"
      ],
      [
        "DIS:Method",
        "Synonym-Of",
        "dual image segmentation:Method"
      ],
      [
        "Perspectiveaware scene parsing:Method",
        "Synonym-Of",
        "FoveaNet:Method"
      ],
      [
        "BiSeNet:Method",
        "Synonym-Of",
        "Bilateral segmentation network:Method"
      ],
      [
        "SPGNet:Method",
        "Synonym-Of",
        "Semantic Prediction Guidance for Scene Parsing:Method"
      ],
      [
        "AC - Net:Method",
        "Synonym-Of",
        "Adaptive context network:Method"
      ],
      [
        "DSSPN:Method",
        "Synonym-Of",
        "Dynamic - structured semantic propagation network:Method"
      ],
      [
        "SGR:Method",
        "Synonym-Of",
        "symbolic graph reasoning:Method"
      ],
      [
        "SAC:Method",
        "Synonym-Of",
        "Scale - adaptive convolutions:Method"
      ],
      [
        "UperNet:Method",
        "Synonym-Of",
        "Unified perceptual parsing:Method"
      ],
      [
        "Panoptic segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "Panoptic Feature Pyramid Network:Method",
        "Used-For",
        "Panoptic segmentation:Task"
      ],
      [
        "attention - guided network:Method",
        "Used-For",
        "Panoptic segmentation:Task"
      ],
      [
        "attention - guided network:Method",
        "Used-For",
        "Seamless Scene Segmentation:Task"
      ],
      [
        "Panoptic Feature Pyramid Network:Method",
        "Used-For",
        "Seamless Scene Segmentation:Task"
      ],
      [
        "DL:Method",
        "Used-For",
        "semantic segmentation:Task"
      ],
      [
        "DL:Method",
        "Used-For",
        "instance segmentation:Task"
      ],
      [
        "Data augmentation:Method",
        "Used-For",
        "medical image analysis:Task"
      ],
      [
        "VOC:Dataset",
        "Synonym-Of",
        "PASCAL Visual Object Classes:Dataset"
      ],
      [
        "PASCAL Visual Object Classes:Dataset",
        "Benchmark-For",
        "computer vision:Task"
      ],
      [
        "classification:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "segmentation:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "detection:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "action recognition:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "PASCAL Visual Object Classes:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "PASCAL Visual Object Classes:Dataset",
        "Benchmark-For",
        "segmentation:Task"
      ],
      [
        "PASCAL Visual Object Classes:Dataset",
        "Benchmark-For",
        "detection:Task"
      ],
      [
        "PASCAL Visual Object Classes:Dataset",
        "Benchmark-For",
        "action recognition:Task"
      ],
      [
        "PASCAL Visual Object Classes:Dataset",
        "Benchmark-For",
        "person layout:Task"
      ],
      [
        "COCO:Dataset",
        "Benchmark-For",
        "segmentation map:Task"
      ],
      [
        "Cityscapes:Dataset",
        "Benchmark-For",
        "semantic understanding of urban street scenes:Task"
      ],
      [
        "scene parsing:Method",
        "Evaluated-With",
        "ADE 2 0 K:Dataset"
      ],
      [
        "scene parsing:Method",
        "Evaluated-With",
        "MIT Scene Parsing:Dataset"
      ],
      [
        "scene parsing:Method",
        "Evaluated-With",
        "SceneParse 1 5 0:Dataset"
      ],
      [
        "BSD:Dataset",
        "Synonym-Of",
        "Berkeley Segmentation Dataset:Dataset"
      ],
      [
        "KITTI:Dataset",
        "Benchmark-For",
        "mobile robotics:Task"
      ],
      [
        "KITTI:Dataset",
        "Benchmark-For",
        "autonomous driving:Task"
      ],
      [
        "Semantic Boundaries Dataset:Dataset",
        "Benchmark-For",
        "image segmentation:Task"
      ],
      [
        "PASCAL Part:Dataset",
        "Benchmark-For",
        "image segmentation:Task"
      ],
      [
        "SYNTHIA:Dataset",
        "Benchmark-For",
        "image segmentation:Task"
      ],
      [
        "Adobes Portrait Segmentation:Dataset",
        "Benchmark-For",
        "image segmentation:Task"
      ],
      [
        "SBD:Dataset",
        "Synonym-Of",
        "Semantic Boundaries Dataset:Dataset"
      ],
      [
        "SUN RGB - D:Dataset",
        "Benchmark-For",
        "scene understanding:Task"
      ],
      [
        "WordNet:Dataset",
        "Compare-With",
        "ImageNet:Dataset"
      ],
      [
        "ScanNet:Dataset",
        "Benchmark-For",
        "3D camera poses:Task"
      ],
      [
        "ScanNet:Dataset",
        "Benchmark-For",
        "surface reconstructions:Task"
      ],
      [
        "ScanNet:Dataset",
        "Benchmark-For",
        "instancelevel semantic segmentations:Task"
      ],
      [
        "3D object classification:Task",
        "SubTask-Of",
        "3D scene understanding:Task"
      ],
      [
        "semantic voxel labeling:Task",
        "SubTask-Of",
        "3D scene understanding:Task"
      ],
      [
        "CAD model retrieval:Task",
        "SubTask-Of",
        "3D scene understanding:Task"
      ],
      [
        "ShapeNetCore:Dataset",
        "SubClass-Of",
        "ShapeNet:Dataset"
      ],
      [
        "DL:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "segmentation models:Method",
        "Used-For",
        "drones:Task"
      ],
      [
        "segmentation models:Method",
        "Used-For",
        "selfdriving cars:Task"
      ],
      [
        "segmentation models:Method",
        "Used-For",
        "robotics:Task"
      ],
      [
        "DL:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "FCN:Method",
        "SubClass-Of",
        "DL - based image segmentation model:Method"
      ],
      [
        "NYUD - v 2:Dataset",
        "Benchmark-For",
        "RGB - D segmentation:Task"
      ],
      [
        "SUN - RGBD:Dataset",
        "Benchmark-For",
        "RGB - D segmentation:Task"
      ],
      [
        "NYUD - v 2:Dataset",
        "Benchmark-For",
        "segmentation:Task"
      ],
      [
        "SUN - RGBD:Dataset",
        "Benchmark-For",
        "segmentation:Task"
      ],
      [
        "image segmentation:Task",
        "SubTask-Of",
        "segmentation:Task"
      ],
      [
        "image segmentation:Task",
        "Used-For",
        "medical image analysis:Task"
      ],
      [
        "self - supervised learning:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "dilated convolution:Method",
        "Used-For",
        "segmentation:Task"
      ],
      [
        "pointcloud segmentation:Task",
        "Used-For",
        "3D modeling:Task"
      ],
      [
        "pointcloud segmentation:Task",
        "Used-For",
        "self - driving cars:Task"
      ],
      [
        "pointcloud segmentation:Task",
        "Used-For",
        "robotics:Task"
      ],
      [
        "pointcloud segmentation:Task",
        "Used-For",
        "building modeling:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "CNN:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "FCN:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "RNN:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "dilated CNN:Method",
        "Used-For",
        "image segmentation:Task"
      ]
    ]
  },
  {
    "doc_id": "160009536",
    "chunk_id": 1,
    "content": [
      "In our experiments , we show how our method improves not only optical flow estimation , but also gesture recognition , offering a speed - accuracy trade - off more realistic for practical robot applications .",
      "The speech recognition modules installed in house service robots ( e.g. , DeepSpeech [ 4 ] or Google Speech API ) have achieved near human level performance .",
      "In order to achieve a more reliable human - robot interaction ( HRI ) , an alternative way to communicate with the robot is necessary .",
      "As with humans , gestures provide an alternative modality for communication , and thus , gesture recognition plays an important role in HRI . 1 is with the Graduate School of Information Science and Technology , The University of Tokyo , Japan .",
      "Improved optical flow estimation Gesture recognition Pick up object Open door … In computer vision , gestures can be recognized using an action recognition method [ 5 ] [ 6 ] .",
      "In this work , we present a gesture recognition pipeline ( Fig. 1 ) designed for human - robot interaction that achieves a good trade - off balance between speed and accuracy .",
      "Our contributions are : • Three novel optical flow estimation methods that incorporate attention to allow for better gesture recognition . • An optical - flow based gesture recognition pipeline with improved speed - accuracy trade - off , designed for human - robot interaction . • A self - generated dataset for human - robot interaction , MIBURI 1 , designed to command the robot to complete household tasks .   Nowadays , most human - robot interaction methods are speech recognition - based [ 7 ] [ 8 ] .",
      "Most existing works on human - robot interaction ( HRI ) via gestures focus on hand gesture recognition [ 1 1 ] .",
      "Recent approaches such as Temporal Segment Networks ( TSN ) [ 6 ] , and [ 1 4 ] further extended the two - stream approach .",
      "In optical flow - based gesture recognition , such as the previously described [ 1 4 ] and [ 6 ] , in order to estimate optical flow from the input images , researchers deployed traditional optical flow estimation methods , namely the TVL 1 [ 1 5 ] , and the work of Brox et al. [ 1 6 ] .",
      "The first attempt of a deep optical flow estimation network , FlowNet [ 1 7 ] , proposed two networks , namely FlowNetS and FlowNetC , and greatly improved the runtime of previous work .",
      "Zhu et al. [ 1 8 ] developed an optical flow estimation method called DenseNet by extending a deep network for image classification , and demonstrated that it outperforms other unsupervised methods .",
      "FlowNet 2. 0 [ 1 9 ] is a follow - up paper of FlowNet [ 1 7 ] .",
      "They stacked two optical flow estimation networks ( FlowNetS and FlowNetC ) and achieved state - of - the - art results comparable to traditional optical flow methods while running on 8 to 1 4 0 fps depending on the desired accuracy ( the higher the accuracy , the slower the runtime ) .",
      "Figure 1 depicts an overview of the basic pipeline of our gesture recognition method for human - robot interaction ( HRI ) .",
      "While most gesture/action recognition methods employ optical flow ( Sec. II - C ) , they mainly use traditional optical flow estimation .",
      "We argue that using deep learning for estimating optical flow results in a better trade - off between gesture recognition runtime and accuracy , and thus , is adequate for HRI .",
      "We also argue that adding an attention mechanism to deep learning optical flow estimation results in a higher accuracy , not only for optical flow estimation ( reduces blurriness in contours ) , but also for gesture recognition ( refines the user 's silhouette ) .",
      "Following the same criteria , we feed our optical flow to the TSN action recognition network [ 6 ] ( Sec. II - B ) , as it features state - of - the - art accuracy with short recognition time ."
    ],
    "relations": [
      [
        "DeepSpeech:Method",
        "SubClass-Of",
        "speech recognition modules:Method"
      ],
      [
        "Google Speech API:Method",
        "SubClass-Of",
        "speech recognition modules:Method"
      ],
      [
        "HRI:Task",
        "Synonym-Of",
        "human - robot interaction:Task"
      ],
      [
        "gesture recognition:Task",
        "Used-For",
        "HRI:Task"
      ],
      [
        "action recognition:Task",
        "SubTask-Of",
        "computer vision:Task"
      ],
      [
        "gesture recognition pipeline:Method",
        "Used-For",
        "human - robot interaction:Task"
      ],
      [
        "optical flow estimation methods:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "optical - flow based gesture recognition pipeline:Method",
        "Used-For",
        "human - robot interaction:Task"
      ],
      [
        "MIBURI:Dataset",
        "Benchmark-For",
        "human - robot interaction:Task"
      ],
      [
        "speech recognition - based:Method",
        "Used-For",
        "human - robot interaction:Task"
      ],
      [
        "HRI:Task",
        "Synonym-Of",
        "human - robot interaction:Task"
      ],
      [
        "hand gesture recognition:Task",
        "Used-For",
        "human - robot interaction:Task"
      ],
      [
        "TSN:Method",
        "Synonym-Of",
        "Temporal Segment Networks:Method"
      ],
      [
        "optical flow - based gesture recognition:Method",
        "Used-For",
        "estimate optical flow:Task"
      ],
      [
        "TVL 1:Method",
        "SubClass-Of",
        "optical flow estimation methods:Method"
      ],
      [
        "FlowNet:Method",
        "SubClass-Of",
        "deep optical flow estimation network:Method"
      ],
      [
        "FlowNetS:Method",
        "Part-Of",
        "deep optical flow estimation network:Method"
      ],
      [
        "FlowNetC:Method",
        "Part-Of",
        "deep optical flow estimation network:Method"
      ],
      [
        "DenseNet:Method",
        "SubClass-Of",
        "optical flow estimation method:Method"
      ],
      [
        "DenseNet:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "FlowNet 2. 0:Method",
        "SubClass-Of",
        "FlowNet:Method"
      ],
      [
        "FlowNetS:Method",
        "Part-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "FlowNetC:Method",
        "Part-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "optical flow estimation networks:Method",
        "Compare-With",
        "optical flow methods:Method"
      ],
      [
        "HRI:Task",
        "Synonym-Of",
        "human - robot interaction:Task"
      ],
      [
        "gesture recognition method:Method",
        "Used-For",
        "human - robot interaction:Task"
      ],
      [
        "optical flow:Method",
        "Part-Of",
        "gesture/action recognition methods:Method"
      ],
      [
        "optical flow estimation:Method",
        "Part-Of",
        "gesture/action recognition methods:Method"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "estimating optical flow:Task"
      ],
      [
        "deep learning:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "gesture recognition:Task",
        "Used-For",
        "HRI:Task"
      ],
      [
        "attention mechanism:Method",
        "Part-Of",
        "deep learning optical flow estimation:Method"
      ],
      [
        "deep learning optical flow estimation:Method",
        "Used-For",
        "optical flow estimation:Task"
      ],
      [
        "deep learning optical flow estimation:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "optical flow:Method",
        "Part-Of",
        "TSN action recognition network:Method"
      ],
      [
        "TSN action recognition network:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "feature extractors:Method",
        "Part-Of",
        "optical flow estimation:Method"
      ],
      [
        "feature extractors:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "feature extraction blocks:Method",
        "Part-Of",
        "optical flow estimation:Method"
      ]
    ]
  },
  {
    "doc_id": "160009536",
    "chunk_id": 2,
    "content": [
      "As shown in [ 1 8 ] , in optical flow estimation , feature extractors trained on image classification tasks are recommendable for unsupervised learning .",
      "Our novel optical flow estimation uses stronger feature extraction blocks ( i.e. blocks capable of extracting more discriminative features , tested in image classification tasks ) onto an existing baseline estimation network .",
      "For this purpose , we resorted to four feature extractors widely used for classifying the Im - ageNet dataset [ 2 1 ] : ResNet [ 2 2 ] , Inception [ 2 3 ] , ResNext ( 4x 3 2 d block ) and ResNext ( 4x 6 4 d block ) [ 2 4 ] .",
      "We replaced the simple convolutional layers present in FlowNetS [ 1 7 ] with an adaptation of ResNet , Inception and ResNext .",
      "We named these optical flow estimation networks as FlowNe - tRes , FlowNetInc , FlowNeXt 3 2 and FlowNeXt 6 4 respectively ( Fig. 2 ) .",
      "Once the feature extraction is improved , we target the optical - flow estimation .",
      "We named these optical flow estimation networks as AttFlowNetRes , AttFlowNetInc , AttFlowNext 3 2 , and At - tFlowNext 6 4 respectively .",
      "It is also noted that starting from here , we added additional upscaling back to the original resolution , as the final estimation resolution of FlowNetS [ 1 7 ] is only to the 1/ 4 of the original resolution .   One of the major challenges with optical flow estimation is predicting a correct general direction of the motion .",
      "We named these optical flow estimation networks MidFlowNetRes , MidFlowNetInc , MidFlowNext 3 2 , and MidFlowNext 6 4 respectively .",
      "We named these optical flow estimation networks as AttMidFlowNetRes , AttMidFlowNetInc , AttMidFlowNext 3 2 , and AttMidFlowNext 6 4 respectively .",
      "To the best of our knowledge , previous gesture recognition works do not discuss whether having a more accurate optical flow estimator results on more accurate gesture recognition .",
      "In each experiment , our proposed method for optical flow estimation and gesture recognition is trained end - to - end .",
      "For this , in order to evaluate the recognition of gestures to command a home service robot , we generated our own dataset , MIBURI .",
      "We evaluated our networks with the FlyingChairs dataset [ 1 7 ] , and the MPI Sintel dataset [ 2 9 ] .",
      "FlyingChairs [ 1 7 ] is a synthetic dataset designed specifically for training CNNs to estimate optical flow .",
      "For training , we used exclusively FlyingChairs train set , since MPI Sintel is too small for large - scale deep learning training .",
      "Then , FlyingChairs is tested in its test set , and both Sintel results are on their respective train sets . 1 ) Discussion : Stronger feature extractors generalize better : Our base networks with stronger feature extractors perform similarly to FlowNetS and FlowNetC on our training dataset , Fly - ingChairs , but outperform both of these networks on both of the Sintel passes .",
      "This is because , while FlowNetS and FlowNetC achieved a good EPE on FlyingChairs , they do not generalize well to other datasets .",
      "Stronger feature extractors are more robust to noise : Although some related works , e.g. MSCSL , achieved better Sintel clean results , we observed that our base networks achieved better Sintel final results ."
    ],
    "relations": [
      [
        "ResNet:Method",
        "Evaluated-With",
        "Im - ageNet:Dataset"
      ],
      [
        "Inception:Method",
        "Evaluated-With",
        "Im - ageNet:Dataset"
      ],
      [
        "ResNext ( 4x 3 2 d block ):Method",
        "Evaluated-With",
        "Im - ageNet:Dataset"
      ],
      [
        "ResNext ( 4x 6 4 d block ):Method",
        "Evaluated-With",
        "Im - ageNet:Dataset"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "FlowNetS:Method"
      ],
      [
        "ResNext:Method",
        "Part-Of",
        "FlowNetS:Method"
      ],
      [
        "Inception:Method",
        "Part-Of",
        "FlowNetS:Method"
      ],
      [
        "ResNet:Method",
        "Part-Of",
        "FlowNetS:Method"
      ],
      [
        "FlowNe - tRes:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "FlowNetInc:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "FlowNeXt 3 2:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "FlowNeXt 6 4:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "feature extraction:Task",
        "Used-For",
        "optical - flow estimation:Task"
      ],
      [
        "AttFlowNetRes:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "AttFlowNetInc:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "AttFlowNext 3 2:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "At - tFlowNext 6 4:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "MidFlowNetRes:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "MidFlowNetInc:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "MidFlowNext 3 2:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "MidFlowNext 6 4:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "AttMidFlowNetRes:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "AttMidFlowNetInc:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "AttMidFlowNext 3 2:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "AttMidFlowNext 6 4:Method",
        "SubClass-Of",
        "optical flow estimation networks:Method"
      ],
      [
        "optical flow estimator:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "CNNs:Method",
        "Trained-With",
        "FlyingChairs:Dataset"
      ],
      [
        "CNNs:Method",
        "Used-For",
        "estimate optical flow:Task"
      ],
      [
        "FlyingChairs:Dataset",
        "Benchmark-For",
        "estimate optical flow:Task"
      ],
      [
        "deep learning:Method",
        "Trained-With",
        "FlyingChairs:Dataset"
      ],
      [
        "FlowNetS:Method",
        "Evaluated-With",
        "Fly - ingChairs:Dataset"
      ],
      [
        "FlowNetC:Method",
        "Evaluated-With",
        "Fly - ingChairs:Dataset"
      ],
      [
        "FlowNetS:Method",
        "Evaluated-With",
        "Sintel:Dataset"
      ],
      [
        "FlowNetC:Method",
        "Evaluated-With",
        "Sintel:Dataset"
      ],
      [
        "FlowNetC:Method",
        "Evaluated-With",
        "FlyingChairs:Dataset"
      ],
      [
        "FlowNetS:Method",
        "Evaluated-With",
        "FlyingChairs:Dataset"
      ],
      [
        "MSCSL:Method",
        "Evaluated-With",
        "Sintel:Dataset"
      ],
      [
        "feature extractors:Method",
        "Part-Of",
        "optical flow:Method"
      ],
      [
        "Sintel:Dataset",
        "Compare-With",
        "FlyingChairs:Dataset"
      ]
    ]
  },
  {
    "doc_id": "160009536",
    "chunk_id": 3,
    "content": [
      "This proves that by adapting stronger feature extractors onto learning optical flow , the networks can understand motion better and ignore the additional atmospheric effects and motion blurs included in the final pass better , thus obtaining closer results between the Sintel clean and Sintel final benchmarks .",
      "Our method outperforms others in FlyingChairs : By adding the attention mechanism , midway estimations and attention - midway combined , we significantly outperformed other methods on the training set , FlyingChairs , with an improvement of between 2 0 % to 4 5 % .",
      "We think it is because of the dataset difference between Sintel and Fly - ingChairs , also discussed in [ 1 7 ] .",
      "Sintel has larger motions than FlyingChairs .",
      "Hence , when we reached very accurate FlyingChairs estimations , which feature more smaller motions , we will suffer on Sintel .",
      "Table IV shows the gesture recognition accuracy when using our optical flow estimation methods .",
      "For that , we used our MIBURI dataset and the Isolated Gesture Recognition Challenge ( IGR ) dataset .",
      "The IGR dataset ( in ICPR 2 0 1 6 ) was derived from one of the popular gesture recognition datasets , the ChaLearn Gesture Dataset 2 0 1 1 [ 3 0 ] .",
      "We chose the optical flow branch of the Temporal Segment Network ( TSN ) [ 6 ] as our gesture recognition method , since its performance is state - of - the - art and the speed - accuracy trade - off is well balanced .",
      "We compared it to three baselines : First , improved dense trajectories ( iDT ) [ 3 1 ] with Fisher vector ( FV ) encoding [ 3 2 ] and support vector machines ( SVM ) classifier , which combined were the state of the art before deep learning [ 3 1 ] .",
      "Second , TSN ( default ) uses traditional ( i.e. , non - deep learning - based ) optical flow estimation [ 1 5 ] .",
      "Third , TSN/FlowNetS is TSN 's optical flow branch replaced with optical flow estimation from FlowNetS [ 1 7 ] .",
      "Attention improves gesture recognition : Adding attention to our deep learning - based networks translated to a better gesture recognition performance in both datasets .",
      "This effect can be observed for TSN/FlowNeXt{ 3 2 / 6 4 } vs TSN/AttFlowNeXt{ 3 2 / 6 4 } and TSN/MidFlowNeXt{ 3 2 / 6 4 } vs TSN/AttMidFlowNext{ 3 2 / 6 4 }. As we hypothesized , since attention provides a more refined human silhouette , human motion is better represented , and thus , the performance of optical flow - based gesture recognition methods is improved .",
      "Better EPE does not necessarily imply better gesture recognition : In our experiments with MIBURI , improvement in optical flow and gesture recognition is correlated .",
      "For ChaLearn IGR on the other hand , in spite of being outperformed in optical flow estimation by our networks , TSN/FlowNetS achieves better recognition accuracy than our base .",
      "Our method improves the speed - accuracy trade - off : Compared to traditional optical flow estimation methods , deep learning - based methods allow for a faster gesture recognition , which has a lot of impact in HRI .",
      "With our MIBURI dataset , comparing TSN/FlowNeXt{ 3 2 / 6 4 } vs TSN/AttFlowNeXt{ 3 2 / 6 4 } , adding attention improved recognition accuracy by 7% to 1 0 % while runtime decreases , achieving the best speed - accuracy trade - off .",
      "In this paper , we presented a novel optical flow estimation method and evaluated it in a gesture recognition pipeline for human - robot interaction ."
    ],
    "relations": [
      [
        "FlyingChairs:Dataset",
        "Compare-With",
        "Sintel:Dataset"
      ],
      [
        "optical flow estimation methods:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "IGR:Dataset",
        "Synonym-Of",
        "Isolated Gesture Recognition Challenge:Dataset"
      ],
      [
        "IGR dataset:Dataset",
        "Benchmark-For",
        "gesture recognition:Task"
      ],
      [
        "ChaLearn Gesture Dataset 2 0 1 1:Dataset",
        "Benchmark-For",
        "gesture recognition:Task"
      ],
      [
        "TSN:Method",
        "Synonym-Of",
        "Temporal Segment Network:Method"
      ],
      [
        "optical flow branch:Method",
        "Part-Of",
        "Temporal Segment Network:Method"
      ],
      [
        "optical flow branch:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "iDT:Method",
        "Synonym-Of",
        "dense trajectories:Method"
      ],
      [
        "Fisher vector:Method",
        "Part-Of",
        "dense trajectories:Method"
      ],
      [
        "FV:Method",
        "Synonym-Of",
        "Fisher vector:Method"
      ],
      [
        "SVM:Method",
        "Synonym-Of",
        "support vector machines:Method"
      ],
      [
        "TSN:Method",
        "Used-For",
        "optical flow estimation:Method"
      ],
      [
        "optical flow branch:Method",
        "Part-Of",
        "TSN:Method"
      ],
      [
        "TSN/FlowNetS:Method",
        "SubClass-Of",
        "optical flow branch:Method"
      ],
      [
        "optical flow estimation:Method",
        "Part-Of",
        "FlowNetS:Method"
      ],
      [
        "Attention:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "attention:Method",
        "Part-Of",
        "deep learning - based networks:Method"
      ],
      [
        "deep learning - based networks:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "TSN/FlowNeXt{:Method",
        "Compare-With",
        "TSN/AttFlowNeXt{:Method"
      ],
      [
        "TSN/MidFlowNeXt{:Method",
        "Compare-With",
        "TSN/AttMidFlowNext{:Method"
      ],
      [
        "attention:Method",
        "Part-Of",
        "optical flow - based gesture recognition methods:Method"
      ],
      [
        "MIBURI:Dataset",
        "Benchmark-For",
        "gesture recognition:Task"
      ],
      [
        "optical flow estimation:Method",
        "Evaluated-With",
        "ChaLearn IGR:Dataset"
      ],
      [
        "TSN/FlowNetS:Method",
        "Evaluated-With",
        "ChaLearn IGR:Dataset"
      ],
      [
        "TSN/FlowNetS:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "optical flow estimation:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "ChaLearn IGR:Dataset",
        "Benchmark-For",
        "recognition:Task"
      ],
      [
        "deep learning - based methods:Method",
        "Compare-With",
        "optical flow estimation:Method"
      ],
      [
        "deep learning - based methods:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "deep learning - based methods:Method",
        "Used-For",
        "HRI:Task"
      ],
      [
        "TSN/FlowNeXt{:Method",
        "Evaluated-With",
        "MIBURI:Dataset"
      ],
      [
        "TSN/AttFlowNeXt{:Method",
        "Evaluated-With",
        "MIBURI:Dataset"
      ],
      [
        "TSN/FlowNeXt{:Method",
        "Compare-With",
        "TSN/AttFlowNeXt{:Method"
      ],
      [
        "TSN/FlowNeXt{:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "TSN/AttFlowNeXt{:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "attention:Method",
        "Used-For",
        "recognition:Task"
      ],
      [
        "optical flow estimation:Method",
        "Used-For",
        "gesture recognition:Task"
      ],
      [
        "optical flow estimation:Method",
        "Used-For",
        "human - robot interaction:Task"
      ]
    ]
  },
  {
    "doc_id": "202888938",
    "chunk_id": 1,
    "content": [
      "To address this deficiency , we develop a new HSI classification method based on the recently proposed Graph Convolutional Network ( GCN ) , as it can flexibly encode the relations among arbitrarily structured non - Euclidean data .",
      "Different from traditional GCN , there are two novel strategies adopted by our method to further exploit the contextual relations for accurate HSI classification .",
      "H YPERSPECTRAL image ( HSI ) has recently received considerable attention in a variety of applications such as military target detection , mineral identification , and disaster prevention [ 1 ] .",
      "The earlystaged algorithms are mainly based on the simple combination of spectral signatures and conventional pattern recognition methods , such as nearest neighbor classifier and Support Vector Machines ( SVM ) [ 3 ] , [ 4 ] .",
      "After that , Markov Random Field ( MRF ) [ 8 ] , which is an undirected graphical model [ 9 ] , became a popular approach to include spatial context for HSI classification .",
      "For instance , in [ 1 0 ] , a relative homogeneity index for each pixel is introduced in MRF - based classification to determine an appropriate weighting coefficient for the contextual contribution .",
      "Apart from this , a novel framework combining Support Vector Machine ( SVM ) and MRF is proposed for contextual HSI classification [ 1 1 ] .",
      "Additionally , multiple kernel learning [ 1 4 ] based on spatial context is proposed to improve the classification performance of SVM classifier on hyperspectral data .",
      "In order to effectively and precisely exploit the contextual relations in HSI , in this paper , we propose a novel ' Context - Aware Dynamic Graph Convolutional Network ' ( CAD - GCN ) which includes the following three key techniques : ( 1 ) The incorporation of Graph Convolutional Network ( GCN ) for sufficiently exploiting contextual relations among pixels ; ( 2 ) The employment of graph projection and re - projection framework for exploring long range contextual relations ; and ( 3 ) The utilization of dynamic graph refinement for accurately characterizing contextual relations and timely finding precise region representations .",
      "Specifically , in our CAD - GCN , the recently proposed GCN [ 1 7 ] is utilized .",
      "GCN is the extension of Convolutional Neural Network ( CNN ) for the non - grid data , and is able to aggregate features and propagate information across graph nodes .",
      "Consequently , the convolution operation of GCN is adaptively dominated by the neighborhood structure and can be applied to the non - Euclidean irregular data based on the graph which encodes contextual relations among graph nodes .",
      "Nevertheless , the contextual relations revealed by a predefined fixed graph [ 1 7 ] for implementing GCN is still inadequate for HSI classification .",
      "In this section , we review some representative works on HSI classification and GCN , as they are related to this work .",
      "Many classical HSI classification approaches are only based on spectral information [ 3 ] , [ 2 2 ] and ignore the crucial spatial information contained in HSI , which may decrease the classification performance [ 9 ] .",
      "Since then , AP and its extensions , including extended AP [ 3 4 ] and extended multi - attribute profiles [ 3 5 ] , have attracted increasing attention in HSI classification . 3 ) Superpixel establishment .",
      "Recently , some works focus on developing segmentationbased methods for HSI classification with superpixel technique [ 3 6 ] , [ 3 7 ] , in order to jointly combine the spectral - spatial correlations and discrimination to improve classification performance .",
      "For instance , in [ 3 8 ] , superpixel technique is utilized to generate homogeneous region before constructing a graph on superpixels , which produces satisfactory classification results .",
      "In additional to spatial methods , graph convolution can also be defined by spectral methods via convolution theorem .",
      "As the pioneering work of spectral methods , spectral CNN [ 4 1 ] converts signals defined in vertex domain into spectral domain by leveraging graph Fourier transform , where the convolution kernel is taken as a set of learnable coefficients associated with Fourier bases ( i.e. , the eigenvectors of Laplacian matrix ) .",
      "Subsequently , ChebyNet [ 4 2 ] considers the convolution kernel as a polynomial function of the diagonal matrix containing the eigenvalues of Laplacian matrix .",
      "Afterwards , GCN [ 1 7 ] was proposed by Kipf and Welling via using a localized first - order approximation to ChebyNet , which brings about more efficient filtering operations than spectral CNN .",
      "Recently , GCN gains remarkable success in processing graph - structured data and has been widely adopted in many areas , such as social network mining [ 4 3 ] , recommendation system [ 4 4 ] , natural language processing [ 4 5 ] , and scene understanding [ 4 6 ] .",
      "Due to the effectiveness of GCN in handling the non - Euclidean data , we plan to employ GCN to capture the contextual relations among pixels in HSI ."
    ],
    "relations": [
      [
        "Graph Convolutional Network:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Graph Convolutional Network:Method"
      ],
      [
        "GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "nearest neighbor classifier:Method",
        "SubClass-Of",
        "pattern recognition methods:Method"
      ],
      [
        "Support Vector Machines:Method",
        "SubClass-Of",
        "pattern recognition methods:Method"
      ],
      [
        "SVM:Method",
        "Synonym-Of",
        "Support Vector Machines:Method"
      ],
      [
        "MRF:Method",
        "Synonym-Of",
        "Markov Random Field:Method"
      ],
      [
        "Markov Random Field:Method",
        "SubClass-Of",
        "undirected graphical model:Method"
      ],
      [
        "Markov Random Field:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "MRF:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "SVM:Method",
        "Synonym-Of",
        "Support Vector Machine:Method"
      ],
      [
        "Support Vector Machine:Method",
        "Used-For",
        "contextual HSI classification:Task"
      ],
      [
        "MRF:Method",
        "Used-For",
        "contextual HSI classification:Task"
      ],
      [
        "SVM:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Synonym-Of",
        "Context - Aware Dynamic Graph Convolutional Network:Method"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Graph Convolutional Network:Method"
      ],
      [
        "dynamic graph refinement:Method",
        "Used-For",
        "characterizing contextual relations:Task"
      ],
      [
        "dynamic graph refinement:Method",
        "Used-For",
        "finding precise region representations:Task"
      ],
      [
        "GCN:Method",
        "Part-Of",
        "CAD - GCN:Method"
      ],
      [
        "CNN:Method",
        "Synonym-Of",
        "Convolutional Neural Network:Method"
      ],
      [
        "GCN:Method",
        "SubClass-Of",
        "Convolutional Neural Network:Method"
      ],
      [
        "convolution operation:Method",
        "Part-Of",
        "GCN:Method"
      ],
      [
        "GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "superpixel technique:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "superpixel technique:Method",
        "Used-For",
        "generate homogeneous region:Task"
      ],
      [
        "graph convolution:Method",
        "SubClass-Of",
        "convolution:Method"
      ],
      [
        "graph Fourier transform:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "convolution kernel:Method",
        "Part-Of",
        "graph Fourier transform:Method"
      ],
      [
        "convolution kernel:Method",
        "Part-Of",
        "ChebyNet:Method"
      ],
      [
        "first - order approximation:Method",
        "Part-Of",
        "ChebyNet:Method"
      ],
      [
        "ChebyNet:Method",
        "Compare-With",
        "spectral CNN:Method"
      ],
      [
        "GCN:Method",
        "Used-For",
        "social network mining:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "recommendation system:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "natural language processing:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "scene understanding:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "graph convolution:Method",
        "Part-Of",
        "contextaware dynamic GCN:Method"
      ],
      [
        "GCN:Method",
        "SubClass-Of",
        "multi - layer neural network:Method"
      ],
      [
        "softplus:Method",
        "Synonym-Of",
        "non - linear operation:Method"
      ],
      [
        "convolution operation:Method",
        "Part-Of",
        "GCN:Method"
      ],
      [
        "softplus:Method",
        "Part-Of",
        "CAD - GCN:Method"
      ],
      [
        "SLIC:Method",
        "Synonym-Of",
        "Simple Linear Iterative Clustering:Method"
      ],
      [
        "Simple Linear Iterative Clustering:Method",
        "Used-For",
        "image segmentation:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "HSI Classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ]
    ]
  },
  {
    "doc_id": "202888938",
    "chunk_id": 2,
    "content": [
      "To the best of our knowledge , only one prior work has employed GCN for HSI classification , i.e. , [ 4 7 ] .",
      "To address these problems , we proposed a novel contextaware dynamic GCN which dynamically refines the graph along with graph convolution process and captures long range contextual relations among the image pixels via using a graph projection technique .",
      "The critical operations in the proposed CAD - GCN will be detailed by presenting the GCN backbone ( Section III - A ) , explaining the graph projection with pixelto - region assignment ( Section III - B ) , describing the dynamic graph refinement ( Section III - C ) , and elaborating the graph re - projection with region - to - pixel assignment ( Section III - D ) .",
      "Inspired by CNN , GCN [ 1 7 ] is a multi - layer neural network which directly operates on a graph and aims to extract highlevel features through aggregating feature information from the neighborhoods of graph nodes .",
      "In our CAD - GCN model , the first - order neighborhood is considered , i.e. , K = 1 , and thus Eq. ( 4 ) turns to a linear function on the graph Laplacian spectrum with respect to L. Afterwards , a neural network based on graph convolutions can be built by stacking multiple convolutional layers in the form of Eq. ( 4 ) , where each layer is followed by an elementwise non - linear operation ( i.e. , softplus ( · ) [ 4 8 ] ) .",
      "To solve this deficiency , Kipf and Welling [ 1 7 ] performed the re - normalization trick As a result , the convolution operation of GCN model can then be expressed as where H ( l ) denotes the output of the l th layer , σ ( · ) represents an activation function , such as the softplus function [ 4 8 ] used in our proposed CAD - GCN , and W ( l ) is the trainable weight matrix involved in the l th layer .",
      "Specifically , the Simple Linear Iterative Clustering ( SLIC ) algorithm [ 5 0 ] , which has been widely used for image segmentation , is employed to obtain the initial regions .",
      "Then Eq. ( 1 1 ) can be rewritten as where h ( l ) i is the representation of x i generated by the l th layer with h During graph construction , connections among the regions from different classes may be incorporated , which will lead to the aggregation of inter - class feature information and further degrade the discriminability of graph convolution results .",
      "With the edge filter , the graph convolutional layer can then be reformulated as with H ( 0 ) = X. After conducting the dynamic graph convolution on the region level , we need to re - project the new region features ( i.e. , the learned graph representation ) H ( L ) back into 2D image with grids of pixels , and this process is called graph re - projection , Specifically , the region - to - pixel assignment is accomplished by linearly interpolating pixel features based on the soft assignment matrix P , namely PH ( L ) , where L denotes the number of graph convolutional layers .",
      "With the region - to - pixel assignment , the output of our proposed CAD - GCN can be obtained as Algorithm 1 The Proposed CAD - GCN for HSI Classification Input : Input image ; number of iterations T ; learning rate η ; number of graph convolutional layers L ; 1 : Initialize the anchor point matrix V with SLIC algorithm ; 2 : // Train the CAD - GCN model 3 : for t = 1 to T do 4 : Learn the region features X through Eq. ( 8) and Eq. ( 9 ) ; 5 : Dynamically refine the graph A ( l ) using Eq. ( 1 2 ) and Eq. ( 1 3 ) along with the graph convolution operation of Eq. ( 1 4 ) ; 6 : Interpolate the region features back into the original 2D grids by Eq. ( 1 5 ) ; 7 : Calculate the error term according to Eq. ( 1 6 ) , and update the weight matrices W ( l ) ( 1 ≤ l ≤ L ) using fullbatch gradient descent ; 8 : end for 9 : Conduct label prediction via Eq. ( 1 4 ) and Eq. ( 1 5 ) ; Output : Predicted label for each pixel .",
      "Algorithm 1 shows the summarization of our proposed CAD - GCN classification method .",
      "To test the effectiveness of the proposed CAD - GCN model , in this section , we conduct exhaustive experiments on three real - world benchmark datasets , namely Indian Pines , University of Pavia , and Salinas .",
      "The performance of our proposed CAD - GCN is evaluated on three real - world benchmark datasets , i.e. , the Indian Pines 1 , Grass - trees 3 0 7 0 0 7 Grass - pasture - mowed 1 5 1 3 8 Hay -windrowed   3 0   4 4 8   9   Oats   1 5   5   1 0   Soybean - notill   3 0   9 4 2   1 1   Soybean - mintill   3 0   2 4 2 5   1 2   Soybean - clean   3 0   5 6 3   1 3   Wheat   3 0   1 7 5   1 4   Woods   3 0   1 2 3 5   1 5 Buildings - grass - trees - drives 3 0 3 5 6 1 6 Stone - steel - towers 3 0 6 3 the University of Pavia 2 , and the Salinas 3 , which will be introduced below . 1 ) Indian Pines : The Indian Pines dataset was gathered by Airborne Visible/Infrared Imaging Spectrometer sensor in 1 9 9 2 , which records north - western India .",
      "The amounts of labeled and unlabeled pixels of various classes are listed in Table I . 2 ) University of Pavia : The University of Pavia dataset captures the Pavia University of Italy with the ROSIS sensor .",
      "Table II shows the amounts of labeled and unlabeled pixels of each class . 3 ) Salinas : The Salinas dataset is another classic HSI which is collected by the AVIRIS sensor over Salinas Valley , California .",
      "In our experiments , the proposed CAD - GCN algorithm is implemented by TensorFlow with Adam optimizer .",
      "For all the adopted three datasets mentioned in Section IV - A , usually 3 0 To evaluate the classification ability of our proposed CAD - GCN , other recent state - of - the - art HSI classification methods are also utilized for comparison .",
      "Specifically , we employ two GCN - based methods , i.e. , Graph Convolutional Network ( GCN ) [ 1 7 ] and Spectral - Spatial Graph Convolutional Network ( S 2 GCN ) [ 4 7 ] , together with two CNN - based methods , i.e. , Recurrent 2D - CNN ( R - 2 D - CNN ) [ 5 1 ] and CNN - Pixel - Pair Features ( CNN - PPF ) [ 5 2 ] .",
      "Meanwhile , we also compare the proposed CAD - GCN with two traditional HSI classification methods , namely Joint collaborative representation and SVM with Decision Fusion ( JSDF ) [ 5 4 ] and Multiple Feature Learning ( MFL ) [ 5 3 ] , respectively .",
      "To show the effectiveness of our proposed CAD - GCN , here we quantitatively and qualitatively evaluate the classification performance by comparing CAD - GCN with the aforementioned baseline methods . 1 ) Results on the Indian Pines Dataset : The quantitative results acquired by different methods on the Indian Pines dataset are presented in Table IV , and the highest record regarding each class ( i.e. , each row ) has been highlighted in bold .",
      "Meanwhile , the proposed CAD - GCN acquires stable and very high classification accuracies on most of the land - cover classes .",
      "All these statistics demonstrate the effectiveness of our CAD - GCN in HSI classification .",
      "The classification maps generated by different methods on the Indian Pines dataset are exhibited in Fig. 6 .",
      "A visual inspection reveals that the proposed CAD - GCN method produces much more compact classification map and shows fewer misclassifications than other methods ."
    ],
    "relations": [
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Grass - trees:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Grass - pasture - mowed:Dataset"
      ],
      [
        "Adam optimizer:Method",
        "Part-Of",
        "CAD - GCN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "Graph Convolutional Network:Method",
        "SubClass-Of",
        "GCN - based methods:Method"
      ],
      [
        "Spectral - Spatial Graph Convolutional Network:Method",
        "SubClass-Of",
        "GCN - based methods:Method"
      ],
      [
        "GCN:Method",
        "Synonym-Of",
        "Graph Convolutional Network:Method"
      ],
      [
        "S 2 GCN:Method",
        "Synonym-Of",
        "Spectral - Spatial Graph Convolutional Network:Method"
      ],
      [
        "Recurrent 2D - CNN:Method",
        "SubClass-Of",
        "CNN - based methods:Method"
      ],
      [
        "CNN - Pixel - Pair Features:Method",
        "SubClass-Of",
        "CNN - based methods:Method"
      ],
      [
        "R - 2 D - CNN:Method",
        "Synonym-Of",
        "Recurrent 2D - CNN:Method"
      ],
      [
        "CNN - PPF:Method",
        "Synonym-Of",
        "CNN - Pixel - Pair Features:Method"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "Joint collaborative representation and SVM with Decision Fusion:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "Multiple Feature Learning:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "JSDF:Method",
        "Synonym-Of",
        "Joint collaborative representation and SVM with Decision Fusion:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "Joint collaborative representation and SVM with Decision Fusion:Method"
      ],
      [
        "MFL:Method",
        "Synonym-Of",
        "Multiple Feature Learning:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "Multiple Feature Learning:Method"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "Indian Pines:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "S 2 GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "R - 2 D - CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CNN - PPF:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "MFL:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "JSDF:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "JSDF:Method"
      ],
      [
        "JSDF:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "R - 2 D - CNN:Method",
        "SubClass-Of",
        "CNN - based methods:Method"
      ],
      [
        "CNN - PPF:Method",
        "SubClass-Of",
        "CNN - based methods:Method"
      ],
      [
        "CNN - based methods:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "R - 2 D - CNN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "CNN - PPF:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "convolutional kernels:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "University of Pavia:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "dynamic graph refinement operations:Method",
        "Part-Of",
        "CAD - GCN:Method"
      ],
      [
        "graph projection:Method",
        "Part-Of",
        "CAD - GCN:Method"
      ],
      [
        "S 2 GCN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "GCN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "S 2 GCN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ]
    ]
  },
  {
    "doc_id": "202888938",
    "chunk_id": 3,
    "content": [
      "More concretely , in the classification maps of GCN , S 2 GCN , R - 2 D - CNN , CNN - PPF , and MFL , the errors are almost uniformly distributed ( the salt - and - pepper effect in the homogeneous regions ) , while in the classification maps of JSDF and our proposed CAD - GCN , the errors only appear in some highly heterogeneous areas , where the spatial separability between classes is quite low .",
      "Moreover , by comparing CAD - GCN with JSDF , we can also find that JSDF produces more errors around class boundaries than our CAD - GCN method , which reveals the good discriminability of the proposed CAD - GCN in boundary regions . 2 ) Results on the University of Pavia Dataset : In Table V , different methods are compared on the aforementioned three datasets , where per - class accuracy , OA , AA , and Kappa coefficient are reported , and the highest value in each row is highlighted in bold .",
      "Compared with the two CNN - based methods ( i.e. , R - 2 D - CNN and CNN - PPF ) , the proposed CAD - GCN increases the OA by 9. 9 4 % and 3. 6 0 % , respectively , which suggests that the refined contextual relations captured by our CAD - GCN is superior to the spatial context characterized by the fixed convolutional kernels of CNN .",
      "Fig. 7 visualizes the classification results generated by the seven different methods on the University of Pavia dataset .",
      "As depicted in Fig. 7(h ) , the classification map of our proposed CAD - GCN are noticeably closer to the ground truth map ( see Fig. 7(a ) ) than other methods , which is consistent with previous results in Table V .",
      "Different from GCN and S 2 GCN , our CAD - GCN employs graph projection and dynamic graph refinement operations to effectively exploit the improved contextual relations of HSI .",
      "As a result , GCN and S 2 GCN which use the fixed coarse graph convolution produce more errors than our proposed CAD - GCN . 3 ) Results on the Salinas Dataset : Table VI presents the experimental results of different methods on the Salinas dataset .",
      "The proposed CAD - GCN is obviously superior to the CNNbased methods ( i.e. , R - 2 D - CNN and CNN - PPF ) and all the other competitors .",
      "For instance , in Table VI , CAD - GCN yields over 1 0 % higher OA than R - 2 D - CNN and approximately 8% higher OA than CNN - PPF .",
      "Especially in some classes such as ' Grapes untrained ' ( ID = 8) and ' Vineyard untrained ' ( ID = 1 5 ) , the class - specific accuracies of our proposed CAD - GCN are even approximately 2 0 % higher than those of the CNN - based methods . results obtained by different methods .",
      "It is observable that some areas in the classification map of our proposed CAD - GCN are less noisy than those of other methods , e.g. , the regions of ' Grapes untrained ' and ' Vineyard untrained ' , which is in consistence with the results listed in Table VI .",
      "To be specific , we vary the number of labeled examples per class form 5 to 3 0 with an interval of 5 , and report the OA acquired by all the methods on the Indian Pines , the University of Pavia , and the Salinas datasets ( see Fig. 9 ) .",
      "From the results , we can find that the proposed CAD - GCN consistently outperforms the GCN , S 2 GCN and all the other competitors on the three   There are several important hyperparameters that should be manually tuned in the designed CAD - GCN architecture .",
      "Here , we will evaluate in detail the sensitivity of the classification performance to different hyperparameter settings of the proposed CAD - GCN .",
      "The results on Indian Pines , University of Pavia , and Salinas datasets are shown in Fig. 1 0 .",
      "In order to obtain promising classification performance , we set the hyperparameters to T = 1 5 0 0 , η = 0.0 0 1 , u = 6 0 , β = 0.0 1 for the Indian Pines dataset , T = 5 0 0 , η = 0.0 0 1 , u = 2 1 0 , β = 0.0 5 for the University of Pavia dataset , and T = 2 0 0 0 , η = 0.0 0 0 1 , u = 1 1 0 , β = 0.0 2 for the Salinas dataset , respectively .",
      "To shed light on the contributions of these three components , every time we report the classification results of CAD - GCN without one of the three components on the three adopted datasets ( namely , the Indian Pines , the University of Pavia , and the Salinas ) .",
      "For simplicity , we adopt ' CAD - GCN - v 1 ' , ' CAD - GCN - v 2 ' , and ' CAD - GCN - v 3 ' to represent the reduced model by removing dynamic refinement of node similarities , the edge filter , and the graph projection framework , respectively .",
      "In order to reveal the advantage of our proposed CAD - GCN to the baselines in terms of efficiency , in Table X , we report the running time of different deep models , including GCN , S 2 GCN , R - 2 D - CNN , CNN - PPF , and the proposed CAD - GCN on three datasets ( i.e. , the Indian Pines , the University of Pavia , and the Salinas ) , where the number of labeled pixels IP   5 8   7 1   2 1 5 6   1 4 9 5   4 4   paviaU   1 7 8 3   1 8 0 3   2 2 7 2   1 5 4 5   9 0   Salinas   3 4 9 7   3 5 2 8   2 3 6 1   1 7 6 9   5 5 6 per class is kept identical to the experiments presented in Section IV - C. The codes for all methods are written in Python , and the running time is reported on a server with a 3. 6 0 - GHz Intel Xeon CPU with 2 6 4 GB of RAM and a Tesla P 4 0 GPU .",
      "Compared with the running time of GCN , S 2 GCN , our proposed CAD - GCN shows remarkably higher efficiency in large - scale datasets ( i.e. , the University of Pavia and the Salinas dataset ) , which is owing much to the employment of graph projection operation .",
      "In this paper , we have developed a novel Context - Aware Dynamic Graph Convolutional Network ( CAD - GCN ) for HSI classification .",
      "Therefore , the contextual relations among pixels can be gradually refined along with graph convolution , which significantly improves the performance of CAD - GCN on representation and classification of HSI .",
      "The experimental results on three realworld HSI datasets indicate that the proposed CAD - GCN is able to yield better performance when compared with the stateof - the - art HSI classification methods ."
    ],
    "relations": [
      [
        "GCN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "CNNbased methods:Method"
      ],
      [
        "CNN - PPF:Method",
        "SubClass-Of",
        "CNNbased methods:Method"
      ],
      [
        "R - 2 D - CNN:Method",
        "SubClass-Of",
        "CNNbased methods:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "R - 2 D - CNN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "CNN - PPF:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "CNN - based methods:Method"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "GCN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Compare-With",
        "S 2 GCN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "Indian Pines:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "University of Pavia:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "Salinas:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "CNN - PPF:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "R - 2 D - CNN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "S 2 GCN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "GCN:Method",
        "Evaluated-With",
        "Indian Pines:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "CNN - PPF:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "R - 2 D - CNN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "S 2 GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "CNN - PPF:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "R - 2 D - CNN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "S 2 GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "GCN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "S 2 GCN:Method",
        "Compare-With",
        "CAD - GCN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "S 2 GCN:Method",
        "Evaluated-With",
        "University of Pavia:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "S 2 GCN:Method",
        "Evaluated-With",
        "Salinas:Dataset"
      ],
      [
        "CAD - GCN:Method",
        "Synonym-Of",
        "Context - Aware Dynamic Graph Convolutional Network:Method"
      ],
      [
        "Context - Aware Dynamic Graph Convolutional Network:Method",
        "Used-For",
        "HSI classification:Task"
      ],
      [
        "graph convolution:Method",
        "Part-Of",
        "CAD - GCN:Method"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "classification of HSI:Task"
      ],
      [
        "CAD - GCN:Method",
        "Used-For",
        "HSI classification:Task"
      ]
    ]
  },
  {
    "doc_id": "244256",
    "chunk_id": 1,
    "content": [
      "Specifically relevant to ADAS application is the dramatic increase in accuracy of image object classification [ 1 6 , 2 2 , 2 1 , 1 2 ] and localization [ 2 0 , 2 6 , 1 0 , 9 , 1 9 ] in the last few years .",
      "R - CNN and its faster variants have become the state of the art in different object detection tasks .",
      "In this work , we leverage this method to establish a number of observations related to car detection on the challenging KITTI [ 8 ] dataset .",
      "Surprisingly , even shallow models like AlexNet provide high accuracy on the detection task .",
      "We do additional exploration of R - CNN based configurations in Section 7 We summarize our findings in the context of the related work in Section 8 , and we conclude in Section 9 .   Deformable parts models ( DPM ) were the state of the art for image object detection [ 7 ] before the emergence of deep convolutional neural nets .",
      "The R - CNN method uses selective search for object region proposal [ 1 0 ] .",
      "The proposed regions in an image are warped to a fixed size and fed into a classification network called R - CNN .",
      "Fast R - CNN was introduced to reuse the shared convolution features for the region proposals [ 9 ] .",
      "In Fast R - CNN [ 9 ] , the inference speed is still dominated by the region proposal in the selective search method .",
      "Inspired by the SPPNet [ 1 1 ] method , Faster R - CNN uses a region proposal network ( RPN ) to regress proposal boxes to ground truth boxes .",
      "The regions proposed by the RPN network is fed into the R - CNN network for classification .",
      "For example , the OverFeat [ 2 0 ] method predicts a single box for localization whereas the Multibox [ 5 , 2 3 ] method predicts multiple boxes in a class - agnostic way .",
      "The SPP method [ 1 1 ] uses shared convolutional feature maps for fast object detection .",
      "Deep neural networks are the backbone of most high - accuracy approaches to identifying objects such as cars in KITTI and similar datasets .",
      "A high - accuracy method for identifying objects in KITTI dataset is scale dependent pooling ( SDP ) combined with cascaded region classifiers ( CRC ) [ 2 5 ] .",
      "The crux of SDP+CRC lies in selecting a high - resolution CNN layer ( e.g. conv 3 3 in VGG 1 6 [ 2 1 ] ) or a heavily downsampled CNN layer ( e.g. conv 5 3 ) , depending on the resolution of each region proposal .",
      "By combining features from multiple convolution layers , they were able to achieve very high accuracy on KITTI 's object detection task .",
      "Another approach is Monocular 3D ( Mono 3 D ) which actually uses 2D images , but it aims to identify the pose of objects , with the goal of detecting objects as 3D bounding boxes .",
      "Like SDP+CRC , Mono 3 D is built around a version of R - CNN .",
      "There are also a number of anonymous and/or sparsely - explained submissions to the KITTI website 's leaderboard that are reportedly built on top of R - CNN .",
      "To build supervised 2D datasets such as ImageNet [ 4 ] and PASCAL [ 6 ] , a widely - used approach is to have mechanical turk workers annotate user - generated images and videos from websites such as Flickr or YouTube ."
    ],
    "relations": [
      [
        "R - CNN:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "KITTI:Dataset",
        "Benchmark-For",
        "car detection:Task"
      ],
      [
        "AlexNet:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "DPM:Method",
        "Synonym-Of",
        "Deformable parts models:Method"
      ],
      [
        "Deformable parts models:Method",
        "Used-For",
        "image object detection:Task"
      ],
      [
        "convolutional neural nets:Method",
        "Used-For",
        "image object detection:Task"
      ],
      [
        "selective search:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "Fast R - CNN:Method"
      ],
      [
        "Fast R - CNN:Method",
        "Used-For",
        "region proposals:Task"
      ],
      [
        "selective search:Method",
        "Part-Of",
        "Fast R - CNN:Method"
      ],
      [
        "Fast R - CNN:Method",
        "Used-For",
        "region proposal:Task"
      ],
      [
        "region proposal network:Method",
        "Part-Of",
        "Faster R - CNN:Method"
      ],
      [
        "RPN:Method",
        "Synonym-Of",
        "region proposal network:Method"
      ],
      [
        "RPN:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "OverFeat:Method",
        "Compare-With",
        "Multibox:Method"
      ],
      [
        "SPP:Method",
        "Used-For",
        "object detection:Task"
      ],
      [
        "Deep neural networks:Method",
        "Evaluated-With",
        "KITTI:Dataset"
      ],
      [
        "scale dependent pooling:Method",
        "Evaluated-With",
        "KITTI:Dataset"
      ],
      [
        "SDP:Method",
        "Synonym-Of",
        "scale dependent pooling:Method"
      ],
      [
        "cascaded region classifiers:Method",
        "Part-Of",
        "scale dependent pooling:Method"
      ],
      [
        "CRC:Method",
        "Synonym-Of",
        "cascaded region classifiers:Method"
      ],
      [
        "conv 3 3:Method",
        "Synonym-Of",
        "high - resolution CNN layer:Method"
      ],
      [
        "conv 3 3:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "conv 5 3:Method",
        "Synonym-Of",
        "downsampled CNN layer:Method"
      ],
      [
        "SDP+CRC:Method",
        "Used-For",
        "region proposal:Task"
      ],
      [
        "KITTI:Dataset",
        "Benchmark-For",
        "object detection:Task"
      ],
      [
        "Mono 3 D:Method",
        "Synonym-Of",
        "Monocular 3D:Method"
      ]
    ]
  },
  {
    "doc_id": "244256",
    "chunk_id": 2,
    "content": [
      "For example , in VGG 1 6 , for a standard input image size 2 2 4 x 2 2 4 , the features calculated by the first convolution layer is 2 2 4 x 2 2 4 which reduce to 1 4 x 1 4 at the output of conv 5 3 .",
      "Similarly for AlexNet , the convolution feature dimension reduces from 5 5 x 5 5 in conv 1 to 6x 6 in pool 5 .",
      "In some CNN architectures such as AlexNet [ 1 6 ] and VGG 1 6 [ 2 1 ] , the first fully - connected layer expects a specific height and width for its input data ( e.g. 6x 6 for AlexNet ) .",
      "At the CNN architecture level , an easy way around this is to design a CNN architecture that has global average pooling prior to the first FC layer -this approach was popularized in the Network - in - Network ( NiN ) architecture [ 1 7 ] , and it is now used in other architectures such as SqueezeNet [ 1 5 ] and ResNet architectures [ 1 2 ] .",
      "However , when using AlexNet or VGG 1 9 , the R - CNN authors developed a technique called ROI Pooling that allows any size input image to be used in concert with AlexNet/VGG FC layers .",
      "We briefly review how the region proposal network ( RPN ) in Faster R - CNN generate proposals [ 1 9 ] that will be useful later .",
      "RPN starts with convolution layers , which computes a high dimensional , low resolution feature map for the input image .",
      "As we will see later in this paper , reducing this distance , or relatively , increasing the \" anchor density \" will significantly increase the localization accuracy , thus improve the detection accuracy .   We train faster R - CNN networks built on the VGG 1 6 [ 2 1 ] and AlexNet [ 1 6 ] , pretrained on the ImageNet - 1 k [ 4 ] classification dataset .",
      "VGG 1 6 has sixteen convolution layers [ 2 1 ] and AlexNet has only five convolutional layers [ 1 6 ] .",
      "Rather than using the convolution features of the last pooled layer ( as is done in the original faster R - CNN paper ) , we use features from convolutional layers that are the bottom layers of the previous pooling layer .",
      "For VGG 1 6 and AlexNet , fully connected layers are used as the R - CNN branch .",
      "As the standard procedure introduced in faster R - CNN , we randomly sample 1 2 8 positive and 1 2 8 negative roi proposals per batch to train the R - CNN layer .",
      "A total of 7 0 K iterations are run during R - CNN training starting from imagenet pre - trained weights for the convolution layers .",
      "We use the KITTI object detection dataset .",
      "In KITTI 's evaluation criteria , proposal boxes having overlap with the ground truth or IoU greater than 7 0 % are counted as true detection for cars .",
      "The Faster R - CNN algorithm has been shown to deliver high accuracy on the PASCAL [ 6 ] dataset .",
      "Adapting that pipeline from PASCAL to KITTI poses a few natural challenges .",
      "First , the image sizes in the KITTI dataset is 1 2 4 2 x 3 7 5 pixels whereas the image sizes in PASCAL dataset is 5 0 0 pixels in the longest dimension ( many PASCAL images are 5 0 0 x 3 3 3 or 3 3 3 x 5 0 0 ) .",
      "We performed extensive design space search of Faster R - CNN configurations on the KITTI dataset .",
      "Our starting point is the VGG 1 6 network that has achieved high accuracy in both image classification [ 2 1 ] and localization [ 1 9 ] .",
      "KITTI images have a native resolution of 1 2 9 5 x 3 7 5 , so the default Faster R - CNN behavior is to resize KITTI images to 1 0 0 0 x 3 0 2 ."
    ],
    "relations": [
      [
        "Mono 3 D:Method",
        "Compare-With",
        "SDP+CRC:Method"
      ],
      [
        "Mono 3 D:Method",
        "SubClass-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN:Method",
        "Evaluated-With",
        "KITTI:Dataset"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "conv 5 3:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "AlexNet:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "VGG 1 6:Method",
        "SubClass-Of",
        "CNN:Method"
      ],
      [
        "fully - connected layer:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "fully - connected layer:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "fully - connected layer:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "global average pooling:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "FC layer:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "NiN:Method",
        "Synonym-Of",
        "Network - in - Network:Method"
      ],
      [
        "AlexNet:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "VGG 1 9:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "ROI Pooling:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "AlexNet/VGG FC layers:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "RPN:Method",
        "Synonym-Of",
        "region proposal network:Method"
      ],
      [
        "region proposal network:Method",
        "Part-Of",
        "Faster R - CNN:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "RPN:Method"
      ],
      [
        "VGG 1 6:Method",
        "Part-Of",
        "faster R - CNN:Method"
      ],
      [
        "AlexNet:Method",
        "Part-Of",
        "faster R - CNN:Method"
      ],
      [
        "AlexNet:Method",
        "Trained-With",
        "ImageNet - 1 k:Dataset"
      ],
      [
        "VGG 1 6:Method",
        "Trained-With",
        "ImageNet - 1 k:Dataset"
      ],
      [
        "ImageNet - 1 k:Dataset",
        "Benchmark-For",
        "classification:Task"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "convolutional layers:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "faster R - CNN:Method"
      ]
    ]
  },
  {
    "doc_id": "244256",
    "chunk_id": 3,
    "content": [
      "For example , the conv 5 3 activations -which serves as input to both the region proposal network ( RPN ) and the classification network -double in height and width .",
      "In the next section , we consider shallower networks with fewer layers of activation planes , which enables us to move to even higher input resolutions .   So far , we have upsampled the input image until we ran out of on - chip GPU memory when training R - CNN models with a VGG 1 6 - based feature representation .",
      "In VGG 1 6 's scheme for naming layers , conv 4 3 is the 1 0 th layer , and conv 5 3 is the 1 3 th layer in the CNN .",
      "We expected that the accuracy of R - CNN with conv 4 3 would be slightly lower than R - CNN with conv 5 3 , but as we show in Table 2 that the accuracy is higher with conv 4 3 by 5. 5 , 9. 4 , and 1 2 . 4 percentage - points for easy , medium , and hard detections .",
      "We initially considered using the earlier layers of VGG 1 6 as input to the Region Proposal Network .",
      "Besides depth , one of the differences between VGG 1 6 and AlexNet is that AlexNet downsamples more aggressively in the early layers -for example AlexNet has stride= 4 in the conv 1 layer ( 4x downsampling ) , while the conv 1 layer of VGG 1 6 has stride= 1 ( no downsampling ) .",
      "So , to evaluate this question of how using shallower ( < 1 0 conv layers ) network impacts accuracy , we use AlexNet instead of VGG 1 6 .",
      "We use conv 5 ( 5th layer ) activations as input to the R - CNN region - proposal and classification branches , and we report the results in Table 2 .",
      "With resolution of 2 0 0 0 x 6 0 4 for both AlexNet - conv 5 and VGG 1 6 - conv 4 3 , the VGG 1 6 - based configuration delivers significantly higher accuracy on easy , medium , and hard detections in Table 2 .",
      "We have additional memory available when running AlexNet with 2 0 0 0 x 6 0 4 input images , so we now try upsampling the AlexNet input images to 5 0 0 0 x 1 5 1 0 .",
      "In this configuration , on the easy detections , AlexNet with 5 0 0 0 x 1 5 1 0 input is within 0. 5 of a percentage - point of our best VGG 1 6 - based result so far .",
      "On medium and hard categories , VGG 1 6 conv 4 3 with an input resolution of 2 0 0 0 x 6 0 4 delivers higher accuracy than AlexNet with 2 0 0 0 x 6 0 4 or 5 0 0 0 x 1 5 1 0 input images .",
      "We also conduct a sweep of input image sizes applied to an AlexNet - based R - CNN model that uses conv 5 features .",
      "In the experiments with context window , in addition to the original R - CNN branch , an extra R - CNN branch is added that trains on the features extracted from context window .",
      "The original R - CNN features and the context R - CNN features are concatenated before classification .",
      "When applying the context window to an AlexNet - based R - CNN configuration , we find that the accuracy of all the categories improve as shown in Table 4 .",
      "We tested the AlexNet - based Faster R - CNN 's detection accuracy with different anchor box selection schemes , and the result is shown in Table 3 .",
      "The inference time using AlexNet conv 5 layer with input image size of 5 0 0 0 x 1 5 1 0 and VGG 1 6 conv 4 3 layer with input image size of 2 0 0 0 x 6 0 4 is 0. 3 4 s and 0. 6 s respectively .",
      "Inference times for other published high accuracy methods on the KITTI dataset are 3s for 3DOP [ 3 ] and 0. 4 s for SDP+CRC [ 2 5 ] .",
      "We have shown that input image resolution has a large impact on the accuracy of car detection using the faster R - CNN network ."
    ],
    "relations": [
      [
        "fully connected layers:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN layer:Method",
        "Part-Of",
        "faster R - CNN:Method"
      ],
      [
        "convolution:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN:Method",
        "Trained-With",
        "imagenet:Dataset"
      ],
      [
        "Faster R - CNN:Method",
        "Evaluated-With",
        "PASCAL:Dataset"
      ],
      [
        "KITTI:Dataset",
        "Compare-With",
        "PASCAL:Dataset"
      ],
      [
        "Faster R - CNN:Method",
        "Evaluated-With",
        "KITTI:Dataset"
      ],
      [
        "VGG 1 6:Method",
        "Used-For",
        "image classification:Task"
      ],
      [
        "VGG 1 6:Method",
        "Used-For",
        "localization:Task"
      ],
      [
        "RPN:Method",
        "Synonym-Of",
        "region proposal network:Method"
      ],
      [
        "region proposal network:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "VGG 1 6:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "conv 5 3:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "conv 4 3:Method",
        "Part-Of",
        "CNN:Method"
      ],
      [
        "R - CNN with conv 4 3:Method",
        "Compare-With",
        "R - CNN with conv 5 3:Method"
      ],
      [
        "VGG 1 6:Method",
        "Compare-With",
        "AlexNet:Method"
      ],
      [
        "conv 1 layer:Method",
        "Part-Of",
        "AlexNet:Method"
      ],
      [
        "conv 1 layer:Method",
        "Part-Of",
        "VGG 1 6:Method"
      ],
      [
        "AlexNet:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "conv 5:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN branch:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "R - CNN:Method",
        "Used-For",
        "classification:Task"
      ],
      [
        "AlexNet:Method",
        "Part-Of",
        "R - CNN:Method"
      ],
      [
        "AlexNet:Method",
        "Part-Of",
        "Faster R - CNN:Method"
      ],
      [
        "Faster R - CNN:Method",
        "Used-For",
        "detection:Task"
      ],
      [
        "SDP+CRC:Method",
        "Evaluated-With",
        "KITTI:Dataset"
      ],
      [
        "3DOP:Method",
        "Evaluated-With",
        "KITTI:Dataset"
      ],
      [
        "faster R - CNN:Method",
        "Used-For",
        "car detection:Task"
      ]
    ]
  }
]