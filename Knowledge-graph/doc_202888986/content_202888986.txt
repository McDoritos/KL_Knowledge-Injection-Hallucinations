As a result , our best model establishes new state - of - the - art results on the GLUE , RACE , and \squad benchmarks while having fewer parameters compared to BERT - large .
One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high - school English exams in China , the RACE test ( Lai et al. , 2 0 1 7 ) : the paper that originally describes the task and formulates the modeling challenge reports then state - of - the - art machine accuracy at 4 4 . 1 % ; the latest published result reports their model performance at 8 3 . 2 % ; the work we present here pushes it even higher to 8 9 . 4 % , a stunning 4 5 . 3 % improvement that is mainly attributable to our current ability to build high - performance pretrained language representations .
Table 1 and Fig. 1 show a typical example , where we simply increase the hidden size of BERT - large to be 2x larger and get worse results with this BERT - xlarge model .
Model Hidden Size Parameters RACE ( Accuracy ) BERT - large 1 0 2 4 3 3 4 M Existing solutions to the aforementioned problems include model parallelization ( Shoeybi et al. , 2 0 1 9 ) and clever memory management ( Chen et al. , 2 0 1 6 ; .
In this paper , we address all of the aforementioned problems , by designing A Lite BERT ( ALBERT ) architecture that has significantly fewer parameters than a traditional BERT architecture .
An ALBERT configuration similar to BERT - large has 1 8 x fewer parameters and can be trained about 1. 7 x faster .
To further improve the performance of ALBERT , we also introduce a self - supervised loss for sentence - order prediction ( SOP ) .
SOP primary focuses on inter - sentence coherence and is designed to address the ineffectiveness of the next sentence prediction ( NSP ) loss proposed in the original BERT .
As a result of these design decisions , we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT - large but achieve significantly better performance .
We establish new state - of - the - art results on the well - known GLUE , SQuAD , and RACE benchmarks for natural language understanding .
Specifically , we push the RACE accuracy to 8 9 . 4 % , the GLUE benchmark to 8 9 . 4 , and the F 1 score of SQuAD 2. 0 to 9 2 . 2 .   Learning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted ( Mikolov et al. , 2 0 1 3 ; Le & Mikolov , 2 0 1 4 ; Peters et al. , 2 0 1 8 ; Radford et al. , 2 0 1 8 ; .
Different from our observations , Dehghani et al. ( 2 0 1 8) show that networks with cross - layer parameter sharing ( Universal Transformer , UT ) get better performance on language modeling and subject - verb agreement than the standard transformer .
Very recently , Bai et al. ( 2 0 1 9 ) propose a Deep Equilibrium Model ( DQE ) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same .
Hao et al. ( 2 0 1 9 ) combine a parameter - sharing transformer with the standard one , which further increases the number of parameters of the standard transformer .
Our loss is most similar to the sentence ordering objective of Jernite et al. ( 2 0 1 7 ) , where sentence embeddings are learned in order to determine the ordering of two consecutive sentences .
In this section , we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture .
The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder ( Vaswani et al. , 2 0 1 7 ) with GELU nonlinearities ( Hendrycks & Gimpel , 2 0 1 6 ) .
There are three main contributions that ALBERT makes over the design choices of BERT .
In BERT , as well as subsequent modeling improvements such as XLNet and RoBERTa , the WordPiece embedding size E is tied with the hidden layer size H , i.e. , E ≡ H. This decision appears suboptimal for both modeling and practical reasons , as follows .
As such , untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs , which dictate that H E. From a practical perspective , natural language processing usually require the vocabulary size V to be large . 1 If E ≡ H , then increasing H increases the size of the embedding matrix , which has size V × E. This can easily result in a model with billions of parameters , most of which are only updated sparsely during training .
There are multiple ways to share parameters , e.g. , only sharing feed - forward network ( FFN ) parameters across layers , or only sharing attention parameters .
Figure 2 shows the L 2 distances and cosine similarity of the input and output embeddings for each layer , using BERT - large and ALBERT - large configurations ( see Table 2 ) .
We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT .
This shows that the solution space for ALBERT parameters is very different from the one found by DQE .
In addition to the masked language modeling ( MLM ) loss , BERT uses an additional loss called next - sentence prediction ( NSP ) .
The NSP objective was designed to improve performance on downstream tasks , such as natural language inference , that require reasoning about the relationship between sentence pairs .
We conjecture that the main reason behind NSP 's ineffectiveness is its lack of difficulty as a task , as compared to MLM .
That is , for ALBERT , we use a sentence - order prediction ( SOP ) loss , which avoids topic prediction and instead focuses on modeling inter - sentence coherence .
The SOP loss uses as positive examples the same technique as BERT ( two consecutive segments from the same document ) , and as negative examples the same two consecutive segments but with their order swapped .
As we show in Sec. 4. 6 , it turns out that NSP can not solve the SOP task at all ( i.e. , it ends up learning the easier topic - prediction signal , and performs at randombaseline level on the SOP task ) , while SOP can solve the NSP task to a reasonable degree , presumably based on analyzing misaligned coherence cues .
We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 2 .
Due to the design choices discussed above , ALBERT models have much smaller parameter size compared to corresponding BERT models .
For example , ALBERT - large has about 1 8 x fewer parameters compared to BERT - large , 1 8 M versus 3 3 4 M .
In contrast , an ALBERT - xlarge configuration with H = 2 0 4 8 has only 5 9 M parameters , while an ALBERT - xxlarge configuration with H = 4 0 9 6 has 2 3 3 M parameters , i.e. , around 7 0 % of BERT - large 's parameters .
Parameter - sharing    BERT   base   1 0 8 M   1 2   7 6 8   7 6 8   False   large   3 3 4 M   2 4   1 0 2 4   1 0 2 4   False   xlarge   1 2 7 0 M   2 4   2 0 4 8   2 0 4 8   False    ALBERT    base   1 2 M   1 2   7 6 8   1 2 8   True   large   1 8 M   2 4   1 0 2 4   1 2 8   True   xlarge   5 9 M   2 4   2 0 4 8   1 2 8   True   xxlarge   2 3 3 M   1 2   4 0 9 6   1 2 8   True    Table 2 : The configurations of the main BERT and ALBERT models analyzed in this paper .   This improvement in parameter efficiency is the most important advantage of ALBERT 's design choices .
Before we can quantify this advantage , we need to introduce our experimental setup in more detail .   To keep the comparison as meaningful as possible , we follow the BERT setup in using the BOOKCORPUS and English Wikipedia for pretraining baseline models .
Like BERT , we use a vocabulary size of 3 0 , 0 0 0 , tokenized using SentencePiece ( Kudo & Richardson , 2 0 1 8) as in XLNet .
The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models , unless otherwise specified .   To monitor the training progress , we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4. 1 .
We report accuracies for both MLM and sentence classification tasks .
Following Yang et al. ( 2 0 1 9 ) and , we evaluate our models on three popular benchmarks : The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2 0 1 8) , two versions of the Stanford Question Answering Dataset ( SQuAD ; , and the ReAding Comprehension from Examinations ( RACE ) dataset ( Lai et al. , 2 0 1 7 ) .
The improvement in parameter efficiency showcases the most important advantage of ALBERT 's design choices , as shown in Table 3 : with only around 7 0 % of BERT - large 's parameters , ALBERT - xxlarge achieves significant improvements over BERT - large , as measured by the difference on development set scores for several representative downstream tasks : SQuAD v 1 . 1 ( + 1. 7 % ) , SQuAD v 2 . 0 ( + 4. 2 % ) , MNLI ( + 2. 2 % ) , SST - 2 ( + 3. 0 % ) , and RACE ( + 8. 5 % ) .
We also observe that BERT - xlarge gets significantly worse results than BERT - base on all metrics .
Because of less communication and fewer computations , ALBERT models have higher data throughput compared to their corresponding BERT models .
As the models get larger , the differences between BERT and ALBERT models become bigger , e.g. , ALBERT - xlarge can be trained 2. 4 x faster than BERT - xlarge .
We compare the all - shared strategy ( ALBERT - style ) , the not - shared strategy ( BERT - style ) , and intermediate strategies in which only the attention parameters are shared ( but not the FNN ones ) or only the FFN parameters are shared ( but not the attention ones ) .   The all - shared strategy hurts performance under both conditions , but it is less severe for E = 1 2 8 ( - 1 . 5 on Avg ) compared to E = 7 6 8 ( - 2 . 5 on Avg ) .
We compare head - to - head three experimental conditions for the additional inter - sentence loss : none ( XLNet - and RoBERTa - style ) , NSP ( BERT - style ) , and SOP ( ALBERT - style ) , using an ALBERTbase configuration .
Results are shown in Table 6 , both over intrinsic ( accuracy for the MLM , NSP , and SOP tasks ) and downstream tasks .
The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task ( 5 2 . 0 % accuracy , similar to the random - guess performance for the " None " condition ) .
In contrast , the SOP loss does solve the NSP task relatively well ( 7 8 . 9 % accuracy ) , and the SOP task even better ( 8 6 . 5 % accuracy ) .
Even more importantly , the SOP loss appears to consistently improve downstream task performance for multi - sentence encoding tasks ( around + 1% for SQuAD 1 . 1 , + 2% for SQuAD 2 . 0 , + 1. 7 % for RACE ) , for an Avg score improvement of around + 1% .
Networks with 3 or more layers are trained by fine - tuning using the parameters from the depth before ( e.g. , the 1 2 - layer network parameters are fine - tuned from the checkpoint of the 6 - layer network parameters ) . 4 If we compare a 3 - layer ALBERT model with a 1 - layer ALBERT model , although they have the same number of parameters , the performance increases significantly .
The speed - up results in Table 3 indicate that data - throughput for BERT - large is about 3. 1 7 x higher compared to ALBERT - xxlarge .
The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets , as in .
In this section , we report measurements on the impact of the additional data used by both XLNet and RoBERTa .
The plot in Fig. 3b shows that removing dropout significantly improves MLM accuracy .
Intermediate evaluation on ALBERT - xxlarge at around 1M training steps ( Table 1 2 ) also confirms that removing dropout helps the downstream tasks .
The single - model ALBERT configuration incorporates the best - performing settings discussed : an ALBERT - xxlarge configuration ( Table 2 ) using combined MLM and SOP losses , and no dropout .
For the GLUE ( Table 1 3 ) and RACE ( Table 1 4 ) benchmarks , we average the model predictions for the ensemble models , where the candidates are fine - tuned from different training steps using the 1 2 - layer and 2 4 - layer architectures .
Both single - model and ensemble results indicate that ALBERT improves the state - of - the - art significantly for all three benchmarks , achieving a GLUE score of 8 9 . 4 , a SQuAD 2. 0 test F 1 score of 9 2 . 2 , and a RACE test accuracy of 8 9 . 4 .
The latter appears to be a particularly strong improvement , a jump of + 1 7 . 4 % absolute points over BERT , + 7. 6 % over XLNet , + 6. 2 % over RoBERTa , and 5. 3 % over DCMI+ , an ensemble of multiple models specifically designed for reading comprehension tasks .
Our single model achieves an accuracy of 8 6 . 5 % , which is still 2. 4 % better than the state - of - the - art ensemble model .   While ALBERT - xxlarge has less parameters than BERT - large and gets significantly better results , it is computationally more expensive due to its larger structure .