More concretely , we first train a transformer - based masked language model on one language , and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective , freezing parameters of all other layers .
However , we show that it is competitive with multilingual BERT on standard cross - lingual classification benchmarks and on a new Cross - lingual Question Answering Dataset ( XQuAD ) .
We also release XQuAD as a more comprehensive cross - lingual benchmark , which comprises 2 4 0 paragraphs and 1 1 9 0 question - answer pairs from SQuAD v 1 . 1 translated into ten languages by professional translators .
Multilingual pre - training methods such as multilingual BERT ( mBERT , Devlin et al. , 2 0 1 9 ) have been successfully used for zero - shot cross - lingual transfer ( Pires et al. , 2 0 1 9 ; Lample and Conneau , 2 0 1 9 ) .
These methods work by jointly training a * Work done as an intern at DeepMind . transformer model ( Vaswani et al. , 2 0 1 7 ) to perform masked language modeling ( MLM ) in multiple languages , which is then fine - tuned on a downstream task using labeled data in a single language - typically English .
As illustrated in Figure 1 , our method starts with a monolingual transformer trained with MLM , which we transfer to a new language by learning a new embedding matrix through MLM in the new language Figure 1 : Four steps for zero - shot cross - lingual transfer : ( i ) pre - train a monolingual transformer model in English akin to BERT ; ( ii ) freeze the transformer body and learn new token embeddings from scratch for a second language using the same training objective over its monolingual corpus ; ( iii ) fine - tune the model on English while keeping the embeddings frozen ; and ( iv ) zero - shot transfer it to the new language by swapping the token embeddings . while freezing parameters of all other layers .
However , we show that it is competitive with joint multilingual pre - training across standard zero - shot cross - lingual transfer benchmarks ( XNLI , MLDoc , and PAWS - X ) .
We also experiment with a new Cross - lingual Question Answering Dataset ( XQuAD ) , which consists of 2 4 0 paragraphs and 1 1 9 0 questionanswer pairs from SQuAD v 1 . 1 ( Rajpurkar et al. , 2 0 1 6 ) translated into ten languages by professional translators .
Question answering as a task is a classic probe for language understanding .
We believe that XQuAD can serve as a more comprehensive benchmark to evaluate cross - lingual models and make this dataset publicly available at https://github.com/ deepmind/XQuAD .
Our results on XQuAD demonstrate that the monolingual transfer approach can be made competitive with jointly trained multilingual models by learning second language - specific transformations via adapter modules ( Rebuffi et al. , 2 0 1 7 ) .
Pre - train a monolingual BERT ( i.e. a transformer ) in L 1 with masked language modeling ( MLM ) and next sentence prediction ( NSP ) objectives on an unlabeled L 1 corpus . 2 .
Transfer the model to a new language by learning new token embeddings while freezing the transformer body with the same training objectives ( MLM and NSP ) on an unlabeled L 2 corpus . 3 .
In Step 2 , when we transfer the L 1 transformer to L 2 , we add a feed - forward adapter module after the projection following multi - headed attention and after the two feed - forward layers in each transformer layer , similar to Houlsby et al. ( 2 0 1 9 ) .
Note that the original transformer body is still frozen , and only parameters of the adapter modules are trainable ( in addition to the embedding matrix in L 2 ) .
We describe the models that we compare ( § 3. 1 ) , the experimental setting ( § 3. 2 ) , and the results on three classification datasets : XNLI ( § 3. 3 ) , MLDoc ( § 3. 4 ) and PAWS - X ( § 3. 5 ) .
We compare four main models in our experiments : Joint multilingual models ( JOINTMULTI ) .
This model is analogous to mBERT and closely related to other variants like XLM .
Cross - lingual word embedding mappings ( CLWE ) .
The method we described in § 2 operates at the lexical level , and can be seen as a form of learning cross - lingual word embeddings that are aligned to a monolingual transformer body .
We also include a method based on this alternative approach where we train skip - gram embeddings for each language , and map them to a shared space using VecMap ( Artetxe et al. , 2 0 1 8) . 4 We then train an English BERT model using MLM and NSP on top of the frozen mapped embeddings .
Cross - lingual transfer of monolingual models ( MONOTRANS ) .
We use Wikipedia as our training corpus , similar to mBERT and XLM ( Lample and Conneau , 2 0 1 9 ) , which we extract using the WikiExtractor tool . 5 We do not perform any lowercasing or normalization .
We use the model architecture of BERT BASE , similar to mBERT .
In natural language inference ( NLI ) , given two sentences ( a premise and a hypothesis ) , the goal is to decide whether there is an entailment , with the previous results from mBERT and XLM . 6 We summarize our main findings below : • Our JOINTMULTI results are comparable with similar models reported in the literature .
Our best JOINTMULTI model is substantially better than mBERT , and only one point worse ( on average ) than the unsupervised XLM model , which is larger in size . • Among the tested JOINTMULTI variants , we observe that using a larger vocabulary size has a notable positive impact . • JOINTPAIR models with a joint vocabulary perform comparably with JOINTMULTI .
This shows that modeling more languages does not affect the quality of the learned representations ( evaluated on XNLI ) . • The equivalent JOINTPAIR models with a disjoint vocabulary for each language perform better , which demonstrates that a shared subword vocabulary is not necessary for joint multilingual pre - training to work . • CLWE performs poorly .
Larger dimensionalities and weak supervision improve CLWE , but its performance is still below other models . • The basic version of MONOTRANS is only 2. 5 6 mBERT covers 1 0 2 languages and has a shared vocabulary of 1 1 0 k subwords .
In subsequent experiments , we include results for all variants of MONOTRANS and JOINTPAIR , the best CLWE variant ( 7 6 8 d ident ) , and JOINTMULTI with 3 2 k and 2 0 0 k voc .
Our classification experiments demonstrate that MONOTRANS is competitive with JOINTMULTI and JOINTPAIR , despite being multilingual at the embedding layer only ( i.e. the transformer body is trained exclusively on English ) .
For example , previous work has shown that models trained on MultiNLI - from which XNLI was derived - learn to exploit superficial cues in the data ( Gururangan et al. , 2 0 1 8) .
To better understand the cross - lingual generalization ability of these models , we create a new Cross - lingual Question Answering Dataset ( XQuAD ) .
Question answering is a classic probe for natural language understanding ( Hermann et al. , 2 0 1 5 ) and has been shown to be less susceptible to annotation artifacts than other popular tasks ( Kaushik and Lipton , 2 0 1 8) .
In contrast to existing classification benchmarks , question answering requires identifying relevant answer spans in longer context paragraphs , thus requiring some degree of structural transfer across languages .
XQuAD consists of a subset of 2 4 0 paragraphs and 1 1 9 0 question - answer pairs from the development set of SQuAD v 1 . 1 7 together with their translations into ten languages : Spanish , German , Greek , Russian , Turkish , Arabic , Vietnamese , Thai , Chinese , and Hindi .
Similar to our findings in the XNLI experiment , the vocabulary size has a large impact in JOINTMULTI , and JOINTPAIR models with disjoint vocabularies perform the best .
The gap between MONOTRANS and joint and models is larger , but MONOTRANS still performs surprisingly well given the nature Table 3 : XQuAD results ( F 1 ) . of the task .
We probe representations from the resulting English models with the Word in Context ( WiC ; Pilehvar and Camacho - Collados , 2 0 1 9 ) , Stanford Contextual Word Similarity ( SCWS ; Huang et al. , 2 0 1 2 ) , and the syntactic evaluation ( Marvin and Linzen , 2 0 1 8) datasets .
CLWE models - although similar in spirit to MONOTRANS - are only competitive on the easiest and smallest task ( MLDoc ) , and perform poorly on the more challenging ones ( XNLI and XQuAD ) .
To provide a more comprehensive benchmark to evaluate cross - lingual models , we also released the Cross - lingual Question Answering Dataset ( XQuAD ) .   In contrast to You et al. ( 2 0 1 9 ) , we train with a sequence length of 5 1 2 from the beginning , instead of dividing training into two stages .
XQuAD consists of a subset of 2 4 0 context paragraphs and 1 1 9 0 question - answer pairs from the development set of SQuAD v 1 . 1 ( Rajpurkar et al. , 2 0 1 6 ) together with their translations into 1 0 other languages : Spanish , German , Greek , Russian , Turkish , Arabic , Vietnamese , Thai , Chinese , and Hindi .
We show the complete results for cross - lingual word embedding mappings and joint multilingual training on MLDoc and PAWS - X in Table 7 .
Table   8 reports exact match results on XQuAD , while Table 9 reports results for all cross - lingual word embedding mappings and joint multilingual training variants .
To control for the amount of data , we use 3M sentences both for pretraining and alignment in every language . 1 0 Semantic probing We evaluate the representations on two semantic probing tasks , the Word in Context ( WiC ; Pilehvar and Camacho - Collados , 2 0 1 9 ) and Stanford Contextual Word Similarity ( SCWS ; Huang et al. , 2 0 1 2 ) datasets .
WiC is a binary classification task , which requires the model to determine if the occurrences of a word in two contexts refer to the same or different meanings .
SCWS requires estimating the semantic similarity of word pairs that occur in context .