Deep neural networks have been playing an essential role in the task of Visual Question Answering ( VQA ) .
In VQA , the attack can target the image and/or the proposed query question , dubbed main question , and yet there is a lack of proper analysis of this aspect of VQA .
We cast this ranking problem as a LASSO optimization problem .
Visual Question Answering ( VQA ) is one of the most challenging computer vision tasks in which an algorithm is given a natural language question about an image and is tasked with producing a natural language answer for that question - image pair .
The Noise Generator takes a plain text main question ( MQ ) and a plain text basic question dataset ( BQD ) as input .
It starts by ranking the basic questions in BQD by their similarity to MQ using a text similarity ranking method .
To improve the question ranking quality , we propose a new method formulated using LASSO optimization and compare it to other rankings produced by the commonly used textual similarity measures .
Then , we do perform this comparison to rank our proposed BQDs , General Basic Question Dataset ( GBQD ) and Yes/No Basic Question Dataset ( YNBQD ) .
Finally , we conduct extensive experiments to compare our proposed LASSO ranking method with the other metrics in BQD ranking .
In this paper , our main contributions are summarized as follows : • We introduce two large - scale basic questions datasets and make available two datasets for VQA robustness evaluation . • We propose a novel method to measure the robustness of VQA models and test it on six different stateof - the - art VQA models . • We propose a new LASSO - based text similarity ranking method and show that it outperforms seven popular similarity metrics .
Furthermore , in sections 4 and 5 , we present the various analyses on our proposed General Basic Question Dataset ( GBQD ) and Yes/No Basic Question Dataset ( YNBQD ) Huang et al ( 2 0 1 9 ) .
The above works involve different fields including natural language progressing ( NLP ) , computer vision , and machine learning .
Sentence evaluation metrics have been widely used in several areas such as video captioning Yu et al ( 2 0 1 6 ) and text summarization Barzilay and Elhadad ( 1 9 9 9 ) .
The R score -our proposed robustness measure -is generated based the " Accuracy Generated by Ranked Basic Question Dataset " and " Accuracy Generated by Clean VQA Testing Set " .
Then , we have two choices , GBQD and YNBQD , of Basic Question Dataset and eight different choices of question ranking methods .
If a new Basic Question Dataset or ranking method is proposed in the future , we can add them into our proposed method .
BLEU ( BiLingual Evaluation Understudy ) Papineni et al ( 2 0 0 2 ) is one of the most popular metrics in machine translation based on precision .
METEOR Banerjee and Lavie ( 2 0 0 5 ) is based on the harmonic mean of unigram precision and recall , and it can handle the stemming and synonym matching , which is designated to fix problems found with BLEU and produces a better correlation with translations by human experts .
Regarding the difference between METEOR and BLEU , METEOR evaluates the correlation at the sentence and segment level whereas BLEU looks for correlations at the corpus level .
ROUGE ( Recall Oriented Understudy of Gisting Evaluation ) Lin ( 2 0 0 4 ) is another popular recall - based metric in the text summarization community , and it tends to reward the longer sentences with the higher recall .
CIDEr Vedantam et al ( 2 0 1 5 ) , a consensus - based metric , rewards a sentence for being similar to the majority of descriptions written by the human expert and this metric is mostly used in the image captioning community .
In our experiments , we take all of the metrics above and our proposed LASSO ranking approach to rank BQs and compare their BQ ranking performance .
Some commonly used techniques , such as encoding and decoding , in the image captioning task Xu et al ( 2 0 1 5 ) ; Karpathy and Fei - Fei ( 2 0 1 5 ) ; Vinyals et al ( 2 0 1 5 ) ; Fang et al ( 2 0 1 5 ) are also used in the VQA task .
The authors of Vinyals et al ( 2 0 1 5 ) exploit a convolutional neural networks model to extract the high - level image features , and then give an LSTM unit these features as the first input .
Also , although BLEU is commonly used to evaluate the result of the image captioning task , it is n't the most proper metric to evaluate the quality of the image captioning result because of its innate property .
In VQA , we have two types of inputs with different modalities including the question sentence and image , so VQA is a multimodal task .
The authors of Kiros et al ( 2 0 1 4 ) ; Lin et al ( 2 0 1 5 ) have shown that the bilinear interaction between two embedding spaces is very successful in deep learning for fine - grained classification and multimodal language modeling .
The authors of Kim et al ( 2 0 1 7 ) propose a tensor - based method , Multimodal Low - rank Bi - linear ( MLB ) pooling , to parameterize the full bilinear interactions between image and question sentence embedding spaces .
In Ren et al ( 2 0 1 5 a ) , the authors exploit Recurrent Neural Networks ( RNN ) and Convolutional Neural Networks ( CNN ) to build a question generation algorithm , but it sometimes generates questions with invalid grammar .
The authors of ; Gao et al ( 2 0 1 5 ) ; Malinowski et al ( 2 0 1 7 ) exploit RNN to combine the word and image features for the VQA task .
Gated Recurrent Unit ( GRU ) Chung et al ( 2 0 1 4 ) is another variant of RNN , and the authors of Noh et al ( 2 0 1 6 b ) use it to encode an input question .
To the best of our knowledge , DAQUAR ( DAtaset for QUestion Answering on Real - world images ) dataset Malinowski and Fritz ( 2 0 1 4 a ) is the first proposed dataset , which contains about 1 2 . 5 thousand manually annotated question - answer pairs on about 1 4 4 9 indoor scenes Silberman et al ( 2 0 1 2 ) .
After the introduction of DAQUAR , three other VQA datasets based on MS - COCO Lin et al ( 2 0 1 4 ) have been proposed , namely Ren et al ( 2 0 1 5 b ) ; Antol et al ( 2 0 1 5 ) ; Gao et al ( 2 0 1 5 ) .
The VQA test set answers are not released because of the VQA challenge workshop .
In Yu et al ( 2 0 1 5 ) , the authors try to simplify the evaluation of the performance of VQA models by introducing Visual Madlibs , a multiple choice question answering ( QA ) by filling the blanks task .
The authors of the Visual 7 W dataset Zhu et al ( 2 0 1 6 ) have built question - answer pairs based on the Visual Genome dataset Krishna et al ( 2 0 1 7 ) , and it contains around 3 3 0 0 0 0 natural language questions .
In contrast to the other datasets such as VQA or DAQUAR , the Visual Genome dataset focuses on the so - called six Ws , namely what , where , when , who , why , and how , which can be answered with a text - based sentence .
Then , the Visual 7 W contains multiple - choice answers similar to Visual Madlibs Yu et al ( 2 0 1 5 ) .
In Nag Chowdhury et al ( 2 0 1 6 ) , the authors have proposed Xplore - M - Ego , which is a dataset of images with natural language queries , a media retrieval system , and collective memories .
There is another task , called video question answering , which is related to VQA .
In this work , we propose GBQD and YNBQD robustness - based datasets .
The VQA module contains the model we want to do robustness analysis on , while the Noise Generator utilizes eight ranking methods , namely BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 , ROUGE , CIDEr , ME - TEOR , and our proposed LASSO ranking method , to generate noise for a given main question .
Word 2 Vec Mikolov et al ( 2 0 1 3 ) , GloVe Pennington et al ( 2 0 1 4 ) and Skip - thoughts Kiros et al ( 2 0 1 5 ) are popular text encoders .
The Skip - thoughts model exploits an RNN encoder with GRU Chung et al ( 2 0 1 4 ) activations , which maps an English sentence , i.e. , q i , into a feature vector v ∈ R 4 8 0 0 .
As we have mentioned in the Introduction , the existing textual similarity measures , such as BLEU , CIDEr , METEOR , and ROUGE , can not effectively capture the semantic similarity .
To develop our basic question dataset ( BQD ) , we combine the unique questions in the training and validation datasets of the most popular VQA dataset Antol et al ( 2 0 1 5 ) and we use the testing dataset as our main question candidates .
Also , we need to do " question sentences preprocessing " , in particular , making sure that none of the main questions is contained in our basic question dataset , as otherwise , LASSO modelling can not give a useful ranking .
Additionally , because most of the VQA models have the highest accuracy performance in answering yes/no questions , we argue that yes/no questions are the simplest questions for VQA models in the sense of accuracy .
Hence , we also create a Yes/No Basic Question dataset based on the aforementioned basic question generation approach .
In our work , based on the LASSO - based ranking method , we propose two large - scale basic question datasets , General Basic Question Dataset and Yes/No Basic Question Dataset .
The proposed General and Yes/No BQ datasets , with the format { Image , M Q , 2 1 ( BQ + corresponding similarity score ) } , contain 8 1 , 4 3 4 images from the testing images of MS COCO dataset Lin et al ( 2 0 1 4 ) and 2 4 4 , 3 0 2 main questions from the testing questions of VQA dataset ( open - ended task ) Antol et al ( 2 0 1 5 ) .
Furthermore , our General and Yes/No basic questions are extracted from the validation and training questions of VQA dataset ( open - ended task ) and the corresponding similarity scores of General and Yes/No BQ are generated by our LASSO ranking approach .
That is to say , in our GBQD and YNBQD , there are 5 , 1 3 0 , 3 4 2 ( General BQ + corresponding similarity score ) tuplets and 5 , 1 3 0 , 3 4 2 ( Yes/No BQ + corresponding similarity score ) tuplets .
To analyze the robustness of a VQA model , we first measure the accuracy of the model on the clean VQA dataset Antol et al ( 2 0 1 5 ) and we call it Acc vqa .
We conduct the experiments on GBQD , YNBQD and VQA dataset Antol et al ( 2 0 1 5 ) .
The VQA dataset is based on the MS COCO dataset Lin et al ( 2 0 1 4 ) , and it includes 2 4 8 , 3 4 9 training , 1 2 1 , 5 1 2 validation and 2 4 4 , 3 0 2 testing questions .
Regarding the GBQD and YNBQD , please refer to the Details of the Proposed Basic Question Dataset section .
Because the similarity scores are negligible after top 2 1 ranked BQs , we only collect the top 2 1 ranked General and Yes/No BQs and put them into our GBQD and YNBQD .
We compare the performance of LASSO - based ranking method with non - LASSObased ranking methods including seven popular sen - tence evaluation metrics Papineni et al ( 2 0 0 2 ) ; Vedantam et al ( 2 0 1 5 ) ; Lin ( 2 0 0 4 ) ; Banerjee and Lavie ( 2 0 0 5 ) , namely BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 , ROUGE , CIDEr and METEOR that are also used to measure the similarity score between MQ and BQs .
Similar to the setup for building the General Basic Question Dataset ( GBQD ) , we build a general basic question dataset for each metric .
Table 2 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . " - " indicates the results are not available , " -std " represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and " -dev " indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .
Note that when we replace the GBQD by YNBQD and do the same experiment ( refer to Figure 5 -(b) - 1 and Figure 5 -(b) - 2 ) , the trends are similar to those in GBQD .
However , based on Figure 6 , we discover that the accuracy of these 7 similarity met - rics , { ( BLEU 1 ... 4 , ROU GE , CIDEr , M ET EOR ) } , are less monotonous and much more random from the first partition to the seventh partition .
According to the above , we see that the rankings by these 7 sentence similarity metrics are not effective in this context . ( ii ) Which VQA model is the most robust ?
Referring to Table 4 , HAV , HAR , MUA and MLB are attentionbased models whereas LQI and MU are not .
However , when we consider MU and MUA in Table 4 ( R score 2 ) , the non - attention - based model ( MU ) is more robust than the attention - based model ( MUA ) .
Note that the difference between MU and MUA is only the attention mechanism .
Yet , in Table 4 ( R score 1 ) , MUA is more robust than MU .
Finally , based on Table 4 , we conclude that HieCoAtt Lu et al ( 2 0 1 6 ) is the most robust VQA model .
Since the HieCoAtt model with co - attention mechanism which repeatedly exploits the text and image information to guide each other , it makes VQA models more robust Lu et al ( 2 0 1 6 ) ; Huang et al ( 2 0 1 9 ) .
Based on our experimental result , we know that HieCoAtt is the most robust VQA model , and this ( 2 0 1 5 ) datasets .
These results are based on our proposed LASSO BQ ranking method .
Figure 5 in our paper , our LASSO ranking method performance is better than those seven ranking methods . motivates us to conduct the extended experiments for this model . ( iii ) Can basic questions directly help the accuracy of the HieCoAtt model ?
If we do not have the step of question sentences preprocessing , the LASSO ranking method will generate some random ranking result .
For convenience , we take the same HieCoAtt VQA model to demonstrate what the random ranking is .
To compare with our proposed LASSO basic question ranking method , we also conduct the basic question ranking experiments using the seven aforementioned text similarity metrics on the same basic question candidate dataset .
As for our LASSO ranking method , the ranking performance is quite effective , despite its simplicity .
Note that , in practice , we will directly use our proposed datasets to test the robustness of VQA models without running the LASSO ranking method again , so the computational complexity of LASSO ranking method is not an issue in this case . ( vi ) Is the ranking in semantic meaning effective ?
In the LASSO BQ ranking method , the semantic meaning of a question can not be ranked very accurately but it still works quite well .
This is primarily due to the Antol et al ( 2 0 1 5 ) without question sentences preprocessing . " - " indicates the results are not available , " -std " means that the VQA model is evaluated by the complete testing set of BQD and VQA dataset , and " -dev " means that the VQA model is evaluated by the partial testing set of BQD and VQA dataset .
We believe that if more semantic encoders are developed in the future , the LASSO ranking method can readily make use of them to produce more semantically driven ranking .
Although the semantic meaning ranking by LASSO ranking method is not very accurate , it is still acceptable .
We provide some BQ ranking results using our LASSO ranking method in Figure 4 and Table 1 . ( vii ) What affects the quality of BQs ?
We provide some ranking examples based on LASSO ranking method in Table 1 to show the quality of BQs when λ = 1 0 − 6 . ( viii ) Extended experiments on YNBQD dataset .
Although we have done the basic question ranking experiments by the seven different text similarity metrics , BLEU 1 ... 4 , ROU GE , CIDEr , and M ET EOR , on GBQD , we have n't done such ranking experiments by those metrics on YNBQD .
Note that in Figure   9 -(a ) , the grey shade denotes BLEU - 1 , blue shade denotes BLEU - 2 , orange shade denotes BLEU - 3 , purple shade denotes BLEU - 4 and green shade denotes ROUGE .
CIDEr and METEOR in Figure 9 -(b ) and Figure 9 -(c ) , respectively .
Based on Figure 9 , Figure 6 , and Figure   5 , we conclude that the proposed LASSO ranking method performance is better than those seven ranking methods on both YNBQD and GBQD datasets .
In this section , we present state - of - the - art VQA models among our six tested VQA models , Antol et al ( 2 0 1 5 ) ; Lu et al ( 2 0 1 6 ) ; Ben - younes et al ( 2 0 1 7 ) ; Kim et al ( 2 0 1 7 ) , in different senses .
According to Table 4 , we observe that " HieCoAtt ( Alt , VGG 1 9 ) " model has the highest R score 1 , 0. 4 8 .
Furthermore , " HieCoAtt ( Alt , Resnet 2 0 0 ) " has the highest R score 2 , 0. 5 3 .
Therefore , for GBQD , " HieCoAtt ( Alt , VGG 1 9 ) " model is the state - of - the - art VQA model among our six tested VQA models in the sense of robustness .
However , for YNBQD , " HieCoAtt ( Alt , Resnet 2 0 0 ) " model is the state - of - the - art VQA model among our six tested VQA models in the sense of robustness .
Table 2 , we discover that " MUTAN with Attention " model in Table 2 -(d ) has the highest accuracy , 6 5 . 7 7 , and " LSTM Q+I " has the lowest accuracy , 5 8 . 1 8 .
Therefore , " MUTAN with Attention " model is the state - of - the - art VQA model among our six tested VQA models in the sense of accuracy .
Also , these results imply that the attention - based VQA model has higher accuracy than the non - attention - based one .   In this work , we propose a novel method comprised of a number of components namely , large - scale General Basic Question Dataset , Yes/No Basic Question Dataset and robustness measure ( R score ) for measuring the robustness of VQA models .
Moreover , based on our proposed General and Yes/No Basic Question Datasets and R score , we show that our LASSO BQ ranking method has the better ranking performance among most of the popular text evaluation metrics .
Finally , we have some new methods to evaluate the robustness of VQA models , so how to build a robust and accurate VQA model will be interesting future work .
Table 1 3 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . " - " indicates the results are not available , " -std " represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and " -dev " indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .
The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . " - " indicates the results are not available , " -std " represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and " -dev " indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .
Table 2 0 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the YNBQD and VQA dataset . " - " indicates the results are not available , " -std " represents the accuracy of VQA model evaluated on the complete testing set of YNBQD and VQA dataset and " -dev " indicates the accuracy of VQA model evaluated on the partial testing set of YNBQD and VQA dataset .
Table 2 1 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the YNBQD and VQA dataset . " - " indicates the results are not available , " -std " represents the accuracy of VQA model evaluated on the complete testing set of YNBQD and VQA dataset and " -dev " indicates the accuracy of VQA model evaluated on the partial testing set of YNBQD and VQA dataset .