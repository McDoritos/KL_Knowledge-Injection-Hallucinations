[
  {
    "question": "What role do deep neural networks play in Visual Question Answering (VQA)?",
    "answer": "They play an essential role by enabling models to answer natural language questions about images."
  },
  {
    "question": "What two inputs can be attacked in a VQA system?",
    "answer": "The image and the main question."
  },
  {
    "question": "How is the basic question ranking problem formulated in this paper?",
    "answer": "As a LASSO optimization problem."
  },
  {
    "question": "What is the goal of a Visual Question Answering (VQA) algorithm?",
    "answer": "To produce a natural language answer given a natural language question and an image."
  },
  {
    "question": "What does the Noise Generator take as input?",
    "answer": ["A plain text main question (MQ)", "A plain text basic question dataset (BQD)"]
  },
  {
    "question": "How does the Noise Generator begin the ranking process?",
    "answer": "By ranking basic questions by similarity to the main question using a text similarity method."
  },
  {
    "question": "What two basic question datasets are introduced in the paper?",
    "answer": ["General Basic Question Dataset (GBQD)", "Yes/No Basic Question Dataset (YNBQD)"]
  },
  {
    "question": "What are the main contributions of the paper?",
    "answer": [
      "Introduction of two large-scale basic question datasets for VQA robustness evaluation.",
      "A novel robustness measure for VQA models tested on six state-of-the-art models.",
      "A new LASSO-based text similarity ranking method outperforming seven popular similarity metrics."
    ]
  },
  {
    "question": "Which fields are involved in the presented work?",
    "answer": ["Natural Language Processing", "Computer Vision", "Machine Learning"]
  },
  {
    "question": "What measure is used to compute the proposed R score?",
    "answer": "It is based on the accuracy generated by ranked BQDs and by the clean VQA testing set."
  },
  {
    "question": "What popular machine translation metric is based on precision?",
    "answer": "BLEU."
  },
  {
    "question": "What is a key difference between METEOR and BLEU?",
    "answer": "METEOR evaluates correlation at the sentence/segment level, while BLEU evaluates at the corpus level."
  },
  {
    "question": "Which metric is commonly used in text summarization and is recall-based?",
    "answer": "ROUGE."
  },
  {
    "question": "Which consensus-based metric is widely used in image captioning?",
    "answer": "CIDEr."
  },
  {
    "question": "What do CLWE-style embeddings like Word2Vec, GloVe, and Skip-thoughts do?",
    "answer": "They encode sentences or words into vector representations."
  },
  {
    "question": "What is the purpose of question sentence preprocessing when building BQDs?",
    "answer": "To ensure that main questions are not contained in the basic question dataset, which would invalidate LASSO ranking."
  },
  {
    "question": "Why are yes/no questions considered separately in YNBQD?",
    "answer": "Because VQA models tend to achieve the highest accuracy on yes/no questions."
  },
  {
    "question": "How many images and main questions are included in GBQD and YNBQD?",
    "answer": "81,434 images and 244,302 main questions."
  },
  {
    "question": "How many total basic question + similarity score tuplets exist in each dataset (GBQD or YNBQD)?",
    "answer": "5,130,342 tuplets."
  },
  {
    "question": "How is the robustness of a VQA model first measured?",
    "answer": "By computing its accuracy on the clean VQA dataset (Acc_vqa)."
  },
  {
    "question": "Which seven text similarity metrics are compared against the LASSO ranking method?",
    "answer": ["BLEU-1", "BLEU-2", "BLEU-3", "BLEU-4", "ROUGE", "CIDEr", "METEOR"]
  },
  {
    "question": "Which VQA model is concluded to be the most robust?",
    "answer": "HieCoAtt."
  },
  {
    "question": "What mechanism contributes to the robustness of the HieCoAtt model?",
    "answer": "Its co-attention mechanism that repeatedly aligns text and image information."
  },
  {
    "question": "What happens if question preprocessing is skipped when using the LASSO ranking method?",
    "answer": "The method may generate random or meaningless rankings."
  },
  {
    "question": "What limitation does the LASSO ranking method have in terms of semantics?",
    "answer": "It cannot rank semantic meaning very accurately, though it still performs effectively overall."
  },
  {
    "question": "Which model achieved the highest accuracy among the tested VQA models?",
    "answer": "MUTAN with Attention."
  },
  {
    "question": "What new components does the paper propose for measuring VQA robustness?",
    "answer": [
      "General Basic Question Dataset (GBQD)",
      "Yes/No Basic Question Dataset (YNBQD)",
      "Robustness measure (R score)"
    ]
  },
  {
    "question": "What is the general conclusion regarding LASSO versus other similarity metrics?",
    "answer": "The LASSO ranking method outperforms the seven popular sentence evaluation metrics on both GBQD and YNBQD."
  }
]
