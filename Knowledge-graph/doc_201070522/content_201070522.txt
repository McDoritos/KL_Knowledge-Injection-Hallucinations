We believe that language features deserve more attention , and conduct experiments which compare different word embeddings , language models , and embedding augmentation steps on five common VL tasks : image - sentence retrieval , image captioning , visual question answering , phrase grounding , and text - to - clip retrieval .
Our experiments provide some striking results ; an average embedding language model outperforms an LSTM on retrieval - style tasks ; state - of - the - art representations such as BERT perform relatively poorly on vision - language tasks .
This multi - task training is applied to a new Graph Oriented Vision - Language Embedding ( GrOVLE ) , which we adapt from Word 2 Vec using WordNet and an original visual - language graph built from Visual Genome , providing a ready - to - use vision - language embedding : http://ai.bu.edu/grovle .
In recent years many methods have been proposed for vision - language tasks such as image and video captioning [ 1 2 , 2 7 , 4 7 , 4 8 , 5 2 ] , multimodal retrieval [ 1 6 , 2 4 , 2 0 , 4 9 , 3 7 , 4 6 , 5 1 ] , phrase grounding [ 4 2 , 1 9 , 4 1 , 4 3 ] , and visual question answering [ 1 4 , 2 , 5 6 , 4 4 , 5 4 ] .
We provide a side by side comparison of how word - level and sentence - level embeddings , simple and more complex language models , and fine - tuning and post - processing vectors impact performance . these tasks include a simple one - hot encoding of each word in a vocabulary ( e.g. [ 1 4 , 4 8 , 4 9 ] ) , pretrained dense vector representations like Word 2 Vec [ 3 5 ] or GloVe [ 3 8 ] , and Fisher vectors built on top of these dense representations ( e.g. [ 2 4 , 4 0 , 4 9 ] ) .
Although there are more modern embeddings such as FastText [ 4 ] , ELMo [ 3 9 ] and BERT [ 9 ] that have shown significant performance improvements on language tasks such as sentiment analysis and question answering , many vision - language approaches still use the more dated feature representations .
We perform experiments using from - scratch , Word 2 Vec [ 3 5 ] , WordNet retrofitted Word 2 Vec [ 1 3 ] , Fast - Text [ 4 ] , Visual Word 2 Vec [ 2 6 ] , HGLMM ( 3 0 0 - D , 6K - D ) [ 2 4 ] , InferSent [ 8 ] , and BERT [ 9 ] representations in addition to a new embedding , GrOVLE , on five visionlanguage tasks : image - sentence retrieval , visual question answering , phrase grounding , image captioning , and textto - clip retrieval .
For example , we find that using an Average Embedding language model , which ignores word ordering , tends to perform better than a LSTM .
For example , in Word 2 Vec the words " boy " and " girl " have higher cosine similarity than either have to the word " child . " While this is a subtle difference , it can impact tasks such as image captioning where " girl " can be replaced by " child " when describing a visual scene , but not by " boy . " These nuances are not well captured when using text - only information .
To address this , we introduce the Graph Oriented Vision - Language Embedding , GrOVLE , which has been learned for vision - language tasks specifically .
Inspired by multi - task training strategies like PackNet [ 3 4 ] , we train the GrOVLE embedding on all the vision - language tasks in our experiments .
Note that unlike PackNet , GrOVLE operates directly on the word embeddings rather than model weights .
Variance is defined as the average difference between the best and worst performance of the fine - tuned language model options ( e.g. Average Embedding + ft , Self - Attention + ft , LSTM + ft ) .
For the tasks InferSent and BERT operate on , they would land between 7th and 8th place for average rank ; average variance is N/A. Note that average variance is not provided for multi - task trained GrOVLE as it was created with the best model for each task . has been specially trained for vision - language tasks 1 . â€¢ Key insight into the transferability of word embeddings across the five vision - language tasks through the use of multi - task training .
Attention mechanisms have also become a popular way to improve performance : word - level attention has been used in image captioning by learning the weights of words using a LSTM [ 1 ] or a multi - layered perceptron [ 5 2 , 1 1 ] before being passed to a language generation model .
Dual attention [ 3 7 ] has also been used to attend to the question in VQA using feed - forward neural networks .
In Figure 3 an Average Embedding , Self - Attention , and LSTM language architecture are shown .
A more complex language architecture is a LSTM ; word representations are individually passed through a LSTM cell , each producing their own hidden state .
Lastly , we compare a Self - Attention model that is closely related to the Average Embedding architecture .
It is passed through a fully connected layer which applies Softmax to give context " scores " for each word in a sentence .
Word 2 Vec , FastText , InferSent , and BERT are reviewed before results are discussed .
Word 2 Vec introduced two variations of the NNLM model , with the primary distinction being that the nonlinear hidden layer is removed and the projection layer is shared amongst all words , i.e. the words are averaged .
This leads to the first model , Continuous Bag of Words ( CBOW ) , in which given four previous and four future words , the current word is predicted .
FastText [ 4 ] is an extension of the Word 2 Vec model in which the atomic entities of the embeddings are no longer words , but are instead character n - grams .
InferSent [ 8 ] uses a bi - directional LSTM with max - pooling to create a sentence - level embedding .
It is trained using the Natural Language Inference ( NLI ) task , in which the goal is to categorize natural language English sentence ( premise , hypothesis ) pairs into three classes : entailment , contradiction , and neutral .
The NLI model architecture separately encodes each sentence of the input pair using a BiLSTM .
This vector is then fed into a three - class classifier , defined by several FC layers and a Softmax .
The embedding is trained on two tasks : Masked Language Modeling ( MLM ) and Next Sentence Prediction .
This is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt , whose embeddings trained from scratch using their smaller vocabulary compare favorably to Word 2 Vec .
The original Word 2 Vec embedding pretrained on Google News can be considered a second baseline .
While Fast - Text is a more modern embedding , Word 2 Vec only falls behind within a point or two across all tasks , and even outperforms or performs equally as well as FastText for certain tasks ( e.g. text - to - clip , image captioning ) .
This validates works which extend Word 2 Vec such as Retrofitting , HGLMM Fisher Vectors , and GrOVLE , as Word 2 Vec may still provide advantages with additional adaptations ; results for adapted embeddings follow in Section 6 .
When comparing the architecture choices from Figure 3 we see that for retrieval - based tasks ( i.e. where the output is not free - form text ) the Average Embedding and Self - Attention models perform better than a simple LSTM - based approach , with Self - Attention being best on average .
While all language models perform closely on ReferIt phrase grounding , this still suggests that there is no need to use the more complex LSTM language model without additional modification .
Lastly , sentence level embeddings InferSent and BERT are compared in Table 1 ( d ) ; results are without fine - tuning .
Fine - tuning would likely improve performance , but is difficult to incorporate due to size ( e.g. the larger BERT model contains a total of 3 4 0 M parameters while the well - known VGG - 1 6 network uses 1 3 8 M ; fine - tuning the top layers of BERT still requires loading the full model ) .
The two are comparable to each other with the exception of phrase grounding accuracy on Flickr 3 0 K Entities ; BERT surprisingly outperforms InferSent by 1 1 . 5 5 % .
Both InferSent and BERT do not provide the best results across any task , and thus are not a leading option for vision - language tasks .
InferSent and BERT reach comparable values to the best Word 2 Vec models for image - sentence retrieval on Flickr 3 0 K , performing more poorly for the MSCOCO dataset .
For the remaining retrieval tasks , metrics are be - low the best performing model and embedding combination within 1 - 3 points , again noting the unusual exception of In - ferSent on phrase grounding of Flickr 3 0 K Entities , which significantly drops below scratch performance .
Extensions either use language enhancements , visual enhancements , or both ( e.g. WordNet retrofitting , HGLMM vs. Visual Word 2 Vec vs. GrOVLE , respectively ) .
Visual Word 2 Vec [ 2 6 ] is a neural model designed to ground the original Word 2 Vec representation with visual semantics .
Another post - processed embedding we use for this set of experiments is the Hybrid Gaussian - Laplacian Mixture Model ( HGLMM ) representation built off of Fisher vectors for Word 2 Vec [ 2 4 ] .
Fisher vectors instead concatenate the gradients of the log - likelihood of local descriptors ( which in this case are the Word 2 Vec vectors ) with respect to the HGLMM parameters .
Following [ 4 9 , 4 0 ] , we reduce the dimensions of the original encodings ( 1 8 K - D ) to 6K - D or 3 0 0 - D using PCA , as it has been found to improve numerical stability on VL tasks ( except for experiments on ReferIt which we reduce to 2K - D due to its small vocabulary size ) .
We provide a new embedding , GrOVLE , which adapts Word 2 Vec using two knowledge bases : WordNet and Visual Genome .
A joint lexicon is built with WordNet and Visual Genome as opposed to successively retrofitting the two ; this minimized forgetting of the first and thus improved performance .
Visual Genome [ 2 8 ] contains a wealth of language annotations for 1 0 8 K images : descriptions of entities in an image , their attributes , relationships between multiple entities , and whole image and region - based QA pairs .
For the generation - based tasks ( i.e. captioning and VQA ) , the benefits of using adapted embeddings are less clear .
Visual Word 2 Vec performs comparably amongst results for generation tasks ( i.e. image captioning and VQA ) , but these tasks have little variance in results , with less than a point of difference across the adapted embeddings .
The small gain provided in generation tasks by Visual Word 2 Vec does not out - weight the drops in performance across other tasks such as the significant mean recall drop of 6. 3 compared to HGLMM 's 6K - D Self - Attention result in line two of Table 2(c ) and Table 2 ( e ) for image - sentence retrieval of Flickr 3 0 K .
For comparison , GrOVLE 's Self - Attention result in Table 2 ( b ) is only 3 points lower .
We use the best performing language model in our comparisons for each task , i.e. Self - Attention for image - sentence retrieval and phrase grounding , and the LSTM language model for text - to - clip , image captioning , and VQA .
We find similar trends in performance improvements across tasks : larger gains occur for image - sentence retrieval with + 7. 9 mean recall for the Flickr 3 0 K dataset and + 6. 3 for MSCOCO .
All other tasks have performance improvements under one point , showing that while the vision - language tasks appear to transfer well without harming performance , they are leveraged most in image - sentence retrieval , with an exception of phrase grounding accuracy on ReferIt ( + 2. 3 6 % ) .
On retrieval - style tasks , the Average Embedding and Self - Attention language model tend to outperform a simple LSTM . 2 .
Along with these findings , we have introduced GrOVLE , which incorporates hierarchical language relations from WordNet as well as language with visual context from Visual Genome .