[
    {
        "question": "What do the authors investigate in their experiments?",
        "answer": "They compare different word embeddings, language models, and embedding augmentation steps on five vision-language tasks."
    },
    {
        "question": "Which five vision-language tasks are evaluated?",
        "answer": [
            "Image-sentence retrieval",
            "Image captioning",
            "Visual question answering",
            "Phrase grounding",
            "Text-to-clip retrieval"
        ]
    },
    {
        "question": "What surprising result do the authors find regarding language models?",
        "answer": "An average embedding language model outperforms an LSTM on retrieval-style tasks."
    },
    {
        "question": "How do modern models like BERT perform on vision-language tasks?",
        "answer": "BERT performs relatively poorly on vision-language tasks compared to simpler embeddings."
    },
    {
        "question": "What is GrOVLE?",
        "answer": "GrOVLE (Graph Oriented Vision-Language Embedding) is a vision-language embedding adapted from Word2Vec using WordNet and a visual-language graph from Visual Genome."
    },
    {
        "question": "Why was GrOVLE introduced?",
        "answer": "To incorporate hierarchical language relations and visual context into word embeddings specifically for vision-language tasks."
    },
    {
        "question": "Which word embeddings are compared in the experiments?",
        "answer": [
            "From-scratch embeddings",
            "Word2Vec",
            "WordNet-retrofitted Word2Vec",
            "FastText",
            "Visual Word2Vec",
            "HGLMM (300-D and 6K-D)",
            "InferSent",
            "BERT",
            "GrOVLE"
        ]
    },
    {
        "question": "Why can traditional text-only embeddings struggle in vision-language settings?",
        "answer": "Because they capture linguistic similarity but may not reflect visual interchangeability, such as 'girl' being visually replaceable by 'child' but not 'boy'."
    },
    {
        "question": "What training strategy is used to learn GrOVLE?",
        "answer": "A multi-task training strategy inspired by PackNet, applied across all evaluated vision-language tasks."
    },
    {
        "question": "What does variance measure in the comparisons?",
        "answer": "Variance measures the difference between the best and worst performance of fine-tuned language model options for each task."
    },
    {
        "question": "What types of attention mechanisms are mentioned?",
        "answer": [
            "Word-level attention using LSTMs",
            "Word-level attention using MLPs",
            "Dual attention for VQA",
            "Self-attention models"
        ]
    },
    {
        "question": "What are the characteristics of an LSTM language model in this context?",
        "answer": "Each word’s representation is passed through an LSTM cell, producing hidden states that encode sequential dependencies."
    },
    {
        "question": "How does the Self-Attention model work?",
        "answer": "It computes context scores for each word using a fully connected layer with Softmax, producing weighted embeddings."
    },
    {
        "question": "How does Word2Vec’s CBOW model operate?",
        "answer": "It predicts a word based on surrounding context, using shared projection layers and removing the nonlinear hidden layer."
    },
    {
        "question": "What is FastText’s key difference from Word2Vec?",
        "answer": "FastText uses character n-grams instead of whole words as the atomic embedding components."
    },
    {
        "question": "What is InferSent trained on?",
        "answer": "InferSent is trained on Natural Language Inference using a bi-directional LSTM with max-pooling."
    },
    {
        "question": "How is BERT trained?",
        "answer": "BERT is trained using Masked Language Modeling and Next Sentence Prediction."
    },
    {
        "question": "Why can smaller-vocabulary embeddings outperform pretrained Word2Vec on some tasks?",
        "answer": "Because training from scratch on small vocabularies can produce task-aligned embeddings that better fit datasets like ReferIt."
    },
    {
        "question": "How does Word2Vec compare with FastText in the experiments?",
        "answer": "Word2Vec performs within 1–2 points of FastText and even outperforms it on some tasks like text-to-clip and image captioning."
    },
    {
        "question": "What is Visual Word2Vec?",
        "answer": "A neural model that augments Word2Vec with visual semantic grounding."
    },
    {
        "question": "What is HGLMM?",
        "answer": "The Hybrid Gaussian-Laplacian Mixture Model representation, which builds Fisher vectors on top of Word2Vec embeddings."
    },
    {
        "question": "How is dimensionality reduced in HGLMM-based representations?",
        "answer": "Using PCA to reduce dimensions (e.g., from 18K-D to 6K-D or 300-D)."
    },
    {
        "question": "What data sources are combined in GrOVLE?",
        "answer": [
            "WordNet",
            "Visual Genome"
        ]
    },
    {
        "question": "Why does GrOVLE use a joint lexicon rather than sequential retrofitting?",
        "answer": "To minimize catastrophic forgetting and improve performance."
    },
    {
        "question": "How do adapted embeddings perform on generation tasks like captioning and VQA?",
        "answer": "They show little performance variance, with less than one point difference across adapted embeddings."
    },
    {
        "question": "Which tasks show the largest improvements when using adapted embeddings like GrOVLE?",
        "answer": [
            "Image-sentence retrieval (+7.9 on Flickr30K, +6.3 on MSCOCO)",
            "Phrase grounding on ReferIt (+2.36%)"
        ]
    },
    {
        "question": "Which language models perform best on retrieval tasks?",
        "answer": "Average Embedding and Self-Attention models outperform LSTMs on retrieval-style tasks."
    },
    {
        "question": "Which language model configurations are used for each task?",
        "answer": {
            "retrieval tasks": "Self-Attention",
            "phrase grounding": "Self-Attention",
            "text-to-clip retrieval": "LSTM",
            "image captioning": "LSTM",
            "VQA": "LSTM"
        }
    },
    {
        "question": "What is the main contribution of the paper?",
        "answer": "The introduction of GrOVLE and a comprehensive analysis of how word embeddings and language models transfer across vision-language tasks."
    }
]
