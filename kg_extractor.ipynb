{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51a9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from networkx.readwrite import graphml\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20aba2",
   "metadata": {},
   "source": [
    "### Datasets for KG creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea0caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets to be processed: dict_keys(['train', 'dev', 'test', 'test_ood'])\n"
     ]
    }
   ],
   "source": [
    "#Input triplets (taken from postprocessed documents)\n",
    "datasets = {\n",
    "    'train': 'postprocessed-dataset/train_chunks.json',\n",
    "    'dev': 'postprocessed-dataset/dev_chunks.json',\n",
    "    'test': 'postprocessed-dataset/test_chunks.json',\n",
    "    'test_ood': 'postprocessed-dataset/test_ood_chunks.json' \n",
    "}\n",
    "print(\"Datasets to be processed:\", datasets.keys())\n",
    "\n",
    "G = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06382213",
   "metadata": {},
   "source": [
    "### Function for entity stripping of label\n",
    "triplets in this dataset follow the following structure:\n",
    "\n",
    "$$[subject:label,relationship,object:label]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa54527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entity(e):\n",
    "    if \":\" in e:\n",
    "        label, etype = e.split(\":\", 1)\n",
    "    else:\n",
    "        label, etype = e, None\n",
    "    return label.strip(), etype.strip() if etype else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade2e76",
   "metadata": {},
   "source": [
    "### Reading datasets and KG creation\n",
    "Each dataset contains the following fields:\n",
    "- doc_id — Unique identifier for the source document.\n",
    "- chunk_id — Identifier for the text chunk or segment within the document.\n",
    "- relations — List of extracted relations, where each relation follows the structure:\n",
    "    - subject:label, relationship, object:label\n",
    "\n",
    "\n",
    "During processing, the system reads each entry and:\n",
    "- Creates a node for every unique entity (subject and object).\n",
    "- Creates an edge between those nodes representing the specified relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60b3c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing postprocessed-dataset/train_chunks.json...\n",
      "Processing postprocessed-dataset/dev_chunks.json...\n",
      "Processing postprocessed-dataset/test_chunks.json...\n",
      "Processing postprocessed-dataset/test_ood_chunks.json...\n"
     ]
    }
   ],
   "source": [
    "for split, filepath in datasets.items():\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        print(f\"Processing {filepath}...\")\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            doc_id = item.get('doc_id')\n",
    "            chunk_id = item.get('chunk_id')\n",
    "            relations = item.get('relations', [])\n",
    "\n",
    "            for i,(entity1, rel, entity2) in enumerate(relations):\n",
    "                entity1_label, entity1_type = parse_entity(entity1)\n",
    "                entity2_label, entity2_type = parse_entity(entity2)\n",
    "\n",
    "                G.add_node(entity1_label, type=entity1_type)\n",
    "                G.add_node(entity2_label, type=entity2_type)\n",
    "                print\n",
    "                \n",
    "                #Because MultiDiGraph allows multiple edges between nodes, we create a unique key for each edge, \n",
    "                # if not gephi will warn that a key with a ID already exists.\n",
    "                edge_key = f\"{entity1_label}_{entity2_label}_{doc_id}_{chunk_id}_{i}\"\n",
    "\n",
    "                G.add_edge(\n",
    "                    entity1_label,\n",
    "                    entity2_label,\n",
    "                    key=edge_key,\n",
    "                    relation=rel,\n",
    "                    doc_id=doc_id,\n",
    "                    chunk_id=chunk_id\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3569b7",
   "metadata": {},
   "source": [
    "### Saving knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7bb80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 5640, Edges: 12083\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"Knowledge-graph\", exist_ok=True)\n",
    "\n",
    "graph_path = os.path.join(\"Knowledge-graph\", \"kg_total.graphml\")\n",
    "nx.write_graphml(G, graph_path)\n",
    "\n",
    "print(f\"Nodes: {len(G.nodes)}, Edges: {len(G.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9b91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kg(name,input_file):\n",
    "    graph = nx.MultiDiGraph()\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        print(f\"Processing {input_file}...\")\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            doc_id = item.get('doc_id')\n",
    "            chunk_id = item.get('chunk_id')\n",
    "            relations = item.get('relations', [])\n",
    "\n",
    "            for i,(entity1, rel, entity2) in enumerate(relations):\n",
    "                entity1_label, entity1_type = parse_entity(entity1)\n",
    "                entity2_label, entity2_type = parse_entity(entity2)\n",
    "\n",
    "                graph.add_node(entity1_label, type=entity1_type)\n",
    "                graph.add_node(entity2_label, type=entity2_type)\n",
    "                print\n",
    "                \n",
    "                # Because MultiDiGraph allows multiple edges between nodes, we create a unique key for each edge, \n",
    "                # if not gephi will warn that a key with a ID already exists.\n",
    "                edge_key = f\"{entity1_label}_{entity2_label}_{doc_id}_{chunk_id}_{i}\"\n",
    "\n",
    "                graph.add_edge(\n",
    "                    entity1_label,\n",
    "                    entity2_label,\n",
    "                    key=edge_key,\n",
    "                    relation=rel,\n",
    "                    doc_id=doc_id,\n",
    "                    chunk_id=chunk_id\n",
    "            )\n",
    "\n",
    "    os.makedirs(\"Knowledge-graph\", exist_ok=True)\n",
    "\n",
    "    graph_path = os.path.join(\"Knowledge-graph\", f\"kg_{name}.graphml\")\n",
    "    nx.write_graphml(graph, graph_path)\n",
    "\n",
    "    print(f\"Nodes: {len(graph.nodes)}, Edges: {len(graph.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09cd4fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing postprocessed-dataset/train_chunks.json...\n",
      "Nodes: 4164, Edges: 8743\n",
      "Processing postprocessed-dataset/dev_chunks.json...\n",
      "Nodes: 646, Edges: 1132\n",
      "Processing postprocessed-dataset/test_chunks.json...\n",
      "Nodes: 969, Edges: 1626\n",
      "Processing postprocessed-dataset/test_ood_chunks.json...\n",
      "Nodes: 407, Edges: 582\n"
     ]
    }
   ],
   "source": [
    "extract_kg('train', datasets['train'])\n",
    "extract_kg('dev', datasets['dev'])\n",
    "extract_kg('test', datasets['test'])\n",
    "extract_kg('test_ood', datasets['test_ood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bae125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique document IDs in train dataset: {'52910494', '198897554', '60440450', '202888986', '4246700', '210713911', '204402755', '202565512', '210839714', '6116678', '3920676', '209386851', '210164920', '204901567', '202719032', '23569888', '21683040', '59599694', '52009210', '24972096', '104291983', '195347056', '198231883', '209862890', '102351044', '28984897', '211010520', '201124533', '208513596', '199543700', '150374036', '209532167', '210861282', '207880647', '52180375', '211010758', '210860962', '35249701', '22825560', '202734316', '53719742', '202888751', '211004033', '208202241', '210860760', '202750230', '202734254', '51559', '201070697', '53731879', '211020570', '199668978', '147703932', '208548469', '201070522', '198147921', '67855714', '202676714', '4539700', '153312532', '146808333', '201646309', '146120936', '7507210', '210164517', '210920315', '51876625', '202540251', '202577400', '210839545', '4319457', '44148233', '56657874', '51923817', '11241677', '211010786', '54447105', '203593581', '6423078', '202540590'}\n",
      "Total unique document IDs in train dataset: 80\n",
      "Random sample of 5 document IDs from train dataset: ['201070522', '146120936', '208548469', '204901567', '202888986']\n",
      "Saved graph and content for doc_id=146120936: Nodes=80, Edges=125, Chunks=71\n",
      "Saved graph and content for doc_id=208548469: Nodes=91, Edges=146, Chunks=102\n",
      "Saved graph and content for doc_id=204901567: Nodes=53, Edges=77, Chunks=46\n",
      "Saved graph and content for doc_id=202888986: Nodes=49, Edges=86, Chunks=61\n",
      "Saved graph and content for doc_id=201070522: Nodes=70, Edges=144, Chunks=58\n"
     ]
    }
   ],
   "source": [
    "# I want to get the ID of all documents present in the train dataset\n",
    "documents = set()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "with open(datasets['train'], 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        doc_id = item.get('doc_id')\n",
    "        documents.add(doc_id)\n",
    "\n",
    "\n",
    "print(f\"Unique document IDs in train dataset: {documents}\")\n",
    "print(f\"Total unique document IDs in train dataset: {len(documents)}\")\n",
    "\n",
    "ran_docs = random.sample(sorted(documents), 5)\n",
    "print(f\"Random sample of 5 document IDs from train dataset: {ran_docs}\")\n",
    "\n",
    "graphs = {}\n",
    "contents = {}\n",
    "\n",
    "\n",
    "with open(datasets['train'], 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        doc_id = item.get('doc_id')\n",
    "\n",
    "        if doc_id in ran_docs:\n",
    "            # Create the document graph if it doesn't exist\n",
    "            if doc_id not in graphs:\n",
    "                graphs[doc_id] = nx.MultiDiGraph()\n",
    "                contents[doc_id] = []\n",
    "\n",
    "            doc_graph = graphs[doc_id]\n",
    "\n",
    "            chunk_id = item.get('chunk_id')\n",
    "            relations = item.get('relations', [])\n",
    "            content = item.get('content', [])\n",
    "\n",
    "            # saves the chunk content\n",
    "            if isinstance(content, list):\n",
    "                contents[doc_id].extend(content)\n",
    "            else:\n",
    "                contents[doc_id].append(content)\n",
    "\n",
    "            # Adds entities and relations to the graph\n",
    "            for i, (entity1, rel, entity2) in enumerate(relations):\n",
    "                entity1_label, entity1_type = parse_entity(entity1)\n",
    "                entity2_label, entity2_type = parse_entity(entity2)\n",
    "\n",
    "                doc_graph.add_node(entity1_label, type=entity1_type)\n",
    "                doc_graph.add_node(entity2_label, type=entity2_type)\n",
    "\n",
    "                edge_key = f\"{entity1_label}_{entity2_label}_{doc_id}_{chunk_id}_{i}\"\n",
    "\n",
    "                doc_graph.add_edge(\n",
    "                    entity1_label,\n",
    "                    entity2_label,\n",
    "                    key=edge_key,\n",
    "                    relation=rel,\n",
    "                    doc_id=doc_id,\n",
    "                    chunk_id=chunk_id\n",
    "                )\n",
    "\n",
    "# Save each document's graph and content\n",
    "for doc_id, doc_graph in graphs.items():\n",
    "    doc_dir = os.path.join(\"Knowledge-graph\", f\"doc_{doc_id}\")\n",
    "    os.makedirs(doc_dir, exist_ok=True)\n",
    "\n",
    "    # Save KG\n",
    "    graph_path = os.path.join(doc_dir, f\"kg_{doc_id}.graphml\")\n",
    "    nx.write_graphml(doc_graph, graph_path)\n",
    "\n",
    "    # Save content\n",
    "    content_path = os.path.join(doc_dir, f\"content_{doc_id}.txt\")\n",
    "    with open(content_path, \"w\", encoding=\"utf-8\") as cf:\n",
    "        cf.write(\"\\n\".join(contents[doc_id]))\n",
    "\n",
    "    print(f\"Saved graph and content for doc_id={doc_id}: \"\n",
    "          f\"Nodes={len(doc_graph.nodes)}, Edges={len(doc_graph.edges)}, \"\n",
    "          f\"Chunks={len(contents[doc_id])}\")\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
