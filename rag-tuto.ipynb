{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e8dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACCING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbdb87d",
   "metadata": {},
   "source": [
    "#### Chat Model\n",
    "It is necessary to install : langchain[google-genai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a125465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppcro\\Desktop\\Universidade\\Mestrado\\2º ano\\1º Semestre\\Conhecimento e Linguagem\\Project\\cur-git_files\\KL_Knowledge-Injection-Hallucinations\\venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.13) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini:\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b646f",
   "metadata": {},
   "source": [
    "#### Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09272b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13f27a",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ae227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c7b1b",
   "metadata": {},
   "source": [
    "#### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70dd1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "print(docs[0].page_content[:500])  # Print the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52dda6",
   "metadata": {},
   "source": [
    "#### Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e2117ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e5876",
   "metadata": {},
   "source": [
    "#### Storing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c5eb1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c37ec42b-f067-492f-8f49-8b4810b3e2f3', 'd52c4458-e450-4223-a8d6-56de5565d099', '366fd9e2-13fe-4c82-8449-1b7d63b39fe4']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582fa37",
   "metadata": {},
   "source": [
    "#### RAG Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3d5ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d8c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, system_message=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c943f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe user is asking for the standard method of Task Decomposition and its common extensions. I should first use the retrieve_context tool to find out what the standard method is. Then, I will use the same tool again to find common extensions of that method.\n",
      "Action: retrieve_context\n",
      "Action Input: standard method for Task Decomposition\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe provided text describes three methods for task decomposition: LLM with simple prompting, task-specific instructions, and human inputs. It also mentions a distinct approach called LLM+P, which uses an external classical planner with PDDL. The question asks for the \"standard method.\" The first three methods listed (LLM with simple prompting, task-specific instructions, human inputs) seem to be the most straightforward and direct ways to decompose a task, especially when using an LLM. LLM+P, while a method for planning and decomposition, is presented as a \"distinct approach\" and relies on external tools and specific domain knowledge (PDDL), suggesting it might not be the *standard* or most universally applicable method. Therefore, I will consider the LLM-based prompting and instruction-based methods as the standard.\n",
      "\n",
      "Now I need to find common extensions of these methods. I will use the retrieve_context tool again.\n",
      "Action: retrieve_context\n",
      "Action Input: extensions of LLM task decomposition methods\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results describe several methods for task decomposition, but they don't explicitly state which one is the \"standard.\" However, the methods listed as (1) LLM with simple prompting and (2) using task-specific instructions are presented first and seem to be the most direct LLM-based approaches. The text also mentions LLM+P as a \"distinct approach.\" I will consider the LLM-based prompting and instruction-based methods as the standard.\n",
      "\n",
      "Regarding extensions, the text describes LLM+P as a distinct approach that uses an external classical planner with PDDL. This can be considered an extension or an alternative approach that builds upon LLM capabilities for planning and decomposition. The text doesn't explicitly list \"extensions\" of the simpler LLM prompting methods, but LLM+P is a significant advancement in this area.\n",
      "\n",
      "Therefore, I will present the LLM-based methods as standard and LLM+P as a distinct, more advanced approach.\n",
      "Final Answer: The standard methods for task decomposition involve using Large Language Models (LLMs) with simple prompting (e.g., \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"), or by using task-specific instructions (e.g., \"Write a story outline.\" for writing a novel). Human inputs can also be used.\n",
      "\n",
      "A distinct approach, LLM+P, extends these capabilities by relying on an external classical planner for long-horizon planning. This method uses the Planning Domain Definition Language (PDDL) as an intermediate interface. The LLM translates the problem into \"Problem PDDL,\" requests a classical planner to generate a PDDL plan based on an existing \"Domain PDDL,\" and then translates the PDDL plan back into natural language. This approach is particularly suited for domains where PDDL and suitable planners are readily available, such as certain robotic setups.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The standard methods for task decomposition involve using Large Language Models (LLMs) with simple prompting (e.g., \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"), or by using task-specific instructions (e.g., \"Write a story outline.\" for writing a novel). Human inputs can also be used.\n",
      "\n",
      "A distinct approach, LLM+P, extends these capabilities by relying on an external classical planner for long-horizon planning. This method uses the Planning Domain Definition Language (PDDL) as an intermediate interface. The LLM translates the problem into \"Problem PDDL,\" requests a classical planner to generate a PDDL plan based on an existing \"Domain PDDL,\" and then translates the PDDL plan back into natural language. This approach is particularly suited for domains where PDDL and suitable planners are readily available, such as certain robotic setups.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "# Execução simples (mais robusta que usar .stream se o agente não suportar streaming)\n",
    "result = agent.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811336b",
   "metadata": {},
   "source": [
    "#### RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35070af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "def make_agent_with_context(query: str):\n",
    "    retrieved_docs = vector_store.similarity_search(query)\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    # Passe a tool aqui (não uma lista vazia)\n",
    "    tools = [retrieve_context]\n",
    "    agent = initialize_agent(\n",
    "        tools,\n",
    "        model,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        system_message=system_message,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6131e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find the definition of \"task decomposition\". I will use the `retrieve_context` tool to search for this term.\n",
      "Action: retrieve_context\n",
      "Action Input: What is task decomposition?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results provide a clear definition of task decomposition and its methods. I can now answer the question.\n",
      "Final Answer: Task decomposition is the process of breaking down a large task into smaller, more manageable subgoals or steps. This can be achieved through various methods, including using LLMs with simple prompts (e.g., \"Steps for XYZ.\"), employing task-specific instructions (e.g., \"Write a story outline.\"), or through human input. An alternative approach involves using an external classical planner, such as in the LLM+P method, where the LLM translates the problem into a Planning Domain Definition Language (PDDL) format, an external planner generates a plan, and the LLM then translates the plan back into natural language.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Task decomposition is the process of breaking down a large task into smaller, more manageable subgoals or steps. This can be achieved through various methods, including using LLMs with simple prompts (e.g., \"Steps for XYZ.\"), employing task-specific instructions (e.g., \"Write a story outline.\"), or through human input. An alternative approach involves using an external classical planner, such as in the LLM+P method, where the LLM translates the problem into a Planning Domain Definition Language (PDDL) format, an external planner generates a plan, and the LLM then translates the plan back into natural language.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "agent = make_agent_with_context(query)\n",
    "result = agent.run(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
